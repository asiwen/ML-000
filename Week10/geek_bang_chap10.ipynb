{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm9iNMw5nMtl"
   },
   "source": [
    "# PyTorch Training Short:\n",
    "\n",
    "\n",
    "---\n",
    "First let us check whether the whole thing is working\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CI_nJEend03"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change b type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m6u2qdWjdct8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (7) Failed to connect to raw.githubusercontent.com port 443: 拒绝连接\n",
      "python: can't open file 'pytorch-xla-env-setup.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OZkTfRt8dt2i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-lightning==1.1.0rc1\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pytorch-lightning==1.1.0rc1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning==1.1.0rc1 sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8p3Tbx8cWEFA"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M0J_DnaeoBTi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 16.7 gigabytes of available RAM\n",
      "\n",
      "To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n",
      "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
      "re-execute this cell.\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42H2EckBoKAf"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5MT1Sh1ot60"
   },
   "source": [
    "### Create a simple dataset to test the correctness of our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8vPVo_jIoZ9Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "x = torch.randn(100000, 2)\n",
    "noise = torch.randn(100000,)\n",
    "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Jz_JAic0o1_L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86584\n"
     ]
    }
   ],
   "source": [
    "y_np = y.numpy()\n",
    "x_np = x.numpy()\n",
    "y_train, y_test = y_np[:50000], y_np[50000:]\n",
    "x_train, x_test = x_np[:50000, :], x_np[50000:, :]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression() \n",
    "log_reg.fit(x_train, y_train)\n",
    "y_pred = log_reg.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjdyrSD2pIoX"
   },
   "source": [
    "### Now create an evil data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iuI0QJF-o_fe"
   },
   "outputs": [],
   "source": [
    "x_1 = torch.randn(100000)\n",
    "x_2 = torch.randn(100000)\n",
    "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
    "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
    "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
    "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
    "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
    "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
    "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zBxF0QCp-Na"
   },
   "source": [
    "### Now let us test if we have an oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x3jSEESWp6mN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72312\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
    "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
    "oracle_train, oracle_test = x_useful.numpy()[:50000], x_useful.numpy()[50000:]\n",
    "log_reg_2 = LogisticRegression()\n",
    "log_reg_2.fit(oracle_train[:, None],y_train)\n",
    "y_pred = log_reg_2.predict(oracle_test[:, None])\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUZ94ad7qVhZ"
   },
   "source": [
    "### What if the oracle is not here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SETnkiN1qTHU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60806"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
    "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
    "log_reg_3 = LogisticRegression()\n",
    "log_reg_3.fit(x_train, y_train)\n",
    "y_pred = log_reg_3.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH7Anlg0qg2j"
   },
   "source": [
    "### Let us run a basic PyTorch example to test whether the good is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kXXQ_Tbzqe8k"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(100000, 2)\n",
    "noise = torch.randn(100000,)\n",
    "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)\n",
    "x_train, x_test = x[:50000, :], x[50000:, :]\n",
    "y_train, y_test = y[:50000], y[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o0R12sOnquZq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TnHG9ujxqxZt"
   },
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "\n",
    "train_dataset = MyDataSet(x_train, y_train)\n",
    "test_dataset = MyDataSet(x_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JO7lkElKrEOO"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def mish(input):\n",
    "\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        '''\n",
    "        return mish(input)\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, res_coef = 0, dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear  = nn.Linear(dim_in, dim_out)\n",
    "        self.res_coef = res_coef\n",
    "        self.activation = Mish()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.ln = nn.LayerNorm(dim_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.dropout(y)\n",
    "        if self.res_coef == 0:\n",
    "            return self.ln(y)\n",
    "        else:\n",
    "            return self.ln(self.res_coef*x +y )\n",
    "\n",
    "        \n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, dim_in, dim, res_coef=0.5, dropout_p = 0.1, n_layers = 10):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.ModuleList()\n",
    "        self.first_linear = MLPLayer(dim_in, dim)\n",
    "        self.n_layers = n_layers\n",
    "        for i in range(n_layers):\n",
    "            self.mlp.append(MLPLayer(dim, dim, res_coef, dropout_p))\n",
    "        self.final = nn.Linear(dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        x= self.sigmoid(self.final(x))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4ka-3_rfrTKw"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, dim_in, dim, res_coef=0, dropout_p=0, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.backbone = MyNetwork(dim_in, dim, res_coef, dropout_p, n_layers)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Training loss\", loss)\n",
    "        self.log(\"Training acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "import os\n",
    "class CheckpointEveryNSteps(pl.Callback):\n",
    "    def __init__(self, save_step_frequency):\n",
    "        self.save_step_frequency = save_step_frequency\n",
    "\n",
    "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
    "        epoch = trainer.current_epoch\n",
    "        global_step = trainer.global_step\n",
    "        if global_step % self.save_step_frequency == 0:\n",
    "            filename = \"epoch=\" + str(epoch) + \"_step=\" + str(global_step)+\".ckpt\"\n",
    "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
    "            trainer.save_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "47ns8nDyrd3v",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name     | Type      | Params\n",
      "---------------------------------------\n",
      "0 | backbone | MyNetwork | 321   \n",
      "1 | loss     | BCELoss   | 0     \n",
      "2 | accuracy | Accuracy  | 0     \n",
      "---------------------------------------\n",
      "321       Trainable params\n",
      "0         Non-trainable params\n",
      "321       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681d8bf4998c4dab92e28de4bc6ec558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cd6920a036422387f175a7c222cf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db60c34e1b74efba765ec7eb07a48bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f859f0cc1744c38a833705c88c04fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03c5082ef8b4d7b87806fb51870b782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f933e7a78bc745b4ae5bef6233735ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326df0250d394593b69908aacd14d95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e40a3371d742009a3066a073c7b2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145bfad809fb411c9c70d60e951bc7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b7d1d18b6b4d34a7582ca2f2b74faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "save_by_steps = CheckpointEveryNSteps(100)\n",
    "training_module = TrainingModule(2, 10, 0.5, 0.1, 2)\n",
    "trainer = pl.Trainer(max_epochs=2, gpus=None, progress_bar_refresh_rate=100, val_check_interval=0.25, logger=tb_logger)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "n9851mNJrlFi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1702db73152e91c1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1702db73152e91c1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru23N23JSCNM"
   },
   "source": [
    "\n",
    "### Now Let us use the evil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mQRdDlCTSh9T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "x_1 = torch.randn(100000)\n",
    "x_2 = torch.randn(100000)\n",
    "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
    "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
    "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
    "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
    "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
    "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
    "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) \n",
    "\n",
    "num_workers =4\n",
    "x_train, x_test = x[:50000, :], x[50000:, :]\n",
    "y_train, y_test = y[:50000], y[50000:]\n",
    "train_dataset = MyDataSet(x_train, y_train)\n",
    "test_dataset = MyDataSet(x_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MSR6VqoaS-L9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name     | Type      | Params\n",
      "---------------------------------------\n",
      "0 | backbone | MyNetwork | 24.5 K\n",
      "1 | loss     | BCELoss   | 0     \n",
      "2 | accuracy | Accuracy  | 0     \n",
      "---------------------------------------\n",
      "24.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.5 K    Total params\n",
      "0.098     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42e71084e234fbd987568200d3063b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5be69a3b6284af4bdcd108aec191542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7bb3336b884266bc67b013421bd3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56a2a15b0504f13acc7a57faecbb469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21ba6a8d3b84941bbcbe26c16e60017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bc067a452246fbacf27464206e8cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9385de8466ce4317bfe5f1d085c34a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d289251e8354b879157a16084670aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91c173664934a81a2fe0ed03b3a9dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bbb139ca1040d0a850692452059134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d869405714714360a6addfa3efb148c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd26624ce8144a7887c6054fad8846f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efb575646fd40e2a09eb247a6de3be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309c0e43959f49f195e1a34d6a551fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b26b638a4b4615b5c9bca755df5c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b3d68a1c0f43148c6f840676e9921e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe4418d0b414ec2bad22c5df409ae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62e3ce142cc478fab55168aba5d9576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d993925b079c48708ce11405b9ee2035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d958c00f2524a1d9f0e8ef68b78d69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd53cdafc7940e4bd52d78546cc4051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4af6535b67649449bd5faf5a0485b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b87d7da1d846e5b2a17c05b7ad055c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d67afbf7524214a9d0eac111921a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8982acc86fde44f28a47500847c50796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb56569bb3ab46e18073af97d7f70df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619a533b04ab4310a2df57247f429143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e6e8c94fcc4505915219b0c1e30d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e9f0cbfb26422fb48f8d5883b6da1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d2221a637d4d0da70943b3eb00f6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906a6ebee971496fad0903be7ca2687a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b896b7b89fe842c5afc02a03f1a08370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99549abefe0d48229b4b9a2a5bf74a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c3e28e51d742a5956b47170cfe320c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6c46835d0943d8a3d3b41b03b60107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2e10bfe62f447c8fe7dc554d7491e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25abd82d378429d9705c6bf19954e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f93efdcbd746c09ac41e7a261da072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c943aa3f40954ec4a0b74a967f87dc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9094f6d5e2cb4289b333c7ec63781748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783633f4c0824e16969ef0784d06fe12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65d403635ca49cc8a0d7cd8dfc2a389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "\n",
    "training_module = TrainingModule(62, 32, 0.5, 0.1, 20)\n",
    "trainer = pl.Trainer(max_epochs=20, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "id": "1dGk-DQETEs0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-40fe03525231061d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-40fe03525231061d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjdSSleYTN3o"
   },
   "source": [
    "Now let us see how LightGBM works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xF66ZMrMTS6X"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
    "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
    "\n",
    "train_dataset = lgb.Dataset(x_train_np, y_train_np)\n",
    "test_dataset = lgb.Dataset(x_test_np, y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "id": "L-pESKZ2UAXC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30556, number of negative: 19444\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15810\n",
      "[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.611120 -> initscore=0.452023\n",
      "[LightGBM] [Info] Start training from score 0.452023\n",
      "[1]\ttraining's binary_error: 0.38888\tvalid_1's binary_error: 0.39204\n",
      "[2]\ttraining's binary_error: 0.38888\tvalid_1's binary_error: 0.39204\n",
      "[3]\ttraining's binary_error: 0.38836\tvalid_1's binary_error: 0.39174\n",
      "[4]\ttraining's binary_error: 0.31498\tvalid_1's binary_error: 0.32014\n",
      "[5]\ttraining's binary_error: 0.31898\tvalid_1's binary_error: 0.32428\n",
      "[6]\ttraining's binary_error: 0.32242\tvalid_1's binary_error: 0.32954\n",
      "[7]\ttraining's binary_error: 0.35086\tvalid_1's binary_error: 0.35952\n",
      "[8]\ttraining's binary_error: 0.29726\tvalid_1's binary_error: 0.30606\n",
      "[9]\ttraining's binary_error: 0.28656\tvalid_1's binary_error: 0.29774\n",
      "[10]\ttraining's binary_error: 0.27554\tvalid_1's binary_error: 0.28474\n",
      "[11]\ttraining's binary_error: 0.27238\tvalid_1's binary_error: 0.28192\n",
      "[12]\ttraining's binary_error: 0.26954\tvalid_1's binary_error: 0.28144\n",
      "[13]\ttraining's binary_error: 0.2684\tvalid_1's binary_error: 0.27992\n",
      "[14]\ttraining's binary_error: 0.26766\tvalid_1's binary_error: 0.27958\n",
      "[15]\ttraining's binary_error: 0.26812\tvalid_1's binary_error: 0.27744\n",
      "[16]\ttraining's binary_error: 0.26822\tvalid_1's binary_error: 0.27774\n",
      "[17]\ttraining's binary_error: 0.26826\tvalid_1's binary_error: 0.27796\n",
      "[18]\ttraining's binary_error: 0.2685\tvalid_1's binary_error: 0.27762\n",
      "[19]\ttraining's binary_error: 0.26844\tvalid_1's binary_error: 0.27764\n",
      "[20]\ttraining's binary_error: 0.2673\tvalid_1's binary_error: 0.27706\n",
      "[21]\ttraining's binary_error: 0.2671\tvalid_1's binary_error: 0.27672\n",
      "[22]\ttraining's binary_error: 0.26656\tvalid_1's binary_error: 0.27654\n",
      "[23]\ttraining's binary_error: 0.26636\tvalid_1's binary_error: 0.27668\n",
      "[24]\ttraining's binary_error: 0.26602\tvalid_1's binary_error: 0.27668\n",
      "[25]\ttraining's binary_error: 0.26534\tvalid_1's binary_error: 0.27668\n",
      "[26]\ttraining's binary_error: 0.26544\tvalid_1's binary_error: 0.27634\n",
      "[27]\ttraining's binary_error: 0.26482\tvalid_1's binary_error: 0.27648\n",
      "[28]\ttraining's binary_error: 0.26438\tvalid_1's binary_error: 0.27588\n",
      "[29]\ttraining's binary_error: 0.263\tvalid_1's binary_error: 0.2756\n",
      "[30]\ttraining's binary_error: 0.26246\tvalid_1's binary_error: 0.27588\n",
      "[31]\ttraining's binary_error: 0.26192\tvalid_1's binary_error: 0.27612\n",
      "[32]\ttraining's binary_error: 0.26156\tvalid_1's binary_error: 0.27622\n",
      "[33]\ttraining's binary_error: 0.2609\tvalid_1's binary_error: 0.27592\n",
      "[34]\ttraining's binary_error: 0.26052\tvalid_1's binary_error: 0.2758\n",
      "[35]\ttraining's binary_error: 0.26032\tvalid_1's binary_error: 0.27626\n",
      "[36]\ttraining's binary_error: 0.25922\tvalid_1's binary_error: 0.27622\n",
      "[37]\ttraining's binary_error: 0.2586\tvalid_1's binary_error: 0.276\n",
      "[38]\ttraining's binary_error: 0.25782\tvalid_1's binary_error: 0.27602\n",
      "[39]\ttraining's binary_error: 0.25702\tvalid_1's binary_error: 0.27608\n",
      "[40]\ttraining's binary_error: 0.25688\tvalid_1's binary_error: 0.27596\n",
      "[41]\ttraining's binary_error: 0.25636\tvalid_1's binary_error: 0.276\n",
      "[42]\ttraining's binary_error: 0.25594\tvalid_1's binary_error: 0.27622\n",
      "[43]\ttraining's binary_error: 0.25558\tvalid_1's binary_error: 0.2761\n",
      "[44]\ttraining's binary_error: 0.25568\tvalid_1's binary_error: 0.27606\n",
      "[45]\ttraining's binary_error: 0.2553\tvalid_1's binary_error: 0.27622\n",
      "[46]\ttraining's binary_error: 0.25466\tvalid_1's binary_error: 0.27606\n",
      "[47]\ttraining's binary_error: 0.25448\tvalid_1's binary_error: 0.2761\n",
      "[48]\ttraining's binary_error: 0.2537\tvalid_1's binary_error: 0.27646\n",
      "[49]\ttraining's binary_error: 0.25356\tvalid_1's binary_error: 0.27672\n",
      "[50]\ttraining's binary_error: 0.25234\tvalid_1's binary_error: 0.2766\n",
      "[51]\ttraining's binary_error: 0.25184\tvalid_1's binary_error: 0.27666\n",
      "[52]\ttraining's binary_error: 0.25124\tvalid_1's binary_error: 0.27662\n",
      "[53]\ttraining's binary_error: 0.2505\tvalid_1's binary_error: 0.27658\n",
      "[54]\ttraining's binary_error: 0.24956\tvalid_1's binary_error: 0.27702\n",
      "[55]\ttraining's binary_error: 0.2488\tvalid_1's binary_error: 0.27714\n",
      "[56]\ttraining's binary_error: 0.24784\tvalid_1's binary_error: 0.2769\n",
      "[57]\ttraining's binary_error: 0.2469\tvalid_1's binary_error: 0.27692\n",
      "[58]\ttraining's binary_error: 0.24618\tvalid_1's binary_error: 0.2767\n",
      "[59]\ttraining's binary_error: 0.24564\tvalid_1's binary_error: 0.27646\n",
      "[60]\ttraining's binary_error: 0.24524\tvalid_1's binary_error: 0.27666\n",
      "[61]\ttraining's binary_error: 0.24474\tvalid_1's binary_error: 0.27648\n",
      "[62]\ttraining's binary_error: 0.24394\tvalid_1's binary_error: 0.27648\n",
      "[63]\ttraining's binary_error: 0.24328\tvalid_1's binary_error: 0.2764\n",
      "[64]\ttraining's binary_error: 0.24226\tvalid_1's binary_error: 0.27642\n",
      "[65]\ttraining's binary_error: 0.2414\tvalid_1's binary_error: 0.2763\n",
      "[66]\ttraining's binary_error: 0.24094\tvalid_1's binary_error: 0.27642\n",
      "[67]\ttraining's binary_error: 0.24004\tvalid_1's binary_error: 0.27636\n",
      "[68]\ttraining's binary_error: 0.23938\tvalid_1's binary_error: 0.27686\n",
      "[69]\ttraining's binary_error: 0.23898\tvalid_1's binary_error: 0.27716\n",
      "[70]\ttraining's binary_error: 0.23814\tvalid_1's binary_error: 0.27728\n",
      "[71]\ttraining's binary_error: 0.23786\tvalid_1's binary_error: 0.27706\n",
      "[72]\ttraining's binary_error: 0.2371\tvalid_1's binary_error: 0.27668\n",
      "[73]\ttraining's binary_error: 0.23604\tvalid_1's binary_error: 0.27632\n",
      "[74]\ttraining's binary_error: 0.23544\tvalid_1's binary_error: 0.27642\n",
      "[75]\ttraining's binary_error: 0.23466\tvalid_1's binary_error: 0.27624\n",
      "[76]\ttraining's binary_error: 0.23356\tvalid_1's binary_error: 0.27646\n",
      "[77]\ttraining's binary_error: 0.23308\tvalid_1's binary_error: 0.27648\n",
      "[78]\ttraining's binary_error: 0.23266\tvalid_1's binary_error: 0.27666\n",
      "[79]\ttraining's binary_error: 0.23194\tvalid_1's binary_error: 0.27672\n",
      "[80]\ttraining's binary_error: 0.23144\tvalid_1's binary_error: 0.27686\n",
      "[81]\ttraining's binary_error: 0.23066\tvalid_1's binary_error: 0.27708\n",
      "[82]\ttraining's binary_error: 0.23012\tvalid_1's binary_error: 0.27738\n",
      "[83]\ttraining's binary_error: 0.2291\tvalid_1's binary_error: 0.27712\n",
      "[84]\ttraining's binary_error: 0.2283\tvalid_1's binary_error: 0.27704\n",
      "[85]\ttraining's binary_error: 0.22762\tvalid_1's binary_error: 0.277\n",
      "[86]\ttraining's binary_error: 0.22688\tvalid_1's binary_error: 0.27696\n",
      "[87]\ttraining's binary_error: 0.22634\tvalid_1's binary_error: 0.27684\n",
      "[88]\ttraining's binary_error: 0.2255\tvalid_1's binary_error: 0.27662\n",
      "[89]\ttraining's binary_error: 0.22494\tvalid_1's binary_error: 0.277\n",
      "[90]\ttraining's binary_error: 0.22402\tvalid_1's binary_error: 0.2772\n",
      "[91]\ttraining's binary_error: 0.2234\tvalid_1's binary_error: 0.2772\n",
      "[92]\ttraining's binary_error: 0.22282\tvalid_1's binary_error: 0.27708\n",
      "[93]\ttraining's binary_error: 0.2224\tvalid_1's binary_error: 0.27716\n",
      "[94]\ttraining's binary_error: 0.22158\tvalid_1's binary_error: 0.27714\n",
      "[95]\ttraining's binary_error: 0.2209\tvalid_1's binary_error: 0.2773\n",
      "[96]\ttraining's binary_error: 0.22014\tvalid_1's binary_error: 0.27742\n",
      "[97]\ttraining's binary_error: 0.21974\tvalid_1's binary_error: 0.27758\n",
      "[98]\ttraining's binary_error: 0.21872\tvalid_1's binary_error: 0.2779\n",
      "[99]\ttraining's binary_error: 0.2177\tvalid_1's binary_error: 0.2777\n",
      "[100]\ttraining's binary_error: 0.2174\tvalid_1's binary_error: 0.27782\n",
      "[101]\ttraining's binary_error: 0.21654\tvalid_1's binary_error: 0.27776\n",
      "[102]\ttraining's binary_error: 0.21602\tvalid_1's binary_error: 0.27814\n",
      "[103]\ttraining's binary_error: 0.2153\tvalid_1's binary_error: 0.2781\n",
      "[104]\ttraining's binary_error: 0.21456\tvalid_1's binary_error: 0.27822\n",
      "[105]\ttraining's binary_error: 0.21374\tvalid_1's binary_error: 0.27802\n",
      "[106]\ttraining's binary_error: 0.2132\tvalid_1's binary_error: 0.27828\n",
      "[107]\ttraining's binary_error: 0.21264\tvalid_1's binary_error: 0.2782\n",
      "[108]\ttraining's binary_error: 0.21226\tvalid_1's binary_error: 0.27806\n",
      "[109]\ttraining's binary_error: 0.21144\tvalid_1's binary_error: 0.27822\n",
      "[110]\ttraining's binary_error: 0.21078\tvalid_1's binary_error: 0.27852\n",
      "[111]\ttraining's binary_error: 0.2098\tvalid_1's binary_error: 0.2779\n",
      "[112]\ttraining's binary_error: 0.20808\tvalid_1's binary_error: 0.27788\n",
      "[113]\ttraining's binary_error: 0.20734\tvalid_1's binary_error: 0.27812\n",
      "[114]\ttraining's binary_error: 0.20674\tvalid_1's binary_error: 0.27804\n",
      "[115]\ttraining's binary_error: 0.20628\tvalid_1's binary_error: 0.2783\n",
      "[116]\ttraining's binary_error: 0.20566\tvalid_1's binary_error: 0.27812\n",
      "[117]\ttraining's binary_error: 0.205\tvalid_1's binary_error: 0.2781\n",
      "[118]\ttraining's binary_error: 0.20434\tvalid_1's binary_error: 0.27822\n",
      "[119]\ttraining's binary_error: 0.20394\tvalid_1's binary_error: 0.27808\n",
      "[120]\ttraining's binary_error: 0.20342\tvalid_1's binary_error: 0.27802\n",
      "[121]\ttraining's binary_error: 0.203\tvalid_1's binary_error: 0.27858\n",
      "[122]\ttraining's binary_error: 0.20228\tvalid_1's binary_error: 0.27858\n",
      "[123]\ttraining's binary_error: 0.20194\tvalid_1's binary_error: 0.27868\n",
      "[124]\ttraining's binary_error: 0.20114\tvalid_1's binary_error: 0.27846\n",
      "[125]\ttraining's binary_error: 0.20056\tvalid_1's binary_error: 0.27836\n",
      "[126]\ttraining's binary_error: 0.20022\tvalid_1's binary_error: 0.27856\n",
      "[127]\ttraining's binary_error: 0.19954\tvalid_1's binary_error: 0.27846\n",
      "[128]\ttraining's binary_error: 0.1989\tvalid_1's binary_error: 0.27834\n",
      "[129]\ttraining's binary_error: 0.19788\tvalid_1's binary_error: 0.27824\n",
      "[130]\ttraining's binary_error: 0.19742\tvalid_1's binary_error: 0.27784\n",
      "[131]\ttraining's binary_error: 0.19686\tvalid_1's binary_error: 0.27772\n",
      "[132]\ttraining's binary_error: 0.19574\tvalid_1's binary_error: 0.27814\n",
      "[133]\ttraining's binary_error: 0.19546\tvalid_1's binary_error: 0.27826\n",
      "[134]\ttraining's binary_error: 0.19464\tvalid_1's binary_error: 0.27774\n",
      "[135]\ttraining's binary_error: 0.194\tvalid_1's binary_error: 0.27776\n",
      "[136]\ttraining's binary_error: 0.19336\tvalid_1's binary_error: 0.27762\n",
      "[137]\ttraining's binary_error: 0.19262\tvalid_1's binary_error: 0.27746\n",
      "[138]\ttraining's binary_error: 0.19198\tvalid_1's binary_error: 0.27762\n",
      "[139]\ttraining's binary_error: 0.19142\tvalid_1's binary_error: 0.27752\n",
      "[140]\ttraining's binary_error: 0.19084\tvalid_1's binary_error: 0.27774\n",
      "[141]\ttraining's binary_error: 0.19014\tvalid_1's binary_error: 0.27796\n",
      "[142]\ttraining's binary_error: 0.18972\tvalid_1's binary_error: 0.27828\n",
      "[143]\ttraining's binary_error: 0.18906\tvalid_1's binary_error: 0.2779\n",
      "[144]\ttraining's binary_error: 0.18866\tvalid_1's binary_error: 0.27798\n",
      "[145]\ttraining's binary_error: 0.18808\tvalid_1's binary_error: 0.27784\n",
      "[146]\ttraining's binary_error: 0.18752\tvalid_1's binary_error: 0.2778\n",
      "[147]\ttraining's binary_error: 0.18708\tvalid_1's binary_error: 0.27782\n",
      "[148]\ttraining's binary_error: 0.1866\tvalid_1's binary_error: 0.2778\n",
      "[149]\ttraining's binary_error: 0.18608\tvalid_1's binary_error: 0.27756\n",
      "[150]\ttraining's binary_error: 0.1856\tvalid_1's binary_error: 0.27748\n",
      "[151]\ttraining's binary_error: 0.1848\tvalid_1's binary_error: 0.27774\n",
      "[152]\ttraining's binary_error: 0.18414\tvalid_1's binary_error: 0.2776\n",
      "[153]\ttraining's binary_error: 0.18368\tvalid_1's binary_error: 0.27738\n",
      "[154]\ttraining's binary_error: 0.18306\tvalid_1's binary_error: 0.27752\n",
      "[155]\ttraining's binary_error: 0.18242\tvalid_1's binary_error: 0.27742\n",
      "[156]\ttraining's binary_error: 0.18208\tvalid_1's binary_error: 0.2776\n",
      "[157]\ttraining's binary_error: 0.18168\tvalid_1's binary_error: 0.27772\n",
      "[158]\ttraining's binary_error: 0.18114\tvalid_1's binary_error: 0.27758\n",
      "[159]\ttraining's binary_error: 0.18082\tvalid_1's binary_error: 0.27756\n",
      "[160]\ttraining's binary_error: 0.18058\tvalid_1's binary_error: 0.27748\n",
      "[161]\ttraining's binary_error: 0.18018\tvalid_1's binary_error: 0.27726\n",
      "[162]\ttraining's binary_error: 0.1794\tvalid_1's binary_error: 0.27724\n",
      "[163]\ttraining's binary_error: 0.17882\tvalid_1's binary_error: 0.2771\n",
      "[164]\ttraining's binary_error: 0.17842\tvalid_1's binary_error: 0.27724\n",
      "[165]\ttraining's binary_error: 0.17816\tvalid_1's binary_error: 0.27742\n",
      "[166]\ttraining's binary_error: 0.17762\tvalid_1's binary_error: 0.27756\n",
      "[167]\ttraining's binary_error: 0.1773\tvalid_1's binary_error: 0.27796\n",
      "[168]\ttraining's binary_error: 0.1768\tvalid_1's binary_error: 0.27796\n",
      "[169]\ttraining's binary_error: 0.17622\tvalid_1's binary_error: 0.27784\n",
      "[170]\ttraining's binary_error: 0.17578\tvalid_1's binary_error: 0.27788\n",
      "[171]\ttraining's binary_error: 0.17536\tvalid_1's binary_error: 0.27794\n",
      "[172]\ttraining's binary_error: 0.17486\tvalid_1's binary_error: 0.27784\n",
      "[173]\ttraining's binary_error: 0.17424\tvalid_1's binary_error: 0.27782\n",
      "[174]\ttraining's binary_error: 0.1739\tvalid_1's binary_error: 0.2781\n",
      "[175]\ttraining's binary_error: 0.1734\tvalid_1's binary_error: 0.27828\n",
      "[176]\ttraining's binary_error: 0.17266\tvalid_1's binary_error: 0.2787\n",
      "[177]\ttraining's binary_error: 0.17236\tvalid_1's binary_error: 0.2789\n",
      "[178]\ttraining's binary_error: 0.17128\tvalid_1's binary_error: 0.27858\n",
      "[179]\ttraining's binary_error: 0.1708\tvalid_1's binary_error: 0.2784\n",
      "[180]\ttraining's binary_error: 0.17034\tvalid_1's binary_error: 0.27826\n",
      "[181]\ttraining's binary_error: 0.16982\tvalid_1's binary_error: 0.2784\n",
      "[182]\ttraining's binary_error: 0.16934\tvalid_1's binary_error: 0.27836\n",
      "[183]\ttraining's binary_error: 0.16888\tvalid_1's binary_error: 0.27832\n",
      "[184]\ttraining's binary_error: 0.16828\tvalid_1's binary_error: 0.27862\n",
      "[185]\ttraining's binary_error: 0.16792\tvalid_1's binary_error: 0.27888\n",
      "[186]\ttraining's binary_error: 0.16728\tvalid_1's binary_error: 0.27894\n",
      "[187]\ttraining's binary_error: 0.16666\tvalid_1's binary_error: 0.2788\n",
      "[188]\ttraining's binary_error: 0.16582\tvalid_1's binary_error: 0.27918\n",
      "[189]\ttraining's binary_error: 0.16516\tvalid_1's binary_error: 0.27912\n",
      "[190]\ttraining's binary_error: 0.1649\tvalid_1's binary_error: 0.27928\n",
      "[191]\ttraining's binary_error: 0.16416\tvalid_1's binary_error: 0.2793\n",
      "[192]\ttraining's binary_error: 0.16334\tvalid_1's binary_error: 0.27928\n",
      "[193]\ttraining's binary_error: 0.16238\tvalid_1's binary_error: 0.27952\n",
      "[194]\ttraining's binary_error: 0.16148\tvalid_1's binary_error: 0.27976\n",
      "[195]\ttraining's binary_error: 0.16114\tvalid_1's binary_error: 0.2799\n",
      "[196]\ttraining's binary_error: 0.16038\tvalid_1's binary_error: 0.27964\n",
      "[197]\ttraining's binary_error: 0.15984\tvalid_1's binary_error: 0.2795\n",
      "[198]\ttraining's binary_error: 0.15946\tvalid_1's binary_error: 0.27964\n",
      "[199]\ttraining's binary_error: 0.15886\tvalid_1's binary_error: 0.27978\n",
      "[200]\ttraining's binary_error: 0.1584\tvalid_1's binary_error: 0.28006\n",
      "[201]\ttraining's binary_error: 0.15806\tvalid_1's binary_error: 0.28016\n",
      "[202]\ttraining's binary_error: 0.15764\tvalid_1's binary_error: 0.28002\n",
      "[203]\ttraining's binary_error: 0.15706\tvalid_1's binary_error: 0.2798\n",
      "[204]\ttraining's binary_error: 0.15652\tvalid_1's binary_error: 0.27978\n",
      "[205]\ttraining's binary_error: 0.156\tvalid_1's binary_error: 0.27944\n",
      "[206]\ttraining's binary_error: 0.15578\tvalid_1's binary_error: 0.27912\n",
      "[207]\ttraining's binary_error: 0.15498\tvalid_1's binary_error: 0.27922\n",
      "[208]\ttraining's binary_error: 0.15454\tvalid_1's binary_error: 0.27912\n",
      "[209]\ttraining's binary_error: 0.15394\tvalid_1's binary_error: 0.27892\n",
      "[210]\ttraining's binary_error: 0.15322\tvalid_1's binary_error: 0.27886\n",
      "[211]\ttraining's binary_error: 0.15282\tvalid_1's binary_error: 0.27872\n",
      "[212]\ttraining's binary_error: 0.15214\tvalid_1's binary_error: 0.27884\n",
      "[213]\ttraining's binary_error: 0.15172\tvalid_1's binary_error: 0.27884\n",
      "[214]\ttraining's binary_error: 0.15106\tvalid_1's binary_error: 0.27898\n",
      "[215]\ttraining's binary_error: 0.1506\tvalid_1's binary_error: 0.2787\n",
      "[216]\ttraining's binary_error: 0.15014\tvalid_1's binary_error: 0.279\n",
      "[217]\ttraining's binary_error: 0.14952\tvalid_1's binary_error: 0.27896\n",
      "[218]\ttraining's binary_error: 0.14914\tvalid_1's binary_error: 0.27908\n",
      "[219]\ttraining's binary_error: 0.1488\tvalid_1's binary_error: 0.27912\n",
      "[220]\ttraining's binary_error: 0.1484\tvalid_1's binary_error: 0.27916\n",
      "[221]\ttraining's binary_error: 0.14782\tvalid_1's binary_error: 0.27926\n",
      "[222]\ttraining's binary_error: 0.14726\tvalid_1's binary_error: 0.27914\n",
      "[223]\ttraining's binary_error: 0.14686\tvalid_1's binary_error: 0.27906\n",
      "[224]\ttraining's binary_error: 0.14646\tvalid_1's binary_error: 0.2793\n",
      "[225]\ttraining's binary_error: 0.14602\tvalid_1's binary_error: 0.27924\n",
      "[226]\ttraining's binary_error: 0.14542\tvalid_1's binary_error: 0.2794\n",
      "[227]\ttraining's binary_error: 0.1451\tvalid_1's binary_error: 0.27944\n",
      "[228]\ttraining's binary_error: 0.14452\tvalid_1's binary_error: 0.27944\n",
      "[229]\ttraining's binary_error: 0.14412\tvalid_1's binary_error: 0.27958\n",
      "[230]\ttraining's binary_error: 0.14348\tvalid_1's binary_error: 0.27926\n",
      "[231]\ttraining's binary_error: 0.143\tvalid_1's binary_error: 0.2795\n",
      "[232]\ttraining's binary_error: 0.14278\tvalid_1's binary_error: 0.27932\n",
      "[233]\ttraining's binary_error: 0.1424\tvalid_1's binary_error: 0.27934\n",
      "[234]\ttraining's binary_error: 0.14234\tvalid_1's binary_error: 0.27934\n",
      "[235]\ttraining's binary_error: 0.1419\tvalid_1's binary_error: 0.27948\n",
      "[236]\ttraining's binary_error: 0.14104\tvalid_1's binary_error: 0.27958\n",
      "[237]\ttraining's binary_error: 0.1404\tvalid_1's binary_error: 0.27944\n",
      "[238]\ttraining's binary_error: 0.13984\tvalid_1's binary_error: 0.27952\n",
      "[239]\ttraining's binary_error: 0.1391\tvalid_1's binary_error: 0.2791\n",
      "[240]\ttraining's binary_error: 0.13856\tvalid_1's binary_error: 0.27952\n",
      "[241]\ttraining's binary_error: 0.13814\tvalid_1's binary_error: 0.27942\n",
      "[242]\ttraining's binary_error: 0.13762\tvalid_1's binary_error: 0.27968\n",
      "[243]\ttraining's binary_error: 0.13678\tvalid_1's binary_error: 0.2796\n",
      "[244]\ttraining's binary_error: 0.13616\tvalid_1's binary_error: 0.27942\n",
      "[245]\ttraining's binary_error: 0.13582\tvalid_1's binary_error: 0.27964\n",
      "[246]\ttraining's binary_error: 0.13534\tvalid_1's binary_error: 0.2799\n",
      "[247]\ttraining's binary_error: 0.1349\tvalid_1's binary_error: 0.28002\n",
      "[248]\ttraining's binary_error: 0.13458\tvalid_1's binary_error: 0.27982\n",
      "[249]\ttraining's binary_error: 0.13432\tvalid_1's binary_error: 0.27964\n",
      "[250]\ttraining's binary_error: 0.1343\tvalid_1's binary_error: 0.27938\n",
      "[251]\ttraining's binary_error: 0.13394\tvalid_1's binary_error: 0.2794\n",
      "[252]\ttraining's binary_error: 0.13344\tvalid_1's binary_error: 0.27962\n",
      "[253]\ttraining's binary_error: 0.13316\tvalid_1's binary_error: 0.27966\n",
      "[254]\ttraining's binary_error: 0.13282\tvalid_1's binary_error: 0.27968\n",
      "[255]\ttraining's binary_error: 0.13222\tvalid_1's binary_error: 0.27978\n",
      "[256]\ttraining's binary_error: 0.13166\tvalid_1's binary_error: 0.28004\n",
      "[257]\ttraining's binary_error: 0.13136\tvalid_1's binary_error: 0.27988\n",
      "[258]\ttraining's binary_error: 0.13076\tvalid_1's binary_error: 0.27954\n",
      "[259]\ttraining's binary_error: 0.13036\tvalid_1's binary_error: 0.27964\n",
      "[260]\ttraining's binary_error: 0.1295\tvalid_1's binary_error: 0.27966\n",
      "[261]\ttraining's binary_error: 0.1291\tvalid_1's binary_error: 0.27938\n",
      "[262]\ttraining's binary_error: 0.12868\tvalid_1's binary_error: 0.27964\n",
      "[263]\ttraining's binary_error: 0.12842\tvalid_1's binary_error: 0.27966\n",
      "[264]\ttraining's binary_error: 0.12784\tvalid_1's binary_error: 0.2795\n",
      "[265]\ttraining's binary_error: 0.12772\tvalid_1's binary_error: 0.27952\n",
      "[266]\ttraining's binary_error: 0.12726\tvalid_1's binary_error: 0.2796\n",
      "[267]\ttraining's binary_error: 0.12688\tvalid_1's binary_error: 0.2796\n",
      "[268]\ttraining's binary_error: 0.12642\tvalid_1's binary_error: 0.27964\n",
      "[269]\ttraining's binary_error: 0.12604\tvalid_1's binary_error: 0.27954\n",
      "[270]\ttraining's binary_error: 0.12542\tvalid_1's binary_error: 0.27944\n",
      "[271]\ttraining's binary_error: 0.1251\tvalid_1's binary_error: 0.27954\n",
      "[272]\ttraining's binary_error: 0.12484\tvalid_1's binary_error: 0.27976\n",
      "[273]\ttraining's binary_error: 0.12466\tvalid_1's binary_error: 0.2798\n",
      "[274]\ttraining's binary_error: 0.1242\tvalid_1's binary_error: 0.27956\n",
      "[275]\ttraining's binary_error: 0.12402\tvalid_1's binary_error: 0.27972\n",
      "[276]\ttraining's binary_error: 0.12356\tvalid_1's binary_error: 0.27984\n",
      "[277]\ttraining's binary_error: 0.12296\tvalid_1's binary_error: 0.28026\n",
      "[278]\ttraining's binary_error: 0.12258\tvalid_1's binary_error: 0.28026\n",
      "[279]\ttraining's binary_error: 0.12194\tvalid_1's binary_error: 0.28048\n",
      "[280]\ttraining's binary_error: 0.12152\tvalid_1's binary_error: 0.28044\n",
      "[281]\ttraining's binary_error: 0.12086\tvalid_1's binary_error: 0.28094\n",
      "[282]\ttraining's binary_error: 0.12048\tvalid_1's binary_error: 0.2809\n",
      "[283]\ttraining's binary_error: 0.11998\tvalid_1's binary_error: 0.28116\n",
      "[284]\ttraining's binary_error: 0.11968\tvalid_1's binary_error: 0.2809\n",
      "[285]\ttraining's binary_error: 0.11918\tvalid_1's binary_error: 0.28092\n",
      "[286]\ttraining's binary_error: 0.11906\tvalid_1's binary_error: 0.2808\n",
      "[287]\ttraining's binary_error: 0.11872\tvalid_1's binary_error: 0.28044\n",
      "[288]\ttraining's binary_error: 0.11806\tvalid_1's binary_error: 0.28064\n",
      "[289]\ttraining's binary_error: 0.1174\tvalid_1's binary_error: 0.28102\n",
      "[290]\ttraining's binary_error: 0.1169\tvalid_1's binary_error: 0.28112\n",
      "[291]\ttraining's binary_error: 0.1166\tvalid_1's binary_error: 0.281\n",
      "[292]\ttraining's binary_error: 0.1162\tvalid_1's binary_error: 0.28094\n",
      "[293]\ttraining's binary_error: 0.11514\tvalid_1's binary_error: 0.28098\n",
      "[294]\ttraining's binary_error: 0.115\tvalid_1's binary_error: 0.28064\n",
      "[295]\ttraining's binary_error: 0.1148\tvalid_1's binary_error: 0.28068\n",
      "[296]\ttraining's binary_error: 0.11436\tvalid_1's binary_error: 0.28078\n",
      "[297]\ttraining's binary_error: 0.11344\tvalid_1's binary_error: 0.28096\n",
      "[298]\ttraining's binary_error: 0.11306\tvalid_1's binary_error: 0.28124\n",
      "[299]\ttraining's binary_error: 0.11276\tvalid_1's binary_error: 0.2813\n",
      "[300]\ttraining's binary_error: 0.11218\tvalid_1's binary_error: 0.28142\n",
      "[301]\ttraining's binary_error: 0.11176\tvalid_1's binary_error: 0.28102\n",
      "[302]\ttraining's binary_error: 0.1115\tvalid_1's binary_error: 0.28096\n",
      "[303]\ttraining's binary_error: 0.11136\tvalid_1's binary_error: 0.28106\n",
      "[304]\ttraining's binary_error: 0.11104\tvalid_1's binary_error: 0.28106\n",
      "[305]\ttraining's binary_error: 0.11064\tvalid_1's binary_error: 0.28128\n",
      "[306]\ttraining's binary_error: 0.11058\tvalid_1's binary_error: 0.28124\n",
      "[307]\ttraining's binary_error: 0.1102\tvalid_1's binary_error: 0.2813\n",
      "[308]\ttraining's binary_error: 0.10978\tvalid_1's binary_error: 0.2811\n",
      "[309]\ttraining's binary_error: 0.10942\tvalid_1's binary_error: 0.28116\n",
      "[310]\ttraining's binary_error: 0.10862\tvalid_1's binary_error: 0.28156\n",
      "[311]\ttraining's binary_error: 0.1083\tvalid_1's binary_error: 0.28168\n",
      "[312]\ttraining's binary_error: 0.10806\tvalid_1's binary_error: 0.28182\n",
      "[313]\ttraining's binary_error: 0.10748\tvalid_1's binary_error: 0.28226\n",
      "[314]\ttraining's binary_error: 0.1072\tvalid_1's binary_error: 0.2823\n",
      "[315]\ttraining's binary_error: 0.10688\tvalid_1's binary_error: 0.28198\n",
      "[316]\ttraining's binary_error: 0.10676\tvalid_1's binary_error: 0.28206\n",
      "[317]\ttraining's binary_error: 0.10622\tvalid_1's binary_error: 0.28202\n",
      "[318]\ttraining's binary_error: 0.10564\tvalid_1's binary_error: 0.28208\n",
      "[319]\ttraining's binary_error: 0.10514\tvalid_1's binary_error: 0.28214\n",
      "[320]\ttraining's binary_error: 0.10512\tvalid_1's binary_error: 0.28202\n",
      "[321]\ttraining's binary_error: 0.10468\tvalid_1's binary_error: 0.28192\n",
      "[322]\ttraining's binary_error: 0.10414\tvalid_1's binary_error: 0.28196\n",
      "[323]\ttraining's binary_error: 0.10378\tvalid_1's binary_error: 0.28196\n",
      "[324]\ttraining's binary_error: 0.10332\tvalid_1's binary_error: 0.28178\n",
      "[325]\ttraining's binary_error: 0.10294\tvalid_1's binary_error: 0.28204\n",
      "[326]\ttraining's binary_error: 0.10262\tvalid_1's binary_error: 0.28208\n",
      "[327]\ttraining's binary_error: 0.10238\tvalid_1's binary_error: 0.2821\n",
      "[328]\ttraining's binary_error: 0.10216\tvalid_1's binary_error: 0.28236\n",
      "[329]\ttraining's binary_error: 0.10202\tvalid_1's binary_error: 0.28222\n",
      "[330]\ttraining's binary_error: 0.10188\tvalid_1's binary_error: 0.28222\n",
      "[331]\ttraining's binary_error: 0.10164\tvalid_1's binary_error: 0.28244\n",
      "[332]\ttraining's binary_error: 0.1015\tvalid_1's binary_error: 0.2823\n",
      "[333]\ttraining's binary_error: 0.10096\tvalid_1's binary_error: 0.2824\n",
      "[334]\ttraining's binary_error: 0.10058\tvalid_1's binary_error: 0.28252\n",
      "[335]\ttraining's binary_error: 0.10028\tvalid_1's binary_error: 0.2825\n",
      "[336]\ttraining's binary_error: 0.09972\tvalid_1's binary_error: 0.2824\n",
      "[337]\ttraining's binary_error: 0.09938\tvalid_1's binary_error: 0.28266\n",
      "[338]\ttraining's binary_error: 0.09922\tvalid_1's binary_error: 0.2828\n",
      "[339]\ttraining's binary_error: 0.09884\tvalid_1's binary_error: 0.2829\n",
      "[340]\ttraining's binary_error: 0.09834\tvalid_1's binary_error: 0.28276\n",
      "[341]\ttraining's binary_error: 0.0982\tvalid_1's binary_error: 0.28278\n",
      "[342]\ttraining's binary_error: 0.09762\tvalid_1's binary_error: 0.28264\n",
      "[343]\ttraining's binary_error: 0.09716\tvalid_1's binary_error: 0.28276\n",
      "[344]\ttraining's binary_error: 0.09686\tvalid_1's binary_error: 0.28234\n",
      "[345]\ttraining's binary_error: 0.09674\tvalid_1's binary_error: 0.28208\n",
      "[346]\ttraining's binary_error: 0.09654\tvalid_1's binary_error: 0.2822\n",
      "[347]\ttraining's binary_error: 0.0961\tvalid_1's binary_error: 0.28232\n",
      "[348]\ttraining's binary_error: 0.0959\tvalid_1's binary_error: 0.28248\n",
      "[349]\ttraining's binary_error: 0.0954\tvalid_1's binary_error: 0.28228\n",
      "[350]\ttraining's binary_error: 0.09524\tvalid_1's binary_error: 0.2823\n",
      "[351]\ttraining's binary_error: 0.09496\tvalid_1's binary_error: 0.2822\n",
      "[352]\ttraining's binary_error: 0.0947\tvalid_1's binary_error: 0.2826\n",
      "[353]\ttraining's binary_error: 0.09456\tvalid_1's binary_error: 0.28266\n",
      "[354]\ttraining's binary_error: 0.0944\tvalid_1's binary_error: 0.28268\n",
      "[355]\ttraining's binary_error: 0.09416\tvalid_1's binary_error: 0.28272\n",
      "[356]\ttraining's binary_error: 0.0938\tvalid_1's binary_error: 0.28238\n",
      "[357]\ttraining's binary_error: 0.09352\tvalid_1's binary_error: 0.28242\n",
      "[358]\ttraining's binary_error: 0.09322\tvalid_1's binary_error: 0.28228\n",
      "[359]\ttraining's binary_error: 0.09304\tvalid_1's binary_error: 0.28232\n",
      "[360]\ttraining's binary_error: 0.09272\tvalid_1's binary_error: 0.28228\n",
      "[361]\ttraining's binary_error: 0.09214\tvalid_1's binary_error: 0.28226\n",
      "[362]\ttraining's binary_error: 0.09206\tvalid_1's binary_error: 0.28206\n",
      "[363]\ttraining's binary_error: 0.0918\tvalid_1's binary_error: 0.28204\n",
      "[364]\ttraining's binary_error: 0.09158\tvalid_1's binary_error: 0.28196\n",
      "[365]\ttraining's binary_error: 0.09104\tvalid_1's binary_error: 0.28232\n",
      "[366]\ttraining's binary_error: 0.091\tvalid_1's binary_error: 0.28224\n",
      "[367]\ttraining's binary_error: 0.09084\tvalid_1's binary_error: 0.2823\n",
      "[368]\ttraining's binary_error: 0.0907\tvalid_1's binary_error: 0.28242\n",
      "[369]\ttraining's binary_error: 0.09062\tvalid_1's binary_error: 0.28262\n",
      "[370]\ttraining's binary_error: 0.0902\tvalid_1's binary_error: 0.28288\n",
      "[371]\ttraining's binary_error: 0.09014\tvalid_1's binary_error: 0.283\n",
      "[372]\ttraining's binary_error: 0.08984\tvalid_1's binary_error: 0.28316\n",
      "[373]\ttraining's binary_error: 0.0896\tvalid_1's binary_error: 0.28332\n",
      "[374]\ttraining's binary_error: 0.08934\tvalid_1's binary_error: 0.2831\n",
      "[375]\ttraining's binary_error: 0.089\tvalid_1's binary_error: 0.28298\n",
      "[376]\ttraining's binary_error: 0.08862\tvalid_1's binary_error: 0.28338\n",
      "[377]\ttraining's binary_error: 0.0882\tvalid_1's binary_error: 0.28342\n",
      "[378]\ttraining's binary_error: 0.08778\tvalid_1's binary_error: 0.28332\n",
      "[379]\ttraining's binary_error: 0.08738\tvalid_1's binary_error: 0.2832\n",
      "[380]\ttraining's binary_error: 0.0871\tvalid_1's binary_error: 0.28312\n",
      "[381]\ttraining's binary_error: 0.0867\tvalid_1's binary_error: 0.28284\n",
      "[382]\ttraining's binary_error: 0.08624\tvalid_1's binary_error: 0.28282\n",
      "[383]\ttraining's binary_error: 0.08602\tvalid_1's binary_error: 0.28288\n",
      "[384]\ttraining's binary_error: 0.08584\tvalid_1's binary_error: 0.28288\n",
      "[385]\ttraining's binary_error: 0.08572\tvalid_1's binary_error: 0.2831\n",
      "[386]\ttraining's binary_error: 0.0853\tvalid_1's binary_error: 0.28308\n",
      "[387]\ttraining's binary_error: 0.08476\tvalid_1's binary_error: 0.28302\n",
      "[388]\ttraining's binary_error: 0.08454\tvalid_1's binary_error: 0.28288\n",
      "[389]\ttraining's binary_error: 0.08416\tvalid_1's binary_error: 0.28274\n",
      "[390]\ttraining's binary_error: 0.08378\tvalid_1's binary_error: 0.28294\n",
      "[391]\ttraining's binary_error: 0.08372\tvalid_1's binary_error: 0.28294\n",
      "[392]\ttraining's binary_error: 0.08358\tvalid_1's binary_error: 0.2829\n",
      "[393]\ttraining's binary_error: 0.08348\tvalid_1's binary_error: 0.2829\n",
      "[394]\ttraining's binary_error: 0.08314\tvalid_1's binary_error: 0.28316\n",
      "[395]\ttraining's binary_error: 0.0829\tvalid_1's binary_error: 0.28306\n",
      "[396]\ttraining's binary_error: 0.08274\tvalid_1's binary_error: 0.28314\n",
      "[397]\ttraining's binary_error: 0.08242\tvalid_1's binary_error: 0.28304\n",
      "[398]\ttraining's binary_error: 0.08234\tvalid_1's binary_error: 0.28314\n",
      "[399]\ttraining's binary_error: 0.08204\tvalid_1's binary_error: 0.283\n",
      "[400]\ttraining's binary_error: 0.08176\tvalid_1's binary_error: 0.28294\n",
      "[401]\ttraining's binary_error: 0.0813\tvalid_1's binary_error: 0.28282\n",
      "[402]\ttraining's binary_error: 0.0808\tvalid_1's binary_error: 0.28296\n",
      "[403]\ttraining's binary_error: 0.08022\tvalid_1's binary_error: 0.28306\n",
      "[404]\ttraining's binary_error: 0.07988\tvalid_1's binary_error: 0.28322\n",
      "[405]\ttraining's binary_error: 0.07966\tvalid_1's binary_error: 0.28318\n",
      "[406]\ttraining's binary_error: 0.07958\tvalid_1's binary_error: 0.28316\n",
      "[407]\ttraining's binary_error: 0.07912\tvalid_1's binary_error: 0.28296\n",
      "[408]\ttraining's binary_error: 0.07884\tvalid_1's binary_error: 0.2826\n",
      "[409]\ttraining's binary_error: 0.07868\tvalid_1's binary_error: 0.28256\n",
      "[410]\ttraining's binary_error: 0.07838\tvalid_1's binary_error: 0.28274\n",
      "[411]\ttraining's binary_error: 0.07808\tvalid_1's binary_error: 0.28278\n",
      "[412]\ttraining's binary_error: 0.07756\tvalid_1's binary_error: 0.28296\n",
      "[413]\ttraining's binary_error: 0.07732\tvalid_1's binary_error: 0.28298\n",
      "[414]\ttraining's binary_error: 0.07662\tvalid_1's binary_error: 0.28282\n",
      "[415]\ttraining's binary_error: 0.07648\tvalid_1's binary_error: 0.2829\n",
      "[416]\ttraining's binary_error: 0.07632\tvalid_1's binary_error: 0.2829\n",
      "[417]\ttraining's binary_error: 0.07596\tvalid_1's binary_error: 0.2828\n",
      "[418]\ttraining's binary_error: 0.07566\tvalid_1's binary_error: 0.28314\n",
      "[419]\ttraining's binary_error: 0.07528\tvalid_1's binary_error: 0.2831\n",
      "[420]\ttraining's binary_error: 0.075\tvalid_1's binary_error: 0.28324\n",
      "[421]\ttraining's binary_error: 0.07494\tvalid_1's binary_error: 0.28338\n",
      "[422]\ttraining's binary_error: 0.07462\tvalid_1's binary_error: 0.2835\n",
      "[423]\ttraining's binary_error: 0.07468\tvalid_1's binary_error: 0.28332\n",
      "[424]\ttraining's binary_error: 0.07454\tvalid_1's binary_error: 0.28336\n",
      "[425]\ttraining's binary_error: 0.07432\tvalid_1's binary_error: 0.28368\n",
      "[426]\ttraining's binary_error: 0.0741\tvalid_1's binary_error: 0.28384\n",
      "[427]\ttraining's binary_error: 0.07388\tvalid_1's binary_error: 0.28354\n",
      "[428]\ttraining's binary_error: 0.07356\tvalid_1's binary_error: 0.2836\n",
      "[429]\ttraining's binary_error: 0.07336\tvalid_1's binary_error: 0.2836\n",
      "[430]\ttraining's binary_error: 0.07302\tvalid_1's binary_error: 0.2837\n",
      "[431]\ttraining's binary_error: 0.07258\tvalid_1's binary_error: 0.28342\n",
      "[432]\ttraining's binary_error: 0.07218\tvalid_1's binary_error: 0.28364\n",
      "[433]\ttraining's binary_error: 0.0719\tvalid_1's binary_error: 0.28354\n",
      "[434]\ttraining's binary_error: 0.07138\tvalid_1's binary_error: 0.28352\n",
      "[435]\ttraining's binary_error: 0.07124\tvalid_1's binary_error: 0.28364\n",
      "[436]\ttraining's binary_error: 0.07116\tvalid_1's binary_error: 0.28352\n",
      "[437]\ttraining's binary_error: 0.071\tvalid_1's binary_error: 0.2835\n",
      "[438]\ttraining's binary_error: 0.07086\tvalid_1's binary_error: 0.2834\n",
      "[439]\ttraining's binary_error: 0.07076\tvalid_1's binary_error: 0.2833\n",
      "[440]\ttraining's binary_error: 0.07066\tvalid_1's binary_error: 0.28344\n",
      "[441]\ttraining's binary_error: 0.0704\tvalid_1's binary_error: 0.28322\n",
      "[442]\ttraining's binary_error: 0.07026\tvalid_1's binary_error: 0.28308\n",
      "[443]\ttraining's binary_error: 0.06986\tvalid_1's binary_error: 0.28298\n",
      "[444]\ttraining's binary_error: 0.0695\tvalid_1's binary_error: 0.28318\n",
      "[445]\ttraining's binary_error: 0.06912\tvalid_1's binary_error: 0.28336\n",
      "[446]\ttraining's binary_error: 0.06894\tvalid_1's binary_error: 0.2837\n",
      "[447]\ttraining's binary_error: 0.06864\tvalid_1's binary_error: 0.28374\n",
      "[448]\ttraining's binary_error: 0.0684\tvalid_1's binary_error: 0.28354\n",
      "[449]\ttraining's binary_error: 0.06796\tvalid_1's binary_error: 0.2834\n",
      "[450]\ttraining's binary_error: 0.06786\tvalid_1's binary_error: 0.2833\n",
      "[451]\ttraining's binary_error: 0.0676\tvalid_1's binary_error: 0.28322\n",
      "[452]\ttraining's binary_error: 0.0673\tvalid_1's binary_error: 0.28312\n",
      "[453]\ttraining's binary_error: 0.06716\tvalid_1's binary_error: 0.28324\n",
      "[454]\ttraining's binary_error: 0.0671\tvalid_1's binary_error: 0.28346\n",
      "[455]\ttraining's binary_error: 0.0668\tvalid_1's binary_error: 0.28374\n",
      "[456]\ttraining's binary_error: 0.06652\tvalid_1's binary_error: 0.28378\n",
      "[457]\ttraining's binary_error: 0.06632\tvalid_1's binary_error: 0.28388\n",
      "[458]\ttraining's binary_error: 0.06638\tvalid_1's binary_error: 0.28384\n",
      "[459]\ttraining's binary_error: 0.0662\tvalid_1's binary_error: 0.284\n",
      "[460]\ttraining's binary_error: 0.06608\tvalid_1's binary_error: 0.28404\n",
      "[461]\ttraining's binary_error: 0.06602\tvalid_1's binary_error: 0.28396\n",
      "[462]\ttraining's binary_error: 0.06564\tvalid_1's binary_error: 0.284\n",
      "[463]\ttraining's binary_error: 0.06562\tvalid_1's binary_error: 0.28396\n",
      "[464]\ttraining's binary_error: 0.06532\tvalid_1's binary_error: 0.28384\n",
      "[465]\ttraining's binary_error: 0.06516\tvalid_1's binary_error: 0.28392\n",
      "[466]\ttraining's binary_error: 0.0648\tvalid_1's binary_error: 0.28402\n",
      "[467]\ttraining's binary_error: 0.06434\tvalid_1's binary_error: 0.28412\n",
      "[468]\ttraining's binary_error: 0.06436\tvalid_1's binary_error: 0.28418\n",
      "[469]\ttraining's binary_error: 0.0643\tvalid_1's binary_error: 0.28426\n",
      "[470]\ttraining's binary_error: 0.06416\tvalid_1's binary_error: 0.2839\n",
      "[471]\ttraining's binary_error: 0.06406\tvalid_1's binary_error: 0.28356\n",
      "[472]\ttraining's binary_error: 0.06358\tvalid_1's binary_error: 0.28318\n",
      "[473]\ttraining's binary_error: 0.06352\tvalid_1's binary_error: 0.28354\n",
      "[474]\ttraining's binary_error: 0.06344\tvalid_1's binary_error: 0.28342\n",
      "[475]\ttraining's binary_error: 0.063\tvalid_1's binary_error: 0.28366\n",
      "[476]\ttraining's binary_error: 0.063\tvalid_1's binary_error: 0.28366\n",
      "[477]\ttraining's binary_error: 0.06278\tvalid_1's binary_error: 0.28352\n",
      "[478]\ttraining's binary_error: 0.0625\tvalid_1's binary_error: 0.28362\n",
      "[479]\ttraining's binary_error: 0.06228\tvalid_1's binary_error: 0.2835\n",
      "[480]\ttraining's binary_error: 0.06172\tvalid_1's binary_error: 0.28334\n",
      "[481]\ttraining's binary_error: 0.0613\tvalid_1's binary_error: 0.28344\n",
      "[482]\ttraining's binary_error: 0.06098\tvalid_1's binary_error: 0.28328\n",
      "[483]\ttraining's binary_error: 0.0607\tvalid_1's binary_error: 0.28278\n",
      "[484]\ttraining's binary_error: 0.06048\tvalid_1's binary_error: 0.28328\n",
      "[485]\ttraining's binary_error: 0.06026\tvalid_1's binary_error: 0.28314\n",
      "[486]\ttraining's binary_error: 0.05994\tvalid_1's binary_error: 0.28316\n",
      "[487]\ttraining's binary_error: 0.0597\tvalid_1's binary_error: 0.28318\n",
      "[488]\ttraining's binary_error: 0.05944\tvalid_1's binary_error: 0.28334\n",
      "[489]\ttraining's binary_error: 0.05904\tvalid_1's binary_error: 0.2836\n",
      "[490]\ttraining's binary_error: 0.05884\tvalid_1's binary_error: 0.28348\n",
      "[491]\ttraining's binary_error: 0.05872\tvalid_1's binary_error: 0.28368\n",
      "[492]\ttraining's binary_error: 0.05836\tvalid_1's binary_error: 0.28396\n",
      "[493]\ttraining's binary_error: 0.05824\tvalid_1's binary_error: 0.28414\n",
      "[494]\ttraining's binary_error: 0.05808\tvalid_1's binary_error: 0.28424\n",
      "[495]\ttraining's binary_error: 0.0578\tvalid_1's binary_error: 0.28414\n",
      "[496]\ttraining's binary_error: 0.0573\tvalid_1's binary_error: 0.28392\n",
      "[497]\ttraining's binary_error: 0.05706\tvalid_1's binary_error: 0.28396\n",
      "[498]\ttraining's binary_error: 0.05668\tvalid_1's binary_error: 0.2841\n",
      "[499]\ttraining's binary_error: 0.05646\tvalid_1's binary_error: 0.28404\n",
      "[500]\ttraining's binary_error: 0.0561\tvalid_1's binary_error: 0.28412\n",
      "[501]\ttraining's binary_error: 0.0558\tvalid_1's binary_error: 0.28378\n",
      "[502]\ttraining's binary_error: 0.05568\tvalid_1's binary_error: 0.28392\n",
      "[503]\ttraining's binary_error: 0.05572\tvalid_1's binary_error: 0.2837\n",
      "[504]\ttraining's binary_error: 0.05556\tvalid_1's binary_error: 0.2841\n",
      "[505]\ttraining's binary_error: 0.05554\tvalid_1's binary_error: 0.28398\n",
      "[506]\ttraining's binary_error: 0.0553\tvalid_1's binary_error: 0.28414\n",
      "[507]\ttraining's binary_error: 0.05524\tvalid_1's binary_error: 0.28424\n",
      "[508]\ttraining's binary_error: 0.05492\tvalid_1's binary_error: 0.28432\n",
      "[509]\ttraining's binary_error: 0.0547\tvalid_1's binary_error: 0.28462\n",
      "[510]\ttraining's binary_error: 0.05454\tvalid_1's binary_error: 0.2846\n",
      "[511]\ttraining's binary_error: 0.0544\tvalid_1's binary_error: 0.28436\n",
      "[512]\ttraining's binary_error: 0.05432\tvalid_1's binary_error: 0.28438\n",
      "[513]\ttraining's binary_error: 0.05422\tvalid_1's binary_error: 0.28448\n",
      "[514]\ttraining's binary_error: 0.054\tvalid_1's binary_error: 0.28472\n",
      "[515]\ttraining's binary_error: 0.05404\tvalid_1's binary_error: 0.28472\n",
      "[516]\ttraining's binary_error: 0.05378\tvalid_1's binary_error: 0.28442\n",
      "[517]\ttraining's binary_error: 0.05364\tvalid_1's binary_error: 0.28434\n",
      "[518]\ttraining's binary_error: 0.05352\tvalid_1's binary_error: 0.28434\n",
      "[519]\ttraining's binary_error: 0.05338\tvalid_1's binary_error: 0.28442\n",
      "[520]\ttraining's binary_error: 0.0531\tvalid_1's binary_error: 0.2842\n",
      "[521]\ttraining's binary_error: 0.05296\tvalid_1's binary_error: 0.28412\n",
      "[522]\ttraining's binary_error: 0.05266\tvalid_1's binary_error: 0.28418\n",
      "[523]\ttraining's binary_error: 0.05248\tvalid_1's binary_error: 0.28434\n",
      "[524]\ttraining's binary_error: 0.05242\tvalid_1's binary_error: 0.28424\n",
      "[525]\ttraining's binary_error: 0.0522\tvalid_1's binary_error: 0.28428\n",
      "[526]\ttraining's binary_error: 0.05196\tvalid_1's binary_error: 0.28446\n",
      "[527]\ttraining's binary_error: 0.05178\tvalid_1's binary_error: 0.28438\n",
      "[528]\ttraining's binary_error: 0.05148\tvalid_1's binary_error: 0.28436\n",
      "[529]\ttraining's binary_error: 0.05142\tvalid_1's binary_error: 0.28436\n",
      "[530]\ttraining's binary_error: 0.05118\tvalid_1's binary_error: 0.28434\n",
      "[531]\ttraining's binary_error: 0.051\tvalid_1's binary_error: 0.28432\n",
      "[532]\ttraining's binary_error: 0.05088\tvalid_1's binary_error: 0.2841\n",
      "[533]\ttraining's binary_error: 0.05084\tvalid_1's binary_error: 0.28422\n",
      "[534]\ttraining's binary_error: 0.05066\tvalid_1's binary_error: 0.28416\n",
      "[535]\ttraining's binary_error: 0.0506\tvalid_1's binary_error: 0.2842\n",
      "[536]\ttraining's binary_error: 0.05022\tvalid_1's binary_error: 0.2841\n",
      "[537]\ttraining's binary_error: 0.05006\tvalid_1's binary_error: 0.28414\n",
      "[538]\ttraining's binary_error: 0.0498\tvalid_1's binary_error: 0.28418\n",
      "[539]\ttraining's binary_error: 0.04968\tvalid_1's binary_error: 0.28422\n",
      "[540]\ttraining's binary_error: 0.04964\tvalid_1's binary_error: 0.28434\n",
      "[541]\ttraining's binary_error: 0.0495\tvalid_1's binary_error: 0.28432\n",
      "[542]\ttraining's binary_error: 0.0493\tvalid_1's binary_error: 0.28426\n",
      "[543]\ttraining's binary_error: 0.0491\tvalid_1's binary_error: 0.28432\n",
      "[544]\ttraining's binary_error: 0.04888\tvalid_1's binary_error: 0.284\n",
      "[545]\ttraining's binary_error: 0.04868\tvalid_1's binary_error: 0.2836\n",
      "[546]\ttraining's binary_error: 0.04856\tvalid_1's binary_error: 0.2837\n",
      "[547]\ttraining's binary_error: 0.04838\tvalid_1's binary_error: 0.28384\n",
      "[548]\ttraining's binary_error: 0.04812\tvalid_1's binary_error: 0.2837\n",
      "[549]\ttraining's binary_error: 0.04796\tvalid_1's binary_error: 0.28358\n",
      "[550]\ttraining's binary_error: 0.04796\tvalid_1's binary_error: 0.2837\n",
      "[551]\ttraining's binary_error: 0.04786\tvalid_1's binary_error: 0.2841\n",
      "[552]\ttraining's binary_error: 0.04764\tvalid_1's binary_error: 0.28414\n",
      "[553]\ttraining's binary_error: 0.04752\tvalid_1's binary_error: 0.2843\n",
      "[554]\ttraining's binary_error: 0.04724\tvalid_1's binary_error: 0.28426\n",
      "[555]\ttraining's binary_error: 0.04684\tvalid_1's binary_error: 0.2846\n",
      "[556]\ttraining's binary_error: 0.04668\tvalid_1's binary_error: 0.28454\n",
      "[557]\ttraining's binary_error: 0.04658\tvalid_1's binary_error: 0.2843\n",
      "[558]\ttraining's binary_error: 0.0464\tvalid_1's binary_error: 0.28432\n",
      "[559]\ttraining's binary_error: 0.04628\tvalid_1's binary_error: 0.28422\n",
      "[560]\ttraining's binary_error: 0.04618\tvalid_1's binary_error: 0.28426\n",
      "[561]\ttraining's binary_error: 0.04612\tvalid_1's binary_error: 0.28422\n",
      "[562]\ttraining's binary_error: 0.04594\tvalid_1's binary_error: 0.28446\n",
      "[563]\ttraining's binary_error: 0.0456\tvalid_1's binary_error: 0.28438\n",
      "[564]\ttraining's binary_error: 0.0456\tvalid_1's binary_error: 0.28412\n",
      "[565]\ttraining's binary_error: 0.04538\tvalid_1's binary_error: 0.28402\n",
      "[566]\ttraining's binary_error: 0.0453\tvalid_1's binary_error: 0.28404\n",
      "[567]\ttraining's binary_error: 0.0451\tvalid_1's binary_error: 0.28398\n",
      "[568]\ttraining's binary_error: 0.04496\tvalid_1's binary_error: 0.284\n",
      "[569]\ttraining's binary_error: 0.0447\tvalid_1's binary_error: 0.284\n",
      "[570]\ttraining's binary_error: 0.0447\tvalid_1's binary_error: 0.28392\n",
      "[571]\ttraining's binary_error: 0.04452\tvalid_1's binary_error: 0.28422\n",
      "[572]\ttraining's binary_error: 0.0443\tvalid_1's binary_error: 0.2842\n",
      "[573]\ttraining's binary_error: 0.04404\tvalid_1's binary_error: 0.28424\n",
      "[574]\ttraining's binary_error: 0.04388\tvalid_1's binary_error: 0.28412\n",
      "[575]\ttraining's binary_error: 0.04372\tvalid_1's binary_error: 0.2842\n",
      "[576]\ttraining's binary_error: 0.0435\tvalid_1's binary_error: 0.28444\n",
      "[577]\ttraining's binary_error: 0.0434\tvalid_1's binary_error: 0.2844\n",
      "[578]\ttraining's binary_error: 0.04308\tvalid_1's binary_error: 0.284\n",
      "[579]\ttraining's binary_error: 0.04294\tvalid_1's binary_error: 0.28408\n",
      "[580]\ttraining's binary_error: 0.04278\tvalid_1's binary_error: 0.28408\n",
      "[581]\ttraining's binary_error: 0.04256\tvalid_1's binary_error: 0.28432\n",
      "[582]\ttraining's binary_error: 0.04214\tvalid_1's binary_error: 0.28448\n",
      "[583]\ttraining's binary_error: 0.04178\tvalid_1's binary_error: 0.28458\n",
      "[584]\ttraining's binary_error: 0.04154\tvalid_1's binary_error: 0.28436\n",
      "[585]\ttraining's binary_error: 0.04148\tvalid_1's binary_error: 0.28434\n",
      "[586]\ttraining's binary_error: 0.04128\tvalid_1's binary_error: 0.2845\n",
      "[587]\ttraining's binary_error: 0.04106\tvalid_1's binary_error: 0.28452\n",
      "[588]\ttraining's binary_error: 0.04104\tvalid_1's binary_error: 0.28456\n",
      "[589]\ttraining's binary_error: 0.04098\tvalid_1's binary_error: 0.2847\n",
      "[590]\ttraining's binary_error: 0.04094\tvalid_1's binary_error: 0.28468\n",
      "[591]\ttraining's binary_error: 0.04088\tvalid_1's binary_error: 0.28478\n",
      "[592]\ttraining's binary_error: 0.04062\tvalid_1's binary_error: 0.28492\n",
      "[593]\ttraining's binary_error: 0.0404\tvalid_1's binary_error: 0.2851\n",
      "[594]\ttraining's binary_error: 0.04028\tvalid_1's binary_error: 0.28508\n",
      "[595]\ttraining's binary_error: 0.04006\tvalid_1's binary_error: 0.28516\n",
      "[596]\ttraining's binary_error: 0.0402\tvalid_1's binary_error: 0.28514\n",
      "[597]\ttraining's binary_error: 0.04022\tvalid_1's binary_error: 0.28532\n",
      "[598]\ttraining's binary_error: 0.0402\tvalid_1's binary_error: 0.28532\n",
      "[599]\ttraining's binary_error: 0.04004\tvalid_1's binary_error: 0.28552\n",
      "[600]\ttraining's binary_error: 0.04\tvalid_1's binary_error: 0.28562\n",
      "[601]\ttraining's binary_error: 0.03994\tvalid_1's binary_error: 0.28572\n",
      "[602]\ttraining's binary_error: 0.03988\tvalid_1's binary_error: 0.28582\n",
      "[603]\ttraining's binary_error: 0.03972\tvalid_1's binary_error: 0.28584\n",
      "[604]\ttraining's binary_error: 0.03942\tvalid_1's binary_error: 0.28578\n",
      "[605]\ttraining's binary_error: 0.03914\tvalid_1's binary_error: 0.28602\n",
      "[606]\ttraining's binary_error: 0.03896\tvalid_1's binary_error: 0.28594\n",
      "[607]\ttraining's binary_error: 0.03886\tvalid_1's binary_error: 0.28582\n",
      "[608]\ttraining's binary_error: 0.0386\tvalid_1's binary_error: 0.28562\n",
      "[609]\ttraining's binary_error: 0.03852\tvalid_1's binary_error: 0.28588\n",
      "[610]\ttraining's binary_error: 0.03834\tvalid_1's binary_error: 0.28602\n",
      "[611]\ttraining's binary_error: 0.03816\tvalid_1's binary_error: 0.28608\n",
      "[612]\ttraining's binary_error: 0.03804\tvalid_1's binary_error: 0.28586\n",
      "[613]\ttraining's binary_error: 0.03772\tvalid_1's binary_error: 0.28594\n",
      "[614]\ttraining's binary_error: 0.0376\tvalid_1's binary_error: 0.28596\n",
      "[615]\ttraining's binary_error: 0.03754\tvalid_1's binary_error: 0.28618\n",
      "[616]\ttraining's binary_error: 0.03736\tvalid_1's binary_error: 0.28616\n",
      "[617]\ttraining's binary_error: 0.03728\tvalid_1's binary_error: 0.28608\n",
      "[618]\ttraining's binary_error: 0.03716\tvalid_1's binary_error: 0.2858\n",
      "[619]\ttraining's binary_error: 0.03702\tvalid_1's binary_error: 0.286\n",
      "[620]\ttraining's binary_error: 0.03686\tvalid_1's binary_error: 0.28584\n",
      "[621]\ttraining's binary_error: 0.03654\tvalid_1's binary_error: 0.28562\n",
      "[622]\ttraining's binary_error: 0.0364\tvalid_1's binary_error: 0.28576\n",
      "[623]\ttraining's binary_error: 0.03638\tvalid_1's binary_error: 0.286\n",
      "[624]\ttraining's binary_error: 0.03628\tvalid_1's binary_error: 0.28606\n",
      "[625]\ttraining's binary_error: 0.03608\tvalid_1's binary_error: 0.28592\n",
      "[626]\ttraining's binary_error: 0.03586\tvalid_1's binary_error: 0.28586\n",
      "[627]\ttraining's binary_error: 0.03574\tvalid_1's binary_error: 0.28584\n",
      "[628]\ttraining's binary_error: 0.03562\tvalid_1's binary_error: 0.28582\n",
      "[629]\ttraining's binary_error: 0.0356\tvalid_1's binary_error: 0.28568\n",
      "[630]\ttraining's binary_error: 0.0355\tvalid_1's binary_error: 0.2858\n",
      "[631]\ttraining's binary_error: 0.03546\tvalid_1's binary_error: 0.28568\n",
      "[632]\ttraining's binary_error: 0.03524\tvalid_1's binary_error: 0.28548\n",
      "[633]\ttraining's binary_error: 0.03508\tvalid_1's binary_error: 0.28554\n",
      "[634]\ttraining's binary_error: 0.03492\tvalid_1's binary_error: 0.2855\n",
      "[635]\ttraining's binary_error: 0.03474\tvalid_1's binary_error: 0.28542\n",
      "[636]\ttraining's binary_error: 0.03468\tvalid_1's binary_error: 0.28558\n",
      "[637]\ttraining's binary_error: 0.0346\tvalid_1's binary_error: 0.28572\n",
      "[638]\ttraining's binary_error: 0.03456\tvalid_1's binary_error: 0.2857\n",
      "[639]\ttraining's binary_error: 0.03446\tvalid_1's binary_error: 0.2857\n",
      "[640]\ttraining's binary_error: 0.03442\tvalid_1's binary_error: 0.28578\n",
      "[641]\ttraining's binary_error: 0.03438\tvalid_1's binary_error: 0.28586\n",
      "[642]\ttraining's binary_error: 0.03436\tvalid_1's binary_error: 0.286\n",
      "[643]\ttraining's binary_error: 0.03424\tvalid_1's binary_error: 0.2859\n",
      "[644]\ttraining's binary_error: 0.03414\tvalid_1's binary_error: 0.28574\n",
      "[645]\ttraining's binary_error: 0.03402\tvalid_1's binary_error: 0.2856\n",
      "[646]\ttraining's binary_error: 0.03392\tvalid_1's binary_error: 0.2855\n",
      "[647]\ttraining's binary_error: 0.03374\tvalid_1's binary_error: 0.2859\n",
      "[648]\ttraining's binary_error: 0.03356\tvalid_1's binary_error: 0.28612\n",
      "[649]\ttraining's binary_error: 0.03332\tvalid_1's binary_error: 0.28616\n",
      "[650]\ttraining's binary_error: 0.03316\tvalid_1's binary_error: 0.28598\n",
      "[651]\ttraining's binary_error: 0.0331\tvalid_1's binary_error: 0.2859\n",
      "[652]\ttraining's binary_error: 0.03302\tvalid_1's binary_error: 0.28588\n",
      "[653]\ttraining's binary_error: 0.0329\tvalid_1's binary_error: 0.28578\n",
      "[654]\ttraining's binary_error: 0.03288\tvalid_1's binary_error: 0.28602\n",
      "[655]\ttraining's binary_error: 0.03276\tvalid_1's binary_error: 0.28596\n",
      "[656]\ttraining's binary_error: 0.03252\tvalid_1's binary_error: 0.28624\n",
      "[657]\ttraining's binary_error: 0.03228\tvalid_1's binary_error: 0.28626\n",
      "[658]\ttraining's binary_error: 0.03218\tvalid_1's binary_error: 0.28636\n",
      "[659]\ttraining's binary_error: 0.03214\tvalid_1's binary_error: 0.28636\n",
      "[660]\ttraining's binary_error: 0.03184\tvalid_1's binary_error: 0.2864\n",
      "[661]\ttraining's binary_error: 0.0318\tvalid_1's binary_error: 0.2863\n",
      "[662]\ttraining's binary_error: 0.03178\tvalid_1's binary_error: 0.28624\n",
      "[663]\ttraining's binary_error: 0.03162\tvalid_1's binary_error: 0.28644\n",
      "[664]\ttraining's binary_error: 0.03152\tvalid_1's binary_error: 0.28638\n",
      "[665]\ttraining's binary_error: 0.03142\tvalid_1's binary_error: 0.28648\n",
      "[666]\ttraining's binary_error: 0.03152\tvalid_1's binary_error: 0.28624\n",
      "[667]\ttraining's binary_error: 0.03152\tvalid_1's binary_error: 0.28606\n",
      "[668]\ttraining's binary_error: 0.03134\tvalid_1's binary_error: 0.28598\n",
      "[669]\ttraining's binary_error: 0.03116\tvalid_1's binary_error: 0.28578\n",
      "[670]\ttraining's binary_error: 0.03106\tvalid_1's binary_error: 0.28582\n",
      "[671]\ttraining's binary_error: 0.0309\tvalid_1's binary_error: 0.28578\n",
      "[672]\ttraining's binary_error: 0.03084\tvalid_1's binary_error: 0.28586\n",
      "[673]\ttraining's binary_error: 0.0307\tvalid_1's binary_error: 0.28578\n",
      "[674]\ttraining's binary_error: 0.0306\tvalid_1's binary_error: 0.28572\n",
      "[675]\ttraining's binary_error: 0.03048\tvalid_1's binary_error: 0.28596\n",
      "[676]\ttraining's binary_error: 0.03036\tvalid_1's binary_error: 0.28624\n",
      "[677]\ttraining's binary_error: 0.03036\tvalid_1's binary_error: 0.28636\n",
      "[678]\ttraining's binary_error: 0.03026\tvalid_1's binary_error: 0.28634\n",
      "[679]\ttraining's binary_error: 0.0301\tvalid_1's binary_error: 0.2865\n",
      "[680]\ttraining's binary_error: 0.0299\tvalid_1's binary_error: 0.28658\n",
      "[681]\ttraining's binary_error: 0.0297\tvalid_1's binary_error: 0.28656\n",
      "[682]\ttraining's binary_error: 0.0296\tvalid_1's binary_error: 0.28662\n",
      "[683]\ttraining's binary_error: 0.0295\tvalid_1's binary_error: 0.28662\n",
      "[684]\ttraining's binary_error: 0.0295\tvalid_1's binary_error: 0.28658\n",
      "[685]\ttraining's binary_error: 0.02942\tvalid_1's binary_error: 0.287\n",
      "[686]\ttraining's binary_error: 0.029\tvalid_1's binary_error: 0.2867\n",
      "[687]\ttraining's binary_error: 0.02886\tvalid_1's binary_error: 0.28682\n",
      "[688]\ttraining's binary_error: 0.02878\tvalid_1's binary_error: 0.28676\n",
      "[689]\ttraining's binary_error: 0.02864\tvalid_1's binary_error: 0.28692\n",
      "[690]\ttraining's binary_error: 0.02854\tvalid_1's binary_error: 0.28698\n",
      "[691]\ttraining's binary_error: 0.0284\tvalid_1's binary_error: 0.28706\n",
      "[692]\ttraining's binary_error: 0.02834\tvalid_1's binary_error: 0.28732\n",
      "[693]\ttraining's binary_error: 0.02824\tvalid_1's binary_error: 0.28724\n",
      "[694]\ttraining's binary_error: 0.02818\tvalid_1's binary_error: 0.28716\n",
      "[695]\ttraining's binary_error: 0.02788\tvalid_1's binary_error: 0.28718\n",
      "[696]\ttraining's binary_error: 0.02778\tvalid_1's binary_error: 0.28702\n",
      "[697]\ttraining's binary_error: 0.02768\tvalid_1's binary_error: 0.28702\n",
      "[698]\ttraining's binary_error: 0.02746\tvalid_1's binary_error: 0.2868\n",
      "[699]\ttraining's binary_error: 0.02728\tvalid_1's binary_error: 0.28664\n",
      "[700]\ttraining's binary_error: 0.02726\tvalid_1's binary_error: 0.28662\n",
      "[701]\ttraining's binary_error: 0.0271\tvalid_1's binary_error: 0.28678\n",
      "[702]\ttraining's binary_error: 0.02708\tvalid_1's binary_error: 0.28674\n",
      "[703]\ttraining's binary_error: 0.02698\tvalid_1's binary_error: 0.28652\n",
      "[704]\ttraining's binary_error: 0.02692\tvalid_1's binary_error: 0.28664\n",
      "[705]\ttraining's binary_error: 0.02686\tvalid_1's binary_error: 0.28682\n",
      "[706]\ttraining's binary_error: 0.02664\tvalid_1's binary_error: 0.28706\n",
      "[707]\ttraining's binary_error: 0.02652\tvalid_1's binary_error: 0.28676\n",
      "[708]\ttraining's binary_error: 0.02648\tvalid_1's binary_error: 0.28654\n",
      "[709]\ttraining's binary_error: 0.02634\tvalid_1's binary_error: 0.28644\n",
      "[710]\ttraining's binary_error: 0.02626\tvalid_1's binary_error: 0.2865\n",
      "[711]\ttraining's binary_error: 0.02614\tvalid_1's binary_error: 0.2868\n",
      "[712]\ttraining's binary_error: 0.02604\tvalid_1's binary_error: 0.28664\n",
      "[713]\ttraining's binary_error: 0.02598\tvalid_1's binary_error: 0.28664\n",
      "[714]\ttraining's binary_error: 0.02582\tvalid_1's binary_error: 0.28662\n",
      "[715]\ttraining's binary_error: 0.02566\tvalid_1's binary_error: 0.28676\n",
      "[716]\ttraining's binary_error: 0.02554\tvalid_1's binary_error: 0.2868\n",
      "[717]\ttraining's binary_error: 0.02536\tvalid_1's binary_error: 0.28662\n",
      "[718]\ttraining's binary_error: 0.0253\tvalid_1's binary_error: 0.28682\n",
      "[719]\ttraining's binary_error: 0.02528\tvalid_1's binary_error: 0.28694\n",
      "[720]\ttraining's binary_error: 0.02522\tvalid_1's binary_error: 0.28696\n",
      "[721]\ttraining's binary_error: 0.0252\tvalid_1's binary_error: 0.28688\n",
      "[722]\ttraining's binary_error: 0.02512\tvalid_1's binary_error: 0.28684\n",
      "[723]\ttraining's binary_error: 0.02488\tvalid_1's binary_error: 0.28708\n",
      "[724]\ttraining's binary_error: 0.02482\tvalid_1's binary_error: 0.28698\n",
      "[725]\ttraining's binary_error: 0.02474\tvalid_1's binary_error: 0.28694\n",
      "[726]\ttraining's binary_error: 0.02466\tvalid_1's binary_error: 0.28698\n",
      "[727]\ttraining's binary_error: 0.02462\tvalid_1's binary_error: 0.28688\n",
      "[728]\ttraining's binary_error: 0.0246\tvalid_1's binary_error: 0.28686\n",
      "[729]\ttraining's binary_error: 0.02438\tvalid_1's binary_error: 0.2869\n",
      "[730]\ttraining's binary_error: 0.02414\tvalid_1's binary_error: 0.28688\n",
      "[731]\ttraining's binary_error: 0.0241\tvalid_1's binary_error: 0.28676\n",
      "[732]\ttraining's binary_error: 0.02402\tvalid_1's binary_error: 0.28678\n",
      "[733]\ttraining's binary_error: 0.024\tvalid_1's binary_error: 0.2867\n",
      "[734]\ttraining's binary_error: 0.02394\tvalid_1's binary_error: 0.28672\n",
      "[735]\ttraining's binary_error: 0.02388\tvalid_1's binary_error: 0.28662\n",
      "[736]\ttraining's binary_error: 0.02386\tvalid_1's binary_error: 0.28654\n",
      "[737]\ttraining's binary_error: 0.02394\tvalid_1's binary_error: 0.28646\n",
      "[738]\ttraining's binary_error: 0.02382\tvalid_1's binary_error: 0.28634\n",
      "[739]\ttraining's binary_error: 0.0237\tvalid_1's binary_error: 0.28646\n",
      "[740]\ttraining's binary_error: 0.02362\tvalid_1's binary_error: 0.28636\n",
      "[741]\ttraining's binary_error: 0.0235\tvalid_1's binary_error: 0.28636\n",
      "[742]\ttraining's binary_error: 0.0234\tvalid_1's binary_error: 0.28648\n",
      "[743]\ttraining's binary_error: 0.02338\tvalid_1's binary_error: 0.2864\n",
      "[744]\ttraining's binary_error: 0.02338\tvalid_1's binary_error: 0.28614\n",
      "[745]\ttraining's binary_error: 0.02326\tvalid_1's binary_error: 0.28612\n",
      "[746]\ttraining's binary_error: 0.02316\tvalid_1's binary_error: 0.28618\n",
      "[747]\ttraining's binary_error: 0.02304\tvalid_1's binary_error: 0.28604\n",
      "[748]\ttraining's binary_error: 0.0229\tvalid_1's binary_error: 0.28604\n",
      "[749]\ttraining's binary_error: 0.02274\tvalid_1's binary_error: 0.2862\n",
      "[750]\ttraining's binary_error: 0.02264\tvalid_1's binary_error: 0.2861\n",
      "[751]\ttraining's binary_error: 0.02258\tvalid_1's binary_error: 0.2863\n",
      "[752]\ttraining's binary_error: 0.02248\tvalid_1's binary_error: 0.28634\n",
      "[753]\ttraining's binary_error: 0.02242\tvalid_1's binary_error: 0.28626\n",
      "[754]\ttraining's binary_error: 0.0224\tvalid_1's binary_error: 0.28626\n",
      "[755]\ttraining's binary_error: 0.02242\tvalid_1's binary_error: 0.28622\n",
      "[756]\ttraining's binary_error: 0.02228\tvalid_1's binary_error: 0.28616\n",
      "[757]\ttraining's binary_error: 0.0222\tvalid_1's binary_error: 0.2861\n",
      "[758]\ttraining's binary_error: 0.0221\tvalid_1's binary_error: 0.28614\n",
      "[759]\ttraining's binary_error: 0.02196\tvalid_1's binary_error: 0.28604\n",
      "[760]\ttraining's binary_error: 0.02206\tvalid_1's binary_error: 0.28622\n",
      "[761]\ttraining's binary_error: 0.02188\tvalid_1's binary_error: 0.28608\n",
      "[762]\ttraining's binary_error: 0.02188\tvalid_1's binary_error: 0.28618\n",
      "[763]\ttraining's binary_error: 0.02188\tvalid_1's binary_error: 0.28602\n",
      "[764]\ttraining's binary_error: 0.02186\tvalid_1's binary_error: 0.28606\n",
      "[765]\ttraining's binary_error: 0.0218\tvalid_1's binary_error: 0.28618\n",
      "[766]\ttraining's binary_error: 0.02172\tvalid_1's binary_error: 0.28636\n",
      "[767]\ttraining's binary_error: 0.02168\tvalid_1's binary_error: 0.28642\n",
      "[768]\ttraining's binary_error: 0.02156\tvalid_1's binary_error: 0.28626\n",
      "[769]\ttraining's binary_error: 0.02148\tvalid_1's binary_error: 0.28638\n",
      "[770]\ttraining's binary_error: 0.02136\tvalid_1's binary_error: 0.28626\n",
      "[771]\ttraining's binary_error: 0.0212\tvalid_1's binary_error: 0.28658\n",
      "[772]\ttraining's binary_error: 0.0211\tvalid_1's binary_error: 0.2865\n",
      "[773]\ttraining's binary_error: 0.0209\tvalid_1's binary_error: 0.28642\n",
      "[774]\ttraining's binary_error: 0.02082\tvalid_1's binary_error: 0.28626\n",
      "[775]\ttraining's binary_error: 0.02072\tvalid_1's binary_error: 0.2862\n",
      "[776]\ttraining's binary_error: 0.02072\tvalid_1's binary_error: 0.28596\n",
      "[777]\ttraining's binary_error: 0.02066\tvalid_1's binary_error: 0.28602\n",
      "[778]\ttraining's binary_error: 0.02052\tvalid_1's binary_error: 0.28616\n",
      "[779]\ttraining's binary_error: 0.02054\tvalid_1's binary_error: 0.28616\n",
      "[780]\ttraining's binary_error: 0.02044\tvalid_1's binary_error: 0.28618\n",
      "[781]\ttraining's binary_error: 0.02026\tvalid_1's binary_error: 0.28612\n",
      "[782]\ttraining's binary_error: 0.0201\tvalid_1's binary_error: 0.28606\n",
      "[783]\ttraining's binary_error: 0.01996\tvalid_1's binary_error: 0.28602\n",
      "[784]\ttraining's binary_error: 0.01988\tvalid_1's binary_error: 0.28616\n",
      "[785]\ttraining's binary_error: 0.01988\tvalid_1's binary_error: 0.28636\n",
      "[786]\ttraining's binary_error: 0.0197\tvalid_1's binary_error: 0.28632\n",
      "[787]\ttraining's binary_error: 0.01962\tvalid_1's binary_error: 0.28626\n",
      "[788]\ttraining's binary_error: 0.0196\tvalid_1's binary_error: 0.2861\n",
      "[789]\ttraining's binary_error: 0.01954\tvalid_1's binary_error: 0.28604\n",
      "[790]\ttraining's binary_error: 0.01954\tvalid_1's binary_error: 0.2862\n",
      "[791]\ttraining's binary_error: 0.01948\tvalid_1's binary_error: 0.28628\n",
      "[792]\ttraining's binary_error: 0.0193\tvalid_1's binary_error: 0.2863\n",
      "[793]\ttraining's binary_error: 0.01924\tvalid_1's binary_error: 0.28644\n",
      "[794]\ttraining's binary_error: 0.01904\tvalid_1's binary_error: 0.28634\n",
      "[795]\ttraining's binary_error: 0.01902\tvalid_1's binary_error: 0.28646\n",
      "[796]\ttraining's binary_error: 0.019\tvalid_1's binary_error: 0.28648\n",
      "[797]\ttraining's binary_error: 0.0189\tvalid_1's binary_error: 0.28654\n",
      "[798]\ttraining's binary_error: 0.01876\tvalid_1's binary_error: 0.28644\n",
      "[799]\ttraining's binary_error: 0.01866\tvalid_1's binary_error: 0.2865\n",
      "[800]\ttraining's binary_error: 0.01846\tvalid_1's binary_error: 0.28642\n",
      "[801]\ttraining's binary_error: 0.01844\tvalid_1's binary_error: 0.28668\n",
      "[802]\ttraining's binary_error: 0.01826\tvalid_1's binary_error: 0.28682\n",
      "[803]\ttraining's binary_error: 0.01826\tvalid_1's binary_error: 0.2868\n",
      "[804]\ttraining's binary_error: 0.01824\tvalid_1's binary_error: 0.28676\n",
      "[805]\ttraining's binary_error: 0.01818\tvalid_1's binary_error: 0.28682\n",
      "[806]\ttraining's binary_error: 0.01822\tvalid_1's binary_error: 0.2868\n",
      "[807]\ttraining's binary_error: 0.0182\tvalid_1's binary_error: 0.28676\n",
      "[808]\ttraining's binary_error: 0.01818\tvalid_1's binary_error: 0.28672\n",
      "[809]\ttraining's binary_error: 0.01808\tvalid_1's binary_error: 0.2867\n",
      "[810]\ttraining's binary_error: 0.01798\tvalid_1's binary_error: 0.28672\n",
      "[811]\ttraining's binary_error: 0.01784\tvalid_1's binary_error: 0.28664\n",
      "[812]\ttraining's binary_error: 0.01784\tvalid_1's binary_error: 0.28676\n",
      "[813]\ttraining's binary_error: 0.01784\tvalid_1's binary_error: 0.28678\n",
      "[814]\ttraining's binary_error: 0.01778\tvalid_1's binary_error: 0.28658\n",
      "[815]\ttraining's binary_error: 0.01766\tvalid_1's binary_error: 0.28646\n",
      "[816]\ttraining's binary_error: 0.01758\tvalid_1's binary_error: 0.28638\n",
      "[817]\ttraining's binary_error: 0.0175\tvalid_1's binary_error: 0.2867\n",
      "[818]\ttraining's binary_error: 0.01752\tvalid_1's binary_error: 0.2868\n",
      "[819]\ttraining's binary_error: 0.01752\tvalid_1's binary_error: 0.28662\n",
      "[820]\ttraining's binary_error: 0.01732\tvalid_1's binary_error: 0.28666\n",
      "[821]\ttraining's binary_error: 0.01726\tvalid_1's binary_error: 0.28642\n",
      "[822]\ttraining's binary_error: 0.01726\tvalid_1's binary_error: 0.2864\n",
      "[823]\ttraining's binary_error: 0.0172\tvalid_1's binary_error: 0.2864\n",
      "[824]\ttraining's binary_error: 0.01714\tvalid_1's binary_error: 0.28678\n",
      "[825]\ttraining's binary_error: 0.01712\tvalid_1's binary_error: 0.28698\n",
      "[826]\ttraining's binary_error: 0.01708\tvalid_1's binary_error: 0.28694\n",
      "[827]\ttraining's binary_error: 0.01698\tvalid_1's binary_error: 0.28694\n",
      "[828]\ttraining's binary_error: 0.01682\tvalid_1's binary_error: 0.28734\n",
      "[829]\ttraining's binary_error: 0.0168\tvalid_1's binary_error: 0.28736\n",
      "[830]\ttraining's binary_error: 0.01668\tvalid_1's binary_error: 0.28718\n",
      "[831]\ttraining's binary_error: 0.01652\tvalid_1's binary_error: 0.28712\n",
      "[832]\ttraining's binary_error: 0.01648\tvalid_1's binary_error: 0.28716\n",
      "[833]\ttraining's binary_error: 0.01638\tvalid_1's binary_error: 0.28704\n",
      "[834]\ttraining's binary_error: 0.01632\tvalid_1's binary_error: 0.28706\n",
      "[835]\ttraining's binary_error: 0.0163\tvalid_1's binary_error: 0.2873\n",
      "[836]\ttraining's binary_error: 0.01626\tvalid_1's binary_error: 0.28736\n",
      "[837]\ttraining's binary_error: 0.01616\tvalid_1's binary_error: 0.28708\n",
      "[838]\ttraining's binary_error: 0.0162\tvalid_1's binary_error: 0.28712\n",
      "[839]\ttraining's binary_error: 0.01606\tvalid_1's binary_error: 0.28702\n",
      "[840]\ttraining's binary_error: 0.01598\tvalid_1's binary_error: 0.28706\n",
      "[841]\ttraining's binary_error: 0.0159\tvalid_1's binary_error: 0.2869\n",
      "[842]\ttraining's binary_error: 0.01588\tvalid_1's binary_error: 0.2867\n",
      "[843]\ttraining's binary_error: 0.01578\tvalid_1's binary_error: 0.28664\n",
      "[844]\ttraining's binary_error: 0.01578\tvalid_1's binary_error: 0.28644\n",
      "[845]\ttraining's binary_error: 0.01578\tvalid_1's binary_error: 0.2864\n",
      "[846]\ttraining's binary_error: 0.01576\tvalid_1's binary_error: 0.2865\n",
      "[847]\ttraining's binary_error: 0.01568\tvalid_1's binary_error: 0.28672\n",
      "[848]\ttraining's binary_error: 0.0156\tvalid_1's binary_error: 0.2868\n",
      "[849]\ttraining's binary_error: 0.0156\tvalid_1's binary_error: 0.28684\n",
      "[850]\ttraining's binary_error: 0.0155\tvalid_1's binary_error: 0.28694\n",
      "[851]\ttraining's binary_error: 0.01544\tvalid_1's binary_error: 0.28696\n",
      "[852]\ttraining's binary_error: 0.01536\tvalid_1's binary_error: 0.28662\n",
      "[853]\ttraining's binary_error: 0.01534\tvalid_1's binary_error: 0.28678\n",
      "[854]\ttraining's binary_error: 0.01528\tvalid_1's binary_error: 0.28682\n",
      "[855]\ttraining's binary_error: 0.01518\tvalid_1's binary_error: 0.28694\n",
      "[856]\ttraining's binary_error: 0.0151\tvalid_1's binary_error: 0.28682\n",
      "[857]\ttraining's binary_error: 0.0151\tvalid_1's binary_error: 0.2869\n",
      "[858]\ttraining's binary_error: 0.01504\tvalid_1's binary_error: 0.28676\n",
      "[859]\ttraining's binary_error: 0.015\tvalid_1's binary_error: 0.2867\n",
      "[860]\ttraining's binary_error: 0.015\tvalid_1's binary_error: 0.2867\n",
      "[861]\ttraining's binary_error: 0.01498\tvalid_1's binary_error: 0.28688\n",
      "[862]\ttraining's binary_error: 0.01474\tvalid_1's binary_error: 0.2868\n",
      "[863]\ttraining's binary_error: 0.01462\tvalid_1's binary_error: 0.28694\n",
      "[864]\ttraining's binary_error: 0.01458\tvalid_1's binary_error: 0.28708\n",
      "[865]\ttraining's binary_error: 0.01456\tvalid_1's binary_error: 0.28702\n",
      "[866]\ttraining's binary_error: 0.01456\tvalid_1's binary_error: 0.28682\n",
      "[867]\ttraining's binary_error: 0.01442\tvalid_1's binary_error: 0.28674\n",
      "[868]\ttraining's binary_error: 0.01444\tvalid_1's binary_error: 0.28668\n",
      "[869]\ttraining's binary_error: 0.01438\tvalid_1's binary_error: 0.28668\n",
      "[870]\ttraining's binary_error: 0.01438\tvalid_1's binary_error: 0.28668\n",
      "[871]\ttraining's binary_error: 0.01438\tvalid_1's binary_error: 0.2868\n",
      "[872]\ttraining's binary_error: 0.01428\tvalid_1's binary_error: 0.28696\n",
      "[873]\ttraining's binary_error: 0.01426\tvalid_1's binary_error: 0.28696\n",
      "[874]\ttraining's binary_error: 0.01428\tvalid_1's binary_error: 0.28694\n",
      "[875]\ttraining's binary_error: 0.01414\tvalid_1's binary_error: 0.28702\n",
      "[876]\ttraining's binary_error: 0.01404\tvalid_1's binary_error: 0.28704\n",
      "[877]\ttraining's binary_error: 0.01398\tvalid_1's binary_error: 0.28698\n",
      "[878]\ttraining's binary_error: 0.01396\tvalid_1's binary_error: 0.28684\n",
      "[879]\ttraining's binary_error: 0.01386\tvalid_1's binary_error: 0.28716\n",
      "[880]\ttraining's binary_error: 0.01374\tvalid_1's binary_error: 0.2872\n",
      "[881]\ttraining's binary_error: 0.01374\tvalid_1's binary_error: 0.28728\n",
      "[882]\ttraining's binary_error: 0.01364\tvalid_1's binary_error: 0.28708\n",
      "[883]\ttraining's binary_error: 0.01364\tvalid_1's binary_error: 0.28716\n",
      "[884]\ttraining's binary_error: 0.01356\tvalid_1's binary_error: 0.28724\n",
      "[885]\ttraining's binary_error: 0.01352\tvalid_1's binary_error: 0.28706\n",
      "[886]\ttraining's binary_error: 0.01346\tvalid_1's binary_error: 0.28694\n",
      "[887]\ttraining's binary_error: 0.01332\tvalid_1's binary_error: 0.28674\n",
      "[888]\ttraining's binary_error: 0.01322\tvalid_1's binary_error: 0.28672\n",
      "[889]\ttraining's binary_error: 0.0132\tvalid_1's binary_error: 0.28684\n",
      "[890]\ttraining's binary_error: 0.01324\tvalid_1's binary_error: 0.28704\n",
      "[891]\ttraining's binary_error: 0.0132\tvalid_1's binary_error: 0.2868\n",
      "[892]\ttraining's binary_error: 0.01304\tvalid_1's binary_error: 0.2868\n",
      "[893]\ttraining's binary_error: 0.01294\tvalid_1's binary_error: 0.2871\n",
      "[894]\ttraining's binary_error: 0.01288\tvalid_1's binary_error: 0.28702\n",
      "[895]\ttraining's binary_error: 0.01286\tvalid_1's binary_error: 0.287\n",
      "[896]\ttraining's binary_error: 0.01286\tvalid_1's binary_error: 0.28704\n",
      "[897]\ttraining's binary_error: 0.01288\tvalid_1's binary_error: 0.28704\n",
      "[898]\ttraining's binary_error: 0.01278\tvalid_1's binary_error: 0.28688\n",
      "[899]\ttraining's binary_error: 0.01268\tvalid_1's binary_error: 0.28702\n",
      "[900]\ttraining's binary_error: 0.01276\tvalid_1's binary_error: 0.28686\n",
      "[901]\ttraining's binary_error: 0.0127\tvalid_1's binary_error: 0.28676\n",
      "[902]\ttraining's binary_error: 0.01268\tvalid_1's binary_error: 0.28676\n",
      "[903]\ttraining's binary_error: 0.01266\tvalid_1's binary_error: 0.2868\n",
      "[904]\ttraining's binary_error: 0.0126\tvalid_1's binary_error: 0.28662\n",
      "[905]\ttraining's binary_error: 0.01254\tvalid_1's binary_error: 0.2866\n",
      "[906]\ttraining's binary_error: 0.01244\tvalid_1's binary_error: 0.2866\n",
      "[907]\ttraining's binary_error: 0.01238\tvalid_1's binary_error: 0.28636\n",
      "[908]\ttraining's binary_error: 0.01236\tvalid_1's binary_error: 0.2864\n",
      "[909]\ttraining's binary_error: 0.01234\tvalid_1's binary_error: 0.28636\n",
      "[910]\ttraining's binary_error: 0.01224\tvalid_1's binary_error: 0.28642\n",
      "[911]\ttraining's binary_error: 0.01224\tvalid_1's binary_error: 0.2865\n",
      "[912]\ttraining's binary_error: 0.0122\tvalid_1's binary_error: 0.28658\n",
      "[913]\ttraining's binary_error: 0.01216\tvalid_1's binary_error: 0.2867\n",
      "[914]\ttraining's binary_error: 0.01214\tvalid_1's binary_error: 0.2866\n",
      "[915]\ttraining's binary_error: 0.01214\tvalid_1's binary_error: 0.28654\n",
      "[916]\ttraining's binary_error: 0.01208\tvalid_1's binary_error: 0.28626\n",
      "[917]\ttraining's binary_error: 0.01204\tvalid_1's binary_error: 0.28632\n",
      "[918]\ttraining's binary_error: 0.01204\tvalid_1's binary_error: 0.2863\n",
      "[919]\ttraining's binary_error: 0.01196\tvalid_1's binary_error: 0.2863\n",
      "[920]\ttraining's binary_error: 0.01186\tvalid_1's binary_error: 0.28638\n",
      "[921]\ttraining's binary_error: 0.01182\tvalid_1's binary_error: 0.28636\n",
      "[922]\ttraining's binary_error: 0.01182\tvalid_1's binary_error: 0.28638\n",
      "[923]\ttraining's binary_error: 0.0118\tvalid_1's binary_error: 0.28644\n",
      "[924]\ttraining's binary_error: 0.0118\tvalid_1's binary_error: 0.28656\n",
      "[925]\ttraining's binary_error: 0.01168\tvalid_1's binary_error: 0.28706\n",
      "[926]\ttraining's binary_error: 0.01166\tvalid_1's binary_error: 0.28674\n",
      "[927]\ttraining's binary_error: 0.01172\tvalid_1's binary_error: 0.2867\n",
      "[928]\ttraining's binary_error: 0.01166\tvalid_1's binary_error: 0.2864\n",
      "[929]\ttraining's binary_error: 0.01162\tvalid_1's binary_error: 0.28636\n",
      "[930]\ttraining's binary_error: 0.01162\tvalid_1's binary_error: 0.28638\n",
      "[931]\ttraining's binary_error: 0.01162\tvalid_1's binary_error: 0.28646\n",
      "[932]\ttraining's binary_error: 0.01162\tvalid_1's binary_error: 0.28656\n",
      "[933]\ttraining's binary_error: 0.01158\tvalid_1's binary_error: 0.2868\n",
      "[934]\ttraining's binary_error: 0.01146\tvalid_1's binary_error: 0.28652\n",
      "[935]\ttraining's binary_error: 0.01132\tvalid_1's binary_error: 0.28638\n",
      "[936]\ttraining's binary_error: 0.01124\tvalid_1's binary_error: 0.28638\n",
      "[937]\ttraining's binary_error: 0.0111\tvalid_1's binary_error: 0.2864\n",
      "[938]\ttraining's binary_error: 0.011\tvalid_1's binary_error: 0.28638\n",
      "[939]\ttraining's binary_error: 0.01092\tvalid_1's binary_error: 0.28676\n",
      "[940]\ttraining's binary_error: 0.01084\tvalid_1's binary_error: 0.28668\n",
      "[941]\ttraining's binary_error: 0.01076\tvalid_1's binary_error: 0.28664\n",
      "[942]\ttraining's binary_error: 0.01076\tvalid_1's binary_error: 0.2868\n",
      "[943]\ttraining's binary_error: 0.01068\tvalid_1's binary_error: 0.28684\n",
      "[944]\ttraining's binary_error: 0.01056\tvalid_1's binary_error: 0.28708\n",
      "[945]\ttraining's binary_error: 0.01054\tvalid_1's binary_error: 0.287\n",
      "[946]\ttraining's binary_error: 0.01042\tvalid_1's binary_error: 0.28708\n",
      "[947]\ttraining's binary_error: 0.0104\tvalid_1's binary_error: 0.287\n",
      "[948]\ttraining's binary_error: 0.01034\tvalid_1's binary_error: 0.28704\n",
      "[949]\ttraining's binary_error: 0.01032\tvalid_1's binary_error: 0.28698\n",
      "[950]\ttraining's binary_error: 0.0103\tvalid_1's binary_error: 0.2869\n",
      "[951]\ttraining's binary_error: 0.01032\tvalid_1's binary_error: 0.28684\n",
      "[952]\ttraining's binary_error: 0.01026\tvalid_1's binary_error: 0.28668\n",
      "[953]\ttraining's binary_error: 0.01028\tvalid_1's binary_error: 0.28684\n",
      "[954]\ttraining's binary_error: 0.01028\tvalid_1's binary_error: 0.28688\n",
      "[955]\ttraining's binary_error: 0.01024\tvalid_1's binary_error: 0.28692\n",
      "[956]\ttraining's binary_error: 0.01016\tvalid_1's binary_error: 0.2868\n",
      "[957]\ttraining's binary_error: 0.01016\tvalid_1's binary_error: 0.28682\n",
      "[958]\ttraining's binary_error: 0.01012\tvalid_1's binary_error: 0.28668\n",
      "[959]\ttraining's binary_error: 0.01008\tvalid_1's binary_error: 0.2867\n",
      "[960]\ttraining's binary_error: 0.01008\tvalid_1's binary_error: 0.2868\n",
      "[961]\ttraining's binary_error: 0.01014\tvalid_1's binary_error: 0.2867\n",
      "[962]\ttraining's binary_error: 0.01012\tvalid_1's binary_error: 0.28666\n",
      "[963]\ttraining's binary_error: 0.01014\tvalid_1's binary_error: 0.2868\n",
      "[964]\ttraining's binary_error: 0.01002\tvalid_1's binary_error: 0.28674\n",
      "[965]\ttraining's binary_error: 0.00998\tvalid_1's binary_error: 0.28694\n",
      "[966]\ttraining's binary_error: 0.0099\tvalid_1's binary_error: 0.28702\n",
      "[967]\ttraining's binary_error: 0.00992\tvalid_1's binary_error: 0.28688\n",
      "[968]\ttraining's binary_error: 0.00984\tvalid_1's binary_error: 0.287\n",
      "[969]\ttraining's binary_error: 0.00978\tvalid_1's binary_error: 0.2871\n",
      "[970]\ttraining's binary_error: 0.00974\tvalid_1's binary_error: 0.28732\n",
      "[971]\ttraining's binary_error: 0.00962\tvalid_1's binary_error: 0.28728\n",
      "[972]\ttraining's binary_error: 0.0096\tvalid_1's binary_error: 0.28764\n",
      "[973]\ttraining's binary_error: 0.0095\tvalid_1's binary_error: 0.28752\n",
      "[974]\ttraining's binary_error: 0.00944\tvalid_1's binary_error: 0.2874\n",
      "[975]\ttraining's binary_error: 0.0094\tvalid_1's binary_error: 0.2874\n",
      "[976]\ttraining's binary_error: 0.00942\tvalid_1's binary_error: 0.28748\n",
      "[977]\ttraining's binary_error: 0.00942\tvalid_1's binary_error: 0.28724\n",
      "[978]\ttraining's binary_error: 0.0094\tvalid_1's binary_error: 0.28702\n",
      "[979]\ttraining's binary_error: 0.00932\tvalid_1's binary_error: 0.28716\n",
      "[980]\ttraining's binary_error: 0.0093\tvalid_1's binary_error: 0.28694\n",
      "[981]\ttraining's binary_error: 0.00926\tvalid_1's binary_error: 0.28704\n",
      "[982]\ttraining's binary_error: 0.00916\tvalid_1's binary_error: 0.28704\n",
      "[983]\ttraining's binary_error: 0.00914\tvalid_1's binary_error: 0.28718\n",
      "[984]\ttraining's binary_error: 0.00912\tvalid_1's binary_error: 0.28718\n",
      "[985]\ttraining's binary_error: 0.0091\tvalid_1's binary_error: 0.28722\n",
      "[986]\ttraining's binary_error: 0.0091\tvalid_1's binary_error: 0.28728\n",
      "[987]\ttraining's binary_error: 0.0091\tvalid_1's binary_error: 0.28722\n",
      "[988]\ttraining's binary_error: 0.00908\tvalid_1's binary_error: 0.2872\n",
      "[989]\ttraining's binary_error: 0.00896\tvalid_1's binary_error: 0.28726\n",
      "[990]\ttraining's binary_error: 0.00896\tvalid_1's binary_error: 0.28726\n",
      "[991]\ttraining's binary_error: 0.0089\tvalid_1's binary_error: 0.28754\n",
      "[992]\ttraining's binary_error: 0.00876\tvalid_1's binary_error: 0.28742\n",
      "[993]\ttraining's binary_error: 0.00874\tvalid_1's binary_error: 0.2874\n",
      "[994]\ttraining's binary_error: 0.00872\tvalid_1's binary_error: 0.2874\n",
      "[995]\ttraining's binary_error: 0.00868\tvalid_1's binary_error: 0.28742\n",
      "[996]\ttraining's binary_error: 0.00864\tvalid_1's binary_error: 0.28748\n",
      "[997]\ttraining's binary_error: 0.00848\tvalid_1's binary_error: 0.28746\n",
      "[998]\ttraining's binary_error: 0.00844\tvalid_1's binary_error: 0.28764\n",
      "[999]\ttraining's binary_error: 0.00842\tvalid_1's binary_error: 0.2874\n",
      "[1000]\ttraining's binary_error: 0.00822\tvalid_1's binary_error: 0.28754\n",
      "[1001]\ttraining's binary_error: 0.00824\tvalid_1's binary_error: 0.28752\n",
      "[1002]\ttraining's binary_error: 0.0082\tvalid_1's binary_error: 0.2875\n",
      "[1003]\ttraining's binary_error: 0.00818\tvalid_1's binary_error: 0.28752\n",
      "[1004]\ttraining's binary_error: 0.00814\tvalid_1's binary_error: 0.28774\n",
      "[1005]\ttraining's binary_error: 0.00814\tvalid_1's binary_error: 0.28784\n",
      "[1006]\ttraining's binary_error: 0.00804\tvalid_1's binary_error: 0.28772\n",
      "[1007]\ttraining's binary_error: 0.00798\tvalid_1's binary_error: 0.2876\n",
      "[1008]\ttraining's binary_error: 0.00798\tvalid_1's binary_error: 0.28752\n",
      "[1009]\ttraining's binary_error: 0.0079\tvalid_1's binary_error: 0.28744\n",
      "[1010]\ttraining's binary_error: 0.00794\tvalid_1's binary_error: 0.28736\n",
      "[1011]\ttraining's binary_error: 0.0079\tvalid_1's binary_error: 0.28722\n",
      "[1012]\ttraining's binary_error: 0.0079\tvalid_1's binary_error: 0.28716\n",
      "[1013]\ttraining's binary_error: 0.0079\tvalid_1's binary_error: 0.2871\n",
      "[1014]\ttraining's binary_error: 0.00786\tvalid_1's binary_error: 0.28704\n",
      "[1015]\ttraining's binary_error: 0.00784\tvalid_1's binary_error: 0.28696\n",
      "[1016]\ttraining's binary_error: 0.0078\tvalid_1's binary_error: 0.28706\n",
      "[1017]\ttraining's binary_error: 0.00768\tvalid_1's binary_error: 0.28698\n",
      "[1018]\ttraining's binary_error: 0.00766\tvalid_1's binary_error: 0.28674\n",
      "[1019]\ttraining's binary_error: 0.00762\tvalid_1's binary_error: 0.28692\n",
      "[1020]\ttraining's binary_error: 0.00766\tvalid_1's binary_error: 0.28702\n",
      "[1021]\ttraining's binary_error: 0.00762\tvalid_1's binary_error: 0.287\n",
      "[1022]\ttraining's binary_error: 0.00758\tvalid_1's binary_error: 0.28686\n",
      "[1023]\ttraining's binary_error: 0.00762\tvalid_1's binary_error: 0.2866\n",
      "[1024]\ttraining's binary_error: 0.0075\tvalid_1's binary_error: 0.28684\n",
      "[1025]\ttraining's binary_error: 0.00746\tvalid_1's binary_error: 0.28694\n",
      "[1026]\ttraining's binary_error: 0.00734\tvalid_1's binary_error: 0.2869\n",
      "[1027]\ttraining's binary_error: 0.00734\tvalid_1's binary_error: 0.28678\n",
      "[1028]\ttraining's binary_error: 0.00734\tvalid_1's binary_error: 0.28694\n",
      "[1029]\ttraining's binary_error: 0.00736\tvalid_1's binary_error: 0.28694\n",
      "[1030]\ttraining's binary_error: 0.00732\tvalid_1's binary_error: 0.28712\n",
      "[1031]\ttraining's binary_error: 0.00728\tvalid_1's binary_error: 0.28692\n",
      "[1032]\ttraining's binary_error: 0.00724\tvalid_1's binary_error: 0.28666\n",
      "[1033]\ttraining's binary_error: 0.00722\tvalid_1's binary_error: 0.28648\n",
      "[1034]\ttraining's binary_error: 0.00722\tvalid_1's binary_error: 0.28654\n",
      "[1035]\ttraining's binary_error: 0.00724\tvalid_1's binary_error: 0.28664\n",
      "[1036]\ttraining's binary_error: 0.00724\tvalid_1's binary_error: 0.2867\n",
      "[1037]\ttraining's binary_error: 0.00722\tvalid_1's binary_error: 0.28662\n",
      "[1038]\ttraining's binary_error: 0.00716\tvalid_1's binary_error: 0.28658\n",
      "[1039]\ttraining's binary_error: 0.00716\tvalid_1's binary_error: 0.2866\n",
      "[1040]\ttraining's binary_error: 0.00716\tvalid_1's binary_error: 0.28662\n",
      "[1041]\ttraining's binary_error: 0.00712\tvalid_1's binary_error: 0.28676\n",
      "[1042]\ttraining's binary_error: 0.00706\tvalid_1's binary_error: 0.2869\n",
      "[1043]\ttraining's binary_error: 0.00704\tvalid_1's binary_error: 0.28706\n",
      "[1044]\ttraining's binary_error: 0.00706\tvalid_1's binary_error: 0.2872\n",
      "[1045]\ttraining's binary_error: 0.00704\tvalid_1's binary_error: 0.28704\n",
      "[1046]\ttraining's binary_error: 0.00704\tvalid_1's binary_error: 0.28708\n",
      "[1047]\ttraining's binary_error: 0.00694\tvalid_1's binary_error: 0.28696\n",
      "[1048]\ttraining's binary_error: 0.00692\tvalid_1's binary_error: 0.28674\n",
      "[1049]\ttraining's binary_error: 0.00692\tvalid_1's binary_error: 0.2868\n",
      "[1050]\ttraining's binary_error: 0.00692\tvalid_1's binary_error: 0.28674\n",
      "[1051]\ttraining's binary_error: 0.0069\tvalid_1's binary_error: 0.28664\n",
      "[1052]\ttraining's binary_error: 0.0069\tvalid_1's binary_error: 0.28658\n",
      "[1053]\ttraining's binary_error: 0.00684\tvalid_1's binary_error: 0.28688\n",
      "[1054]\ttraining's binary_error: 0.00686\tvalid_1's binary_error: 0.28714\n",
      "[1055]\ttraining's binary_error: 0.00688\tvalid_1's binary_error: 0.28728\n",
      "[1056]\ttraining's binary_error: 0.00686\tvalid_1's binary_error: 0.28724\n",
      "[1057]\ttraining's binary_error: 0.0068\tvalid_1's binary_error: 0.28716\n",
      "[1058]\ttraining's binary_error: 0.0068\tvalid_1's binary_error: 0.28728\n",
      "[1059]\ttraining's binary_error: 0.00678\tvalid_1's binary_error: 0.2873\n",
      "[1060]\ttraining's binary_error: 0.00678\tvalid_1's binary_error: 0.2873\n",
      "[1061]\ttraining's binary_error: 0.00676\tvalid_1's binary_error: 0.28704\n",
      "[1062]\ttraining's binary_error: 0.00672\tvalid_1's binary_error: 0.28688\n",
      "[1063]\ttraining's binary_error: 0.00664\tvalid_1's binary_error: 0.28704\n",
      "[1064]\ttraining's binary_error: 0.00662\tvalid_1's binary_error: 0.28688\n",
      "[1065]\ttraining's binary_error: 0.00658\tvalid_1's binary_error: 0.28706\n",
      "[1066]\ttraining's binary_error: 0.00646\tvalid_1's binary_error: 0.28708\n",
      "[1067]\ttraining's binary_error: 0.00644\tvalid_1's binary_error: 0.28688\n",
      "[1068]\ttraining's binary_error: 0.00644\tvalid_1's binary_error: 0.28688\n",
      "[1069]\ttraining's binary_error: 0.00644\tvalid_1's binary_error: 0.2868\n",
      "[1070]\ttraining's binary_error: 0.00646\tvalid_1's binary_error: 0.2869\n",
      "[1071]\ttraining's binary_error: 0.00646\tvalid_1's binary_error: 0.28704\n",
      "[1072]\ttraining's binary_error: 0.00642\tvalid_1's binary_error: 0.28712\n",
      "[1073]\ttraining's binary_error: 0.00644\tvalid_1's binary_error: 0.28706\n",
      "[1074]\ttraining's binary_error: 0.00636\tvalid_1's binary_error: 0.28698\n",
      "[1075]\ttraining's binary_error: 0.00634\tvalid_1's binary_error: 0.28694\n",
      "[1076]\ttraining's binary_error: 0.00636\tvalid_1's binary_error: 0.28704\n",
      "[1077]\ttraining's binary_error: 0.00634\tvalid_1's binary_error: 0.2871\n",
      "[1078]\ttraining's binary_error: 0.00634\tvalid_1's binary_error: 0.28728\n",
      "[1079]\ttraining's binary_error: 0.00638\tvalid_1's binary_error: 0.28712\n",
      "[1080]\ttraining's binary_error: 0.0063\tvalid_1's binary_error: 0.28712\n",
      "[1081]\ttraining's binary_error: 0.00626\tvalid_1's binary_error: 0.28698\n",
      "[1082]\ttraining's binary_error: 0.00624\tvalid_1's binary_error: 0.28698\n",
      "[1083]\ttraining's binary_error: 0.0062\tvalid_1's binary_error: 0.28696\n",
      "[1084]\ttraining's binary_error: 0.0062\tvalid_1's binary_error: 0.28696\n",
      "[1085]\ttraining's binary_error: 0.00618\tvalid_1's binary_error: 0.28688\n",
      "[1086]\ttraining's binary_error: 0.00614\tvalid_1's binary_error: 0.28698\n",
      "[1087]\ttraining's binary_error: 0.0061\tvalid_1's binary_error: 0.28708\n",
      "[1088]\ttraining's binary_error: 0.00604\tvalid_1's binary_error: 0.28686\n",
      "[1089]\ttraining's binary_error: 0.00604\tvalid_1's binary_error: 0.28692\n",
      "[1090]\ttraining's binary_error: 0.00602\tvalid_1's binary_error: 0.28674\n",
      "[1091]\ttraining's binary_error: 0.00598\tvalid_1's binary_error: 0.28684\n",
      "[1092]\ttraining's binary_error: 0.00598\tvalid_1's binary_error: 0.28678\n",
      "[1093]\ttraining's binary_error: 0.00594\tvalid_1's binary_error: 0.28718\n",
      "[1094]\ttraining's binary_error: 0.0059\tvalid_1's binary_error: 0.28716\n",
      "[1095]\ttraining's binary_error: 0.00586\tvalid_1's binary_error: 0.28714\n",
      "[1096]\ttraining's binary_error: 0.00584\tvalid_1's binary_error: 0.28724\n",
      "[1097]\ttraining's binary_error: 0.00582\tvalid_1's binary_error: 0.28724\n",
      "[1098]\ttraining's binary_error: 0.00572\tvalid_1's binary_error: 0.28714\n",
      "[1099]\ttraining's binary_error: 0.00576\tvalid_1's binary_error: 0.28734\n",
      "[1100]\ttraining's binary_error: 0.00574\tvalid_1's binary_error: 0.28736\n",
      "[1101]\ttraining's binary_error: 0.00574\tvalid_1's binary_error: 0.28722\n",
      "[1102]\ttraining's binary_error: 0.00572\tvalid_1's binary_error: 0.28732\n",
      "[1103]\ttraining's binary_error: 0.00564\tvalid_1's binary_error: 0.28738\n",
      "[1104]\ttraining's binary_error: 0.00564\tvalid_1's binary_error: 0.2876\n",
      "[1105]\ttraining's binary_error: 0.00562\tvalid_1's binary_error: 0.28756\n",
      "[1106]\ttraining's binary_error: 0.0056\tvalid_1's binary_error: 0.28736\n",
      "[1107]\ttraining's binary_error: 0.00556\tvalid_1's binary_error: 0.28732\n",
      "[1108]\ttraining's binary_error: 0.00558\tvalid_1's binary_error: 0.28744\n",
      "[1109]\ttraining's binary_error: 0.00554\tvalid_1's binary_error: 0.28748\n",
      "[1110]\ttraining's binary_error: 0.00552\tvalid_1's binary_error: 0.28756\n",
      "[1111]\ttraining's binary_error: 0.00548\tvalid_1's binary_error: 0.2873\n",
      "[1112]\ttraining's binary_error: 0.00552\tvalid_1's binary_error: 0.28728\n",
      "[1113]\ttraining's binary_error: 0.00552\tvalid_1's binary_error: 0.2874\n",
      "[1114]\ttraining's binary_error: 0.00552\tvalid_1's binary_error: 0.28752\n",
      "[1115]\ttraining's binary_error: 0.0055\tvalid_1's binary_error: 0.28754\n",
      "[1116]\ttraining's binary_error: 0.0055\tvalid_1's binary_error: 0.28752\n",
      "[1117]\ttraining's binary_error: 0.00546\tvalid_1's binary_error: 0.28754\n",
      "[1118]\ttraining's binary_error: 0.0054\tvalid_1's binary_error: 0.28752\n",
      "[1119]\ttraining's binary_error: 0.00532\tvalid_1's binary_error: 0.28752\n",
      "[1120]\ttraining's binary_error: 0.0054\tvalid_1's binary_error: 0.28746\n",
      "[1121]\ttraining's binary_error: 0.00532\tvalid_1's binary_error: 0.28738\n",
      "[1122]\ttraining's binary_error: 0.00524\tvalid_1's binary_error: 0.28748\n",
      "[1123]\ttraining's binary_error: 0.00522\tvalid_1's binary_error: 0.2876\n",
      "[1124]\ttraining's binary_error: 0.00518\tvalid_1's binary_error: 0.28758\n",
      "[1125]\ttraining's binary_error: 0.00524\tvalid_1's binary_error: 0.28754\n",
      "[1126]\ttraining's binary_error: 0.00524\tvalid_1's binary_error: 0.28764\n",
      "[1127]\ttraining's binary_error: 0.00526\tvalid_1's binary_error: 0.28744\n",
      "[1128]\ttraining's binary_error: 0.00514\tvalid_1's binary_error: 0.28738\n",
      "[1129]\ttraining's binary_error: 0.00504\tvalid_1's binary_error: 0.28742\n",
      "[1130]\ttraining's binary_error: 0.005\tvalid_1's binary_error: 0.2874\n",
      "[1131]\ttraining's binary_error: 0.005\tvalid_1's binary_error: 0.28682\n",
      "[1132]\ttraining's binary_error: 0.00498\tvalid_1's binary_error: 0.2869\n",
      "[1133]\ttraining's binary_error: 0.00496\tvalid_1's binary_error: 0.28698\n",
      "[1134]\ttraining's binary_error: 0.00494\tvalid_1's binary_error: 0.28702\n",
      "[1135]\ttraining's binary_error: 0.00496\tvalid_1's binary_error: 0.28696\n",
      "[1136]\ttraining's binary_error: 0.00494\tvalid_1's binary_error: 0.287\n",
      "[1137]\ttraining's binary_error: 0.00486\tvalid_1's binary_error: 0.28704\n",
      "[1138]\ttraining's binary_error: 0.0049\tvalid_1's binary_error: 0.28702\n",
      "[1139]\ttraining's binary_error: 0.0049\tvalid_1's binary_error: 0.287\n",
      "[1140]\ttraining's binary_error: 0.00476\tvalid_1's binary_error: 0.28692\n",
      "[1141]\ttraining's binary_error: 0.00476\tvalid_1's binary_error: 0.28676\n",
      "[1142]\ttraining's binary_error: 0.00476\tvalid_1's binary_error: 0.28672\n",
      "[1143]\ttraining's binary_error: 0.00474\tvalid_1's binary_error: 0.2867\n",
      "[1144]\ttraining's binary_error: 0.00468\tvalid_1's binary_error: 0.2868\n",
      "[1145]\ttraining's binary_error: 0.00468\tvalid_1's binary_error: 0.28678\n",
      "[1146]\ttraining's binary_error: 0.00464\tvalid_1's binary_error: 0.28678\n",
      "[1147]\ttraining's binary_error: 0.00462\tvalid_1's binary_error: 0.2867\n",
      "[1148]\ttraining's binary_error: 0.00458\tvalid_1's binary_error: 0.28676\n",
      "[1149]\ttraining's binary_error: 0.00458\tvalid_1's binary_error: 0.28682\n",
      "[1150]\ttraining's binary_error: 0.00454\tvalid_1's binary_error: 0.28706\n",
      "[1151]\ttraining's binary_error: 0.00448\tvalid_1's binary_error: 0.28698\n",
      "[1152]\ttraining's binary_error: 0.00438\tvalid_1's binary_error: 0.2872\n",
      "[1153]\ttraining's binary_error: 0.00438\tvalid_1's binary_error: 0.28706\n",
      "[1154]\ttraining's binary_error: 0.00436\tvalid_1's binary_error: 0.28684\n",
      "[1155]\ttraining's binary_error: 0.00432\tvalid_1's binary_error: 0.28698\n",
      "[1156]\ttraining's binary_error: 0.00434\tvalid_1's binary_error: 0.28684\n",
      "[1157]\ttraining's binary_error: 0.00424\tvalid_1's binary_error: 0.28648\n",
      "[1158]\ttraining's binary_error: 0.00414\tvalid_1's binary_error: 0.28646\n",
      "[1159]\ttraining's binary_error: 0.00414\tvalid_1's binary_error: 0.2865\n",
      "[1160]\ttraining's binary_error: 0.00414\tvalid_1's binary_error: 0.28644\n",
      "[1161]\ttraining's binary_error: 0.00408\tvalid_1's binary_error: 0.28634\n",
      "[1162]\ttraining's binary_error: 0.00408\tvalid_1's binary_error: 0.2865\n",
      "[1163]\ttraining's binary_error: 0.0041\tvalid_1's binary_error: 0.2865\n",
      "[1164]\ttraining's binary_error: 0.00406\tvalid_1's binary_error: 0.28662\n",
      "[1165]\ttraining's binary_error: 0.00406\tvalid_1's binary_error: 0.2866\n",
      "[1166]\ttraining's binary_error: 0.00406\tvalid_1's binary_error: 0.28652\n",
      "[1167]\ttraining's binary_error: 0.00406\tvalid_1's binary_error: 0.28654\n",
      "[1168]\ttraining's binary_error: 0.004\tvalid_1's binary_error: 0.28672\n",
      "[1169]\ttraining's binary_error: 0.00394\tvalid_1's binary_error: 0.2866\n",
      "[1170]\ttraining's binary_error: 0.00392\tvalid_1's binary_error: 0.28684\n",
      "[1171]\ttraining's binary_error: 0.00394\tvalid_1's binary_error: 0.28682\n",
      "[1172]\ttraining's binary_error: 0.00394\tvalid_1's binary_error: 0.28664\n",
      "[1173]\ttraining's binary_error: 0.00394\tvalid_1's binary_error: 0.2869\n",
      "[1174]\ttraining's binary_error: 0.00392\tvalid_1's binary_error: 0.28684\n",
      "[1175]\ttraining's binary_error: 0.0039\tvalid_1's binary_error: 0.287\n",
      "[1176]\ttraining's binary_error: 0.00388\tvalid_1's binary_error: 0.287\n",
      "[1177]\ttraining's binary_error: 0.00388\tvalid_1's binary_error: 0.287\n",
      "[1178]\ttraining's binary_error: 0.00386\tvalid_1's binary_error: 0.28712\n",
      "[1179]\ttraining's binary_error: 0.00386\tvalid_1's binary_error: 0.287\n",
      "[1180]\ttraining's binary_error: 0.00386\tvalid_1's binary_error: 0.28694\n",
      "[1181]\ttraining's binary_error: 0.00382\tvalid_1's binary_error: 0.28692\n",
      "[1182]\ttraining's binary_error: 0.00382\tvalid_1's binary_error: 0.28704\n",
      "[1183]\ttraining's binary_error: 0.00382\tvalid_1's binary_error: 0.28694\n",
      "[1184]\ttraining's binary_error: 0.0038\tvalid_1's binary_error: 0.28716\n",
      "[1185]\ttraining's binary_error: 0.00376\tvalid_1's binary_error: 0.28724\n",
      "[1186]\ttraining's binary_error: 0.00368\tvalid_1's binary_error: 0.28734\n",
      "[1187]\ttraining's binary_error: 0.00366\tvalid_1's binary_error: 0.28738\n",
      "[1188]\ttraining's binary_error: 0.00356\tvalid_1's binary_error: 0.287\n",
      "[1189]\ttraining's binary_error: 0.00356\tvalid_1's binary_error: 0.28678\n",
      "[1190]\ttraining's binary_error: 0.00356\tvalid_1's binary_error: 0.28686\n",
      "[1191]\ttraining's binary_error: 0.00356\tvalid_1's binary_error: 0.2869\n",
      "[1192]\ttraining's binary_error: 0.0035\tvalid_1's binary_error: 0.28728\n",
      "[1193]\ttraining's binary_error: 0.00346\tvalid_1's binary_error: 0.28734\n",
      "[1194]\ttraining's binary_error: 0.00346\tvalid_1's binary_error: 0.28754\n",
      "[1195]\ttraining's binary_error: 0.00344\tvalid_1's binary_error: 0.28728\n",
      "[1196]\ttraining's binary_error: 0.0034\tvalid_1's binary_error: 0.28732\n",
      "[1197]\ttraining's binary_error: 0.00334\tvalid_1's binary_error: 0.28752\n",
      "[1198]\ttraining's binary_error: 0.00334\tvalid_1's binary_error: 0.28736\n",
      "[1199]\ttraining's binary_error: 0.00334\tvalid_1's binary_error: 0.28758\n",
      "[1200]\ttraining's binary_error: 0.00332\tvalid_1's binary_error: 0.28758\n",
      "[1201]\ttraining's binary_error: 0.00326\tvalid_1's binary_error: 0.28782\n",
      "[1202]\ttraining's binary_error: 0.00326\tvalid_1's binary_error: 0.28748\n",
      "[1203]\ttraining's binary_error: 0.00326\tvalid_1's binary_error: 0.28728\n",
      "[1204]\ttraining's binary_error: 0.00326\tvalid_1's binary_error: 0.2873\n",
      "[1205]\ttraining's binary_error: 0.00318\tvalid_1's binary_error: 0.2873\n",
      "[1206]\ttraining's binary_error: 0.0032\tvalid_1's binary_error: 0.28712\n",
      "[1207]\ttraining's binary_error: 0.00312\tvalid_1's binary_error: 0.2871\n",
      "[1208]\ttraining's binary_error: 0.00308\tvalid_1's binary_error: 0.287\n",
      "[1209]\ttraining's binary_error: 0.00308\tvalid_1's binary_error: 0.2871\n",
      "[1210]\ttraining's binary_error: 0.00306\tvalid_1's binary_error: 0.28702\n",
      "[1211]\ttraining's binary_error: 0.00298\tvalid_1's binary_error: 0.28698\n",
      "[1212]\ttraining's binary_error: 0.00296\tvalid_1's binary_error: 0.28706\n",
      "[1213]\ttraining's binary_error: 0.00294\tvalid_1's binary_error: 0.28694\n",
      "[1214]\ttraining's binary_error: 0.00294\tvalid_1's binary_error: 0.28714\n",
      "[1215]\ttraining's binary_error: 0.00288\tvalid_1's binary_error: 0.28722\n",
      "[1216]\ttraining's binary_error: 0.00286\tvalid_1's binary_error: 0.28722\n",
      "[1217]\ttraining's binary_error: 0.00284\tvalid_1's binary_error: 0.2872\n",
      "[1218]\ttraining's binary_error: 0.00284\tvalid_1's binary_error: 0.28746\n",
      "[1219]\ttraining's binary_error: 0.00282\tvalid_1's binary_error: 0.28776\n",
      "[1220]\ttraining's binary_error: 0.00276\tvalid_1's binary_error: 0.28772\n",
      "[1221]\ttraining's binary_error: 0.00274\tvalid_1's binary_error: 0.28738\n",
      "[1222]\ttraining's binary_error: 0.00276\tvalid_1's binary_error: 0.2874\n",
      "[1223]\ttraining's binary_error: 0.00276\tvalid_1's binary_error: 0.28738\n",
      "[1224]\ttraining's binary_error: 0.00274\tvalid_1's binary_error: 0.28732\n",
      "[1225]\ttraining's binary_error: 0.00274\tvalid_1's binary_error: 0.2875\n",
      "[1226]\ttraining's binary_error: 0.00272\tvalid_1's binary_error: 0.28762\n",
      "[1227]\ttraining's binary_error: 0.00272\tvalid_1's binary_error: 0.28752\n",
      "[1228]\ttraining's binary_error: 0.0027\tvalid_1's binary_error: 0.28728\n",
      "[1229]\ttraining's binary_error: 0.0027\tvalid_1's binary_error: 0.28722\n",
      "[1230]\ttraining's binary_error: 0.00274\tvalid_1's binary_error: 0.28716\n",
      "[1231]\ttraining's binary_error: 0.00274\tvalid_1's binary_error: 0.2872\n",
      "[1232]\ttraining's binary_error: 0.00274\tvalid_1's binary_error: 0.28706\n",
      "[1233]\ttraining's binary_error: 0.00272\tvalid_1's binary_error: 0.28706\n",
      "[1234]\ttraining's binary_error: 0.00274\tvalid_1's binary_error: 0.28696\n",
      "[1235]\ttraining's binary_error: 0.0027\tvalid_1's binary_error: 0.28688\n",
      "[1236]\ttraining's binary_error: 0.00268\tvalid_1's binary_error: 0.28696\n",
      "[1237]\ttraining's binary_error: 0.00264\tvalid_1's binary_error: 0.28682\n",
      "[1238]\ttraining's binary_error: 0.0026\tvalid_1's binary_error: 0.28698\n",
      "[1239]\ttraining's binary_error: 0.00258\tvalid_1's binary_error: 0.28704\n",
      "[1240]\ttraining's binary_error: 0.00256\tvalid_1's binary_error: 0.28704\n",
      "[1241]\ttraining's binary_error: 0.00256\tvalid_1's binary_error: 0.28708\n",
      "[1242]\ttraining's binary_error: 0.00254\tvalid_1's binary_error: 0.28698\n",
      "[1243]\ttraining's binary_error: 0.00254\tvalid_1's binary_error: 0.28702\n",
      "[1244]\ttraining's binary_error: 0.00252\tvalid_1's binary_error: 0.28704\n",
      "[1245]\ttraining's binary_error: 0.0025\tvalid_1's binary_error: 0.28704\n",
      "[1246]\ttraining's binary_error: 0.00248\tvalid_1's binary_error: 0.28694\n",
      "[1247]\ttraining's binary_error: 0.00244\tvalid_1's binary_error: 0.28674\n",
      "[1248]\ttraining's binary_error: 0.00242\tvalid_1's binary_error: 0.28642\n",
      "[1249]\ttraining's binary_error: 0.00242\tvalid_1's binary_error: 0.28642\n",
      "[1250]\ttraining's binary_error: 0.00238\tvalid_1's binary_error: 0.28654\n",
      "[1251]\ttraining's binary_error: 0.00238\tvalid_1's binary_error: 0.28664\n",
      "[1252]\ttraining's binary_error: 0.00232\tvalid_1's binary_error: 0.28682\n",
      "[1253]\ttraining's binary_error: 0.00232\tvalid_1's binary_error: 0.28668\n",
      "[1254]\ttraining's binary_error: 0.00232\tvalid_1's binary_error: 0.2868\n",
      "[1255]\ttraining's binary_error: 0.0023\tvalid_1's binary_error: 0.2865\n",
      "[1256]\ttraining's binary_error: 0.00226\tvalid_1's binary_error: 0.28656\n",
      "[1257]\ttraining's binary_error: 0.00228\tvalid_1's binary_error: 0.28662\n",
      "[1258]\ttraining's binary_error: 0.00228\tvalid_1's binary_error: 0.28658\n",
      "[1259]\ttraining's binary_error: 0.00228\tvalid_1's binary_error: 0.28662\n",
      "[1260]\ttraining's binary_error: 0.00228\tvalid_1's binary_error: 0.28662\n",
      "[1261]\ttraining's binary_error: 0.00226\tvalid_1's binary_error: 0.2866\n",
      "[1262]\ttraining's binary_error: 0.00226\tvalid_1's binary_error: 0.28688\n",
      "[1263]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28682\n",
      "[1264]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28688\n",
      "[1265]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28694\n",
      "[1266]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.2869\n",
      "[1267]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28692\n",
      "[1268]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28688\n",
      "[1269]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28686\n",
      "[1270]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28686\n",
      "[1271]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28666\n",
      "[1272]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28654\n",
      "[1273]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28666\n",
      "[1274]\ttraining's binary_error: 0.00224\tvalid_1's binary_error: 0.28654\n",
      "[1275]\ttraining's binary_error: 0.00222\tvalid_1's binary_error: 0.28646\n",
      "[1276]\ttraining's binary_error: 0.00222\tvalid_1's binary_error: 0.28644\n",
      "[1277]\ttraining's binary_error: 0.00222\tvalid_1's binary_error: 0.28626\n",
      "[1278]\ttraining's binary_error: 0.00222\tvalid_1's binary_error: 0.28632\n",
      "[1279]\ttraining's binary_error: 0.00218\tvalid_1's binary_error: 0.28648\n",
      "[1280]\ttraining's binary_error: 0.00218\tvalid_1's binary_error: 0.2866\n",
      "[1281]\ttraining's binary_error: 0.00218\tvalid_1's binary_error: 0.28652\n",
      "[1282]\ttraining's binary_error: 0.00218\tvalid_1's binary_error: 0.28648\n",
      "[1283]\ttraining's binary_error: 0.00218\tvalid_1's binary_error: 0.28646\n",
      "[1284]\ttraining's binary_error: 0.00218\tvalid_1's binary_error: 0.28644\n",
      "[1285]\ttraining's binary_error: 0.00212\tvalid_1's binary_error: 0.28648\n",
      "[1286]\ttraining's binary_error: 0.0021\tvalid_1's binary_error: 0.2864\n",
      "[1287]\ttraining's binary_error: 0.00208\tvalid_1's binary_error: 0.28642\n",
      "[1288]\ttraining's binary_error: 0.00208\tvalid_1's binary_error: 0.28628\n",
      "[1289]\ttraining's binary_error: 0.00208\tvalid_1's binary_error: 0.28628\n",
      "[1290]\ttraining's binary_error: 0.00206\tvalid_1's binary_error: 0.2864\n",
      "[1291]\ttraining's binary_error: 0.00206\tvalid_1's binary_error: 0.28658\n",
      "[1292]\ttraining's binary_error: 0.00204\tvalid_1's binary_error: 0.2863\n",
      "[1293]\ttraining's binary_error: 0.002\tvalid_1's binary_error: 0.28642\n",
      "[1294]\ttraining's binary_error: 0.002\tvalid_1's binary_error: 0.28644\n",
      "[1295]\ttraining's binary_error: 0.00198\tvalid_1's binary_error: 0.28638\n",
      "[1296]\ttraining's binary_error: 0.00198\tvalid_1's binary_error: 0.2866\n",
      "[1297]\ttraining's binary_error: 0.00198\tvalid_1's binary_error: 0.28662\n",
      "[1298]\ttraining's binary_error: 0.0019\tvalid_1's binary_error: 0.2865\n",
      "[1299]\ttraining's binary_error: 0.00182\tvalid_1's binary_error: 0.28638\n",
      "[1300]\ttraining's binary_error: 0.00182\tvalid_1's binary_error: 0.2865\n",
      "[1301]\ttraining's binary_error: 0.00182\tvalid_1's binary_error: 0.28652\n",
      "[1302]\ttraining's binary_error: 0.0018\tvalid_1's binary_error: 0.2865\n",
      "[1303]\ttraining's binary_error: 0.0018\tvalid_1's binary_error: 0.28666\n",
      "[1304]\ttraining's binary_error: 0.00178\tvalid_1's binary_error: 0.28664\n",
      "[1305]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28684\n",
      "[1306]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28686\n",
      "[1307]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.287\n",
      "[1308]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28708\n",
      "[1309]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28718\n",
      "[1310]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28722\n",
      "[1311]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28714\n",
      "[1312]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28702\n",
      "[1313]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28722\n",
      "[1314]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.2869\n",
      "[1315]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28702\n",
      "[1316]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28698\n",
      "[1317]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28718\n",
      "[1318]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28736\n",
      "[1319]\ttraining's binary_error: 0.00176\tvalid_1's binary_error: 0.28732\n",
      "[1320]\ttraining's binary_error: 0.00168\tvalid_1's binary_error: 0.28712\n",
      "[1321]\ttraining's binary_error: 0.00166\tvalid_1's binary_error: 0.28712\n",
      "[1322]\ttraining's binary_error: 0.00166\tvalid_1's binary_error: 0.28724\n",
      "[1323]\ttraining's binary_error: 0.00164\tvalid_1's binary_error: 0.2871\n",
      "[1324]\ttraining's binary_error: 0.00164\tvalid_1's binary_error: 0.287\n",
      "[1325]\ttraining's binary_error: 0.00162\tvalid_1's binary_error: 0.28694\n",
      "[1326]\ttraining's binary_error: 0.00162\tvalid_1's binary_error: 0.28688\n",
      "[1327]\ttraining's binary_error: 0.0016\tvalid_1's binary_error: 0.28664\n",
      "[1328]\ttraining's binary_error: 0.00158\tvalid_1's binary_error: 0.28642\n",
      "[1329]\ttraining's binary_error: 0.00158\tvalid_1's binary_error: 0.28644\n",
      "[1330]\ttraining's binary_error: 0.00156\tvalid_1's binary_error: 0.28644\n",
      "[1331]\ttraining's binary_error: 0.00156\tvalid_1's binary_error: 0.28644\n",
      "[1332]\ttraining's binary_error: 0.00154\tvalid_1's binary_error: 0.28652\n",
      "[1333]\ttraining's binary_error: 0.0015\tvalid_1's binary_error: 0.2863\n",
      "[1334]\ttraining's binary_error: 0.00148\tvalid_1's binary_error: 0.28628\n",
      "[1335]\ttraining's binary_error: 0.00146\tvalid_1's binary_error: 0.28646\n",
      "[1336]\ttraining's binary_error: 0.00146\tvalid_1's binary_error: 0.2865\n",
      "[1337]\ttraining's binary_error: 0.00146\tvalid_1's binary_error: 0.28658\n",
      "[1338]\ttraining's binary_error: 0.00144\tvalid_1's binary_error: 0.28654\n",
      "[1339]\ttraining's binary_error: 0.00144\tvalid_1's binary_error: 0.28644\n",
      "[1340]\ttraining's binary_error: 0.00144\tvalid_1's binary_error: 0.28624\n",
      "[1341]\ttraining's binary_error: 0.00144\tvalid_1's binary_error: 0.2862\n",
      "[1342]\ttraining's binary_error: 0.00142\tvalid_1's binary_error: 0.28638\n",
      "[1343]\ttraining's binary_error: 0.0014\tvalid_1's binary_error: 0.28624\n",
      "[1344]\ttraining's binary_error: 0.00138\tvalid_1's binary_error: 0.28624\n",
      "[1345]\ttraining's binary_error: 0.00138\tvalid_1's binary_error: 0.28604\n",
      "[1346]\ttraining's binary_error: 0.00138\tvalid_1's binary_error: 0.28622\n",
      "[1347]\ttraining's binary_error: 0.00138\tvalid_1's binary_error: 0.28608\n",
      "[1348]\ttraining's binary_error: 0.00138\tvalid_1's binary_error: 0.28588\n",
      "[1349]\ttraining's binary_error: 0.00138\tvalid_1's binary_error: 0.28602\n",
      "[1350]\ttraining's binary_error: 0.00138\tvalid_1's binary_error: 0.28592\n",
      "[1351]\ttraining's binary_error: 0.00136\tvalid_1's binary_error: 0.286\n",
      "[1352]\ttraining's binary_error: 0.0013\tvalid_1's binary_error: 0.28602\n",
      "[1353]\ttraining's binary_error: 0.00126\tvalid_1's binary_error: 0.2859\n",
      "[1354]\ttraining's binary_error: 0.00126\tvalid_1's binary_error: 0.28588\n",
      "[1355]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28594\n",
      "[1356]\ttraining's binary_error: 0.00124\tvalid_1's binary_error: 0.28594\n",
      "[1357]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28592\n",
      "[1358]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28588\n",
      "[1359]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28606\n",
      "[1360]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28592\n",
      "[1361]\ttraining's binary_error: 0.0012\tvalid_1's binary_error: 0.28628\n",
      "[1362]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28632\n",
      "[1363]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28638\n",
      "[1364]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28636\n",
      "[1365]\ttraining's binary_error: 0.00122\tvalid_1's binary_error: 0.28652\n",
      "[1366]\ttraining's binary_error: 0.0012\tvalid_1's binary_error: 0.28654\n",
      "[1367]\ttraining's binary_error: 0.0012\tvalid_1's binary_error: 0.28658\n",
      "[1368]\ttraining's binary_error: 0.0012\tvalid_1's binary_error: 0.28664\n",
      "[1369]\ttraining's binary_error: 0.00116\tvalid_1's binary_error: 0.28666\n",
      "[1370]\ttraining's binary_error: 0.00114\tvalid_1's binary_error: 0.28666\n",
      "[1371]\ttraining's binary_error: 0.00116\tvalid_1's binary_error: 0.28666\n",
      "[1372]\ttraining's binary_error: 0.00116\tvalid_1's binary_error: 0.2867\n",
      "[1373]\ttraining's binary_error: 0.00114\tvalid_1's binary_error: 0.2868\n",
      "[1374]\ttraining's binary_error: 0.00114\tvalid_1's binary_error: 0.28658\n",
      "[1375]\ttraining's binary_error: 0.00114\tvalid_1's binary_error: 0.28664\n",
      "[1376]\ttraining's binary_error: 0.00114\tvalid_1's binary_error: 0.28664\n",
      "[1377]\ttraining's binary_error: 0.00116\tvalid_1's binary_error: 0.28668\n",
      "[1378]\ttraining's binary_error: 0.00116\tvalid_1's binary_error: 0.28694\n",
      "[1379]\ttraining's binary_error: 0.00116\tvalid_1's binary_error: 0.28696\n",
      "[1380]\ttraining's binary_error: 0.00116\tvalid_1's binary_error: 0.28698\n",
      "[1381]\ttraining's binary_error: 0.00112\tvalid_1's binary_error: 0.28686\n",
      "[1382]\ttraining's binary_error: 0.00112\tvalid_1's binary_error: 0.28676\n",
      "[1383]\ttraining's binary_error: 0.00112\tvalid_1's binary_error: 0.28674\n",
      "[1384]\ttraining's binary_error: 0.00112\tvalid_1's binary_error: 0.28668\n",
      "[1385]\ttraining's binary_error: 0.0011\tvalid_1's binary_error: 0.28666\n",
      "[1386]\ttraining's binary_error: 0.0011\tvalid_1's binary_error: 0.28672\n",
      "[1387]\ttraining's binary_error: 0.00108\tvalid_1's binary_error: 0.28686\n",
      "[1388]\ttraining's binary_error: 0.00108\tvalid_1's binary_error: 0.28686\n",
      "[1389]\ttraining's binary_error: 0.00108\tvalid_1's binary_error: 0.28684\n",
      "[1390]\ttraining's binary_error: 0.00108\tvalid_1's binary_error: 0.28684\n",
      "[1391]\ttraining's binary_error: 0.00106\tvalid_1's binary_error: 0.2868\n",
      "[1392]\ttraining's binary_error: 0.00106\tvalid_1's binary_error: 0.28676\n",
      "[1393]\ttraining's binary_error: 0.00106\tvalid_1's binary_error: 0.28672\n",
      "[1394]\ttraining's binary_error: 0.00108\tvalid_1's binary_error: 0.28664\n",
      "[1395]\ttraining's binary_error: 0.00106\tvalid_1's binary_error: 0.2866\n",
      "[1396]\ttraining's binary_error: 0.00104\tvalid_1's binary_error: 0.28654\n",
      "[1397]\ttraining's binary_error: 0.00104\tvalid_1's binary_error: 0.2867\n",
      "[1398]\ttraining's binary_error: 0.00104\tvalid_1's binary_error: 0.28678\n",
      "[1399]\ttraining's binary_error: 0.00104\tvalid_1's binary_error: 0.28686\n",
      "[1400]\ttraining's binary_error: 0.00104\tvalid_1's binary_error: 0.28688\n",
      "[1401]\ttraining's binary_error: 0.00102\tvalid_1's binary_error: 0.28688\n",
      "[1402]\ttraining's binary_error: 0.00102\tvalid_1's binary_error: 0.2868\n",
      "[1403]\ttraining's binary_error: 0.00102\tvalid_1's binary_error: 0.28678\n",
      "[1404]\ttraining's binary_error: 0.001\tvalid_1's binary_error: 0.28684\n",
      "[1405]\ttraining's binary_error: 0.001\tvalid_1's binary_error: 0.28666\n",
      "[1406]\ttraining's binary_error: 0.00098\tvalid_1's binary_error: 0.28694\n",
      "[1407]\ttraining's binary_error: 0.00098\tvalid_1's binary_error: 0.28706\n",
      "[1408]\ttraining's binary_error: 0.00098\tvalid_1's binary_error: 0.28704\n",
      "[1409]\ttraining's binary_error: 0.00096\tvalid_1's binary_error: 0.2872\n",
      "[1410]\ttraining's binary_error: 0.00096\tvalid_1's binary_error: 0.28702\n",
      "[1411]\ttraining's binary_error: 0.00096\tvalid_1's binary_error: 0.2868\n",
      "[1412]\ttraining's binary_error: 0.00096\tvalid_1's binary_error: 0.28694\n",
      "[1413]\ttraining's binary_error: 0.00096\tvalid_1's binary_error: 0.28694\n",
      "[1414]\ttraining's binary_error: 0.00094\tvalid_1's binary_error: 0.28696\n",
      "[1415]\ttraining's binary_error: 0.00096\tvalid_1's binary_error: 0.28712\n",
      "[1416]\ttraining's binary_error: 0.00094\tvalid_1's binary_error: 0.28718\n",
      "[1417]\ttraining's binary_error: 0.00092\tvalid_1's binary_error: 0.28724\n",
      "[1418]\ttraining's binary_error: 0.0009\tvalid_1's binary_error: 0.28714\n",
      "[1419]\ttraining's binary_error: 0.0009\tvalid_1's binary_error: 0.287\n",
      "[1420]\ttraining's binary_error: 0.0009\tvalid_1's binary_error: 0.28692\n",
      "[1421]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28684\n",
      "[1422]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28672\n",
      "[1423]\ttraining's binary_error: 0.00086\tvalid_1's binary_error: 0.28674\n",
      "[1424]\ttraining's binary_error: 0.00086\tvalid_1's binary_error: 0.28656\n",
      "[1425]\ttraining's binary_error: 0.00086\tvalid_1's binary_error: 0.28656\n",
      "[1426]\ttraining's binary_error: 0.00086\tvalid_1's binary_error: 0.28666\n",
      "[1427]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28682\n",
      "[1428]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28686\n",
      "[1429]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28678\n",
      "[1430]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28668\n",
      "[1431]\ttraining's binary_error: 0.00086\tvalid_1's binary_error: 0.28688\n",
      "[1432]\ttraining's binary_error: 0.00086\tvalid_1's binary_error: 0.28696\n",
      "[1433]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28716\n",
      "[1434]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28724\n",
      "[1435]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28734\n",
      "[1436]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28744\n",
      "[1437]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28724\n",
      "[1438]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28722\n",
      "[1439]\ttraining's binary_error: 0.00086\tvalid_1's binary_error: 0.28718\n",
      "[1440]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28722\n",
      "[1441]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28726\n",
      "[1442]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28726\n",
      "[1443]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28716\n",
      "[1444]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28722\n",
      "[1445]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28728\n",
      "[1446]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28722\n",
      "[1447]\ttraining's binary_error: 0.00088\tvalid_1's binary_error: 0.28708\n",
      "[1448]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28718\n",
      "[1449]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28732\n",
      "[1450]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28732\n",
      "[1451]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28712\n",
      "[1452]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28706\n",
      "[1453]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.2871\n",
      "[1454]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28722\n",
      "[1455]\ttraining's binary_error: 0.00084\tvalid_1's binary_error: 0.28716\n",
      "[1456]\ttraining's binary_error: 0.00082\tvalid_1's binary_error: 0.28696\n",
      "[1457]\ttraining's binary_error: 0.00082\tvalid_1's binary_error: 0.28698\n",
      "[1458]\ttraining's binary_error: 0.0008\tvalid_1's binary_error: 0.28702\n",
      "[1459]\ttraining's binary_error: 0.0008\tvalid_1's binary_error: 0.28694\n",
      "[1460]\ttraining's binary_error: 0.0008\tvalid_1's binary_error: 0.287\n",
      "[1461]\ttraining's binary_error: 0.00076\tvalid_1's binary_error: 0.28694\n",
      "[1462]\ttraining's binary_error: 0.00074\tvalid_1's binary_error: 0.28712\n",
      "[1463]\ttraining's binary_error: 0.00074\tvalid_1's binary_error: 0.28718\n",
      "[1464]\ttraining's binary_error: 0.00074\tvalid_1's binary_error: 0.28714\n",
      "[1465]\ttraining's binary_error: 0.0007\tvalid_1's binary_error: 0.28726\n",
      "[1466]\ttraining's binary_error: 0.0007\tvalid_1's binary_error: 0.28732\n",
      "[1467]\ttraining's binary_error: 0.0007\tvalid_1's binary_error: 0.28716\n",
      "[1468]\ttraining's binary_error: 0.00072\tvalid_1's binary_error: 0.28718\n",
      "[1469]\ttraining's binary_error: 0.00072\tvalid_1's binary_error: 0.28728\n",
      "[1470]\ttraining's binary_error: 0.00072\tvalid_1's binary_error: 0.2873\n",
      "[1471]\ttraining's binary_error: 0.00072\tvalid_1's binary_error: 0.28718\n",
      "[1472]\ttraining's binary_error: 0.0007\tvalid_1's binary_error: 0.28722\n",
      "[1473]\ttraining's binary_error: 0.0007\tvalid_1's binary_error: 0.28718\n",
      "[1474]\ttraining's binary_error: 0.0007\tvalid_1's binary_error: 0.287\n",
      "[1475]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.28704\n",
      "[1476]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.28706\n",
      "[1477]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.28712\n",
      "[1478]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.28698\n",
      "[1479]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.2869\n",
      "[1480]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.287\n",
      "[1481]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.28686\n",
      "[1482]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.28686\n",
      "[1483]\ttraining's binary_error: 0.00068\tvalid_1's binary_error: 0.28664\n",
      "[1484]\ttraining's binary_error: 0.00066\tvalid_1's binary_error: 0.28648\n",
      "[1485]\ttraining's binary_error: 0.00066\tvalid_1's binary_error: 0.28658\n",
      "[1486]\ttraining's binary_error: 0.00066\tvalid_1's binary_error: 0.28656\n",
      "[1487]\ttraining's binary_error: 0.00066\tvalid_1's binary_error: 0.28652\n",
      "[1488]\ttraining's binary_error: 0.00064\tvalid_1's binary_error: 0.28644\n",
      "[1489]\ttraining's binary_error: 0.00064\tvalid_1's binary_error: 0.28636\n",
      "[1490]\ttraining's binary_error: 0.00064\tvalid_1's binary_error: 0.28638\n",
      "[1491]\ttraining's binary_error: 0.00062\tvalid_1's binary_error: 0.2865\n",
      "[1492]\ttraining's binary_error: 0.00062\tvalid_1's binary_error: 0.28622\n",
      "[1493]\ttraining's binary_error: 0.00062\tvalid_1's binary_error: 0.28626\n",
      "[1494]\ttraining's binary_error: 0.00062\tvalid_1's binary_error: 0.2864\n",
      "[1495]\ttraining's binary_error: 0.00058\tvalid_1's binary_error: 0.28632\n",
      "[1496]\ttraining's binary_error: 0.00058\tvalid_1's binary_error: 0.28636\n",
      "[1497]\ttraining's binary_error: 0.00058\tvalid_1's binary_error: 0.28622\n",
      "[1498]\ttraining's binary_error: 0.00058\tvalid_1's binary_error: 0.28636\n",
      "[1499]\ttraining's binary_error: 0.00058\tvalid_1's binary_error: 0.28636\n",
      "[1500]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.28638\n",
      "[1501]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.28658\n",
      "[1502]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.2865\n",
      "[1503]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.28654\n",
      "[1504]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.2865\n",
      "[1505]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.28636\n",
      "[1506]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.28648\n",
      "[1507]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.28632\n",
      "[1508]\ttraining's binary_error: 0.00056\tvalid_1's binary_error: 0.28624\n",
      "[1509]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28626\n",
      "[1510]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28618\n",
      "[1511]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28622\n",
      "[1512]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28632\n",
      "[1513]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28626\n",
      "[1514]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28632\n",
      "[1515]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28634\n",
      "[1516]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.2864\n",
      "[1517]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28632\n",
      "[1518]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28632\n",
      "[1519]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28642\n",
      "[1520]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28632\n",
      "[1521]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.2863\n",
      "[1522]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28618\n",
      "[1523]\ttraining's binary_error: 0.00054\tvalid_1's binary_error: 0.2862\n",
      "[1524]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28624\n",
      "[1525]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28614\n",
      "[1526]\ttraining's binary_error: 0.00052\tvalid_1's binary_error: 0.28606\n",
      "[1527]\ttraining's binary_error: 0.0005\tvalid_1's binary_error: 0.28616\n",
      "[1528]\ttraining's binary_error: 0.0005\tvalid_1's binary_error: 0.28622\n",
      "[1529]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28614\n",
      "[1530]\ttraining's binary_error: 0.0005\tvalid_1's binary_error: 0.28594\n",
      "[1531]\ttraining's binary_error: 0.0005\tvalid_1's binary_error: 0.28594\n",
      "[1532]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28586\n",
      "[1533]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.286\n",
      "[1534]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28608\n",
      "[1535]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28588\n",
      "[1536]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28582\n",
      "[1537]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28586\n",
      "[1538]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.2859\n",
      "[1539]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28602\n",
      "[1540]\ttraining's binary_error: 0.0005\tvalid_1's binary_error: 0.2861\n",
      "[1541]\ttraining's binary_error: 0.0005\tvalid_1's binary_error: 0.28634\n",
      "[1542]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28616\n",
      "[1543]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28634\n",
      "[1544]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28642\n",
      "[1545]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28648\n",
      "[1546]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.2865\n",
      "[1547]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28644\n",
      "[1548]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.2863\n",
      "[1549]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28634\n",
      "[1550]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28638\n",
      "[1551]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28634\n",
      "[1552]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.2865\n",
      "[1553]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28652\n",
      "[1554]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28656\n",
      "[1555]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28632\n",
      "[1556]\ttraining's binary_error: 0.00048\tvalid_1's binary_error: 0.28624\n",
      "[1557]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.28624\n",
      "[1558]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.28626\n",
      "[1559]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.28632\n",
      "[1560]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.28642\n",
      "[1561]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.2864\n",
      "[1562]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.28644\n",
      "[1563]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.28644\n",
      "[1564]\ttraining's binary_error: 0.00046\tvalid_1's binary_error: 0.2864\n",
      "[1565]\ttraining's binary_error: 0.00044\tvalid_1's binary_error: 0.28634\n",
      "[1566]\ttraining's binary_error: 0.00044\tvalid_1's binary_error: 0.28634\n",
      "[1567]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28618\n",
      "[1568]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28598\n",
      "[1569]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28582\n",
      "[1570]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28592\n",
      "[1571]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28586\n",
      "[1572]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.2859\n",
      "[1573]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28578\n",
      "[1574]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28572\n",
      "[1575]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.2857\n",
      "[1576]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28554\n",
      "[1577]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28558\n",
      "[1578]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28566\n",
      "[1579]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.2855\n",
      "[1580]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28542\n",
      "[1581]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28552\n",
      "[1582]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28548\n",
      "[1583]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.2856\n",
      "[1584]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28578\n",
      "[1585]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.28558\n",
      "[1586]\ttraining's binary_error: 0.00042\tvalid_1's binary_error: 0.2856\n",
      "[1587]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28568\n",
      "[1588]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28544\n",
      "[1589]\ttraining's binary_error: 0.00038\tvalid_1's binary_error: 0.28542\n",
      "[1590]\ttraining's binary_error: 0.00038\tvalid_1's binary_error: 0.28546\n",
      "[1591]\ttraining's binary_error: 0.00038\tvalid_1's binary_error: 0.28532\n",
      "[1592]\ttraining's binary_error: 0.00038\tvalid_1's binary_error: 0.2854\n",
      "[1593]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28532\n",
      "[1594]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28504\n",
      "[1595]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28504\n",
      "[1596]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28494\n",
      "[1597]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28506\n",
      "[1598]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28518\n",
      "[1599]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.28506\n",
      "[1600]\ttraining's binary_error: 0.0004\tvalid_1's binary_error: 0.2851\n",
      "[1601]\ttraining's binary_error: 0.00038\tvalid_1's binary_error: 0.28512\n",
      "[1602]\ttraining's binary_error: 0.00036\tvalid_1's binary_error: 0.28534\n",
      "[1603]\ttraining's binary_error: 0.00036\tvalid_1's binary_error: 0.28528\n",
      "[1604]\ttraining's binary_error: 0.00036\tvalid_1's binary_error: 0.28526\n",
      "[1605]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28528\n",
      "[1606]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28528\n",
      "[1607]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28532\n",
      "[1608]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28518\n",
      "[1609]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28506\n",
      "[1610]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28516\n",
      "[1611]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28514\n",
      "[1612]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28534\n",
      "[1613]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.28532\n",
      "[1614]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.28532\n",
      "[1615]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.28536\n",
      "[1616]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.28554\n",
      "[1617]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.2854\n",
      "[1618]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.28558\n",
      "[1619]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.2857\n",
      "[1620]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.2857\n",
      "[1621]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.28568\n",
      "[1622]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28562\n",
      "[1623]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28564\n",
      "[1624]\ttraining's binary_error: 0.0003\tvalid_1's binary_error: 0.28558\n",
      "[1625]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.2856\n",
      "[1626]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28564\n",
      "[1627]\ttraining's binary_error: 0.00032\tvalid_1's binary_error: 0.28548\n",
      "[1628]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.2856\n",
      "[1629]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28558\n",
      "[1630]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28544\n",
      "[1631]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.2856\n",
      "[1632]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28572\n",
      "[1633]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28576\n",
      "[1634]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.286\n",
      "[1635]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28626\n",
      "[1636]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28628\n",
      "[1637]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28622\n",
      "[1638]\ttraining's binary_error: 0.00028\tvalid_1's binary_error: 0.28616\n",
      "[1639]\ttraining's binary_error: 0.00026\tvalid_1's binary_error: 0.2863\n",
      "[1640]\ttraining's binary_error: 0.00026\tvalid_1's binary_error: 0.28652\n",
      "[1641]\ttraining's binary_error: 0.00026\tvalid_1's binary_error: 0.28668\n",
      "[1642]\ttraining's binary_error: 0.00026\tvalid_1's binary_error: 0.28672\n",
      "[1643]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28654\n",
      "[1644]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.2863\n",
      "[1645]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28638\n",
      "[1646]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28632\n",
      "[1647]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28634\n",
      "[1648]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28612\n",
      "[1649]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.2861\n",
      "[1650]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28618\n",
      "[1651]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28628\n",
      "[1652]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28622\n",
      "[1653]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.2863\n",
      "[1654]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.2864\n",
      "[1655]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28634\n",
      "[1656]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28652\n",
      "[1657]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28658\n",
      "[1658]\ttraining's binary_error: 0.00022\tvalid_1's binary_error: 0.28656\n",
      "[1659]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28648\n",
      "[1660]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28636\n",
      "[1661]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28644\n",
      "[1662]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28646\n",
      "[1663]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28664\n",
      "[1664]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28674\n",
      "[1665]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28674\n",
      "[1666]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28682\n",
      "[1667]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28684\n",
      "[1668]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.2868\n",
      "[1669]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28674\n",
      "[1670]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.2867\n",
      "[1671]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28678\n",
      "[1672]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28682\n",
      "[1673]\ttraining's binary_error: 0.0002\tvalid_1's binary_error: 0.28678\n",
      "[1674]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28686\n",
      "[1675]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28688\n",
      "[1676]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28676\n",
      "[1677]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28658\n",
      "[1678]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.2866\n",
      "[1679]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28664\n",
      "[1680]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28676\n",
      "[1681]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28672\n",
      "[1682]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28672\n",
      "[1683]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28666\n",
      "[1684]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28668\n",
      "[1685]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28648\n",
      "[1686]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28658\n",
      "[1687]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28652\n",
      "[1688]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.2866\n",
      "[1689]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28644\n",
      "[1690]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28648\n",
      "[1691]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.2863\n",
      "[1692]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.28632\n",
      "[1693]\ttraining's binary_error: 0.00016\tvalid_1's binary_error: 0.2862\n",
      "[1694]\ttraining's binary_error: 0.00014\tvalid_1's binary_error: 0.28606\n",
      "[1695]\ttraining's binary_error: 0.00014\tvalid_1's binary_error: 0.28602\n",
      "[1696]\ttraining's binary_error: 0.00014\tvalid_1's binary_error: 0.28606\n",
      "[1697]\ttraining's binary_error: 0.00014\tvalid_1's binary_error: 0.28616\n",
      "[1698]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28606\n",
      "[1699]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2862\n",
      "[1700]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28602\n",
      "[1701]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28616\n",
      "[1702]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28624\n",
      "[1703]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28624\n",
      "[1704]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28624\n",
      "[1705]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28634\n",
      "[1706]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28644\n",
      "[1707]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28644\n",
      "[1708]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28608\n",
      "[1709]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28602\n",
      "[1710]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28604\n",
      "[1711]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28606\n",
      "[1712]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28592\n",
      "[1713]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2859\n",
      "[1714]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28594\n",
      "[1715]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.286\n",
      "[1716]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2859\n",
      "[1717]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28598\n",
      "[1718]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28608\n",
      "[1719]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2863\n",
      "[1720]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28626\n",
      "[1721]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28646\n",
      "[1722]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28644\n",
      "[1723]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28634\n",
      "[1724]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2862\n",
      "[1725]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28604\n",
      "[1726]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28602\n",
      "[1727]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28592\n",
      "[1728]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.286\n",
      "[1729]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28592\n",
      "[1730]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28608\n",
      "[1731]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28614\n",
      "[1732]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28606\n",
      "[1733]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28596\n",
      "[1734]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2861\n",
      "[1735]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28598\n",
      "[1736]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28584\n",
      "[1737]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28572\n",
      "[1738]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28602\n",
      "[1739]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28604\n",
      "[1740]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28616\n",
      "[1741]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28612\n",
      "[1742]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28626\n",
      "[1743]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28622\n",
      "[1744]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28644\n",
      "[1745]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28634\n",
      "[1746]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28644\n",
      "[1747]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28658\n",
      "[1748]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2867\n",
      "[1749]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.2868\n",
      "[1750]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28692\n",
      "[1751]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28676\n",
      "[1752]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28674\n",
      "[1753]\ttraining's binary_error: 0.00012\tvalid_1's binary_error: 0.28666\n",
      "[1754]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28678\n",
      "[1755]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2867\n",
      "[1756]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2865\n",
      "[1757]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28648\n",
      "[1758]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2864\n",
      "[1759]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28632\n",
      "[1760]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28632\n",
      "[1761]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28634\n",
      "[1762]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2862\n",
      "[1763]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28616\n",
      "[1764]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28624\n",
      "[1765]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28622\n",
      "[1766]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2864\n",
      "[1767]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28638\n",
      "[1768]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28654\n",
      "[1769]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28646\n",
      "[1770]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28618\n",
      "[1771]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28636\n",
      "[1772]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28624\n",
      "[1773]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28614\n",
      "[1774]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2862\n",
      "[1775]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28608\n",
      "[1776]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.28626\n",
      "[1777]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2862\n",
      "[1778]\ttraining's binary_error: 0.0001\tvalid_1's binary_error: 0.2862\n",
      "[1779]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28626\n",
      "[1780]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.2863\n",
      "[1781]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28624\n",
      "[1782]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28628\n",
      "[1783]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28622\n",
      "[1784]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28656\n",
      "[1785]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28666\n",
      "[1786]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28672\n",
      "[1787]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28676\n",
      "[1788]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28678\n",
      "[1789]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28678\n",
      "[1790]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28694\n",
      "[1791]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28706\n",
      "[1792]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.2871\n",
      "[1793]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28694\n",
      "[1794]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.28682\n",
      "[1795]\ttraining's binary_error: 8e-05\tvalid_1's binary_error: 0.2868\n",
      "[1796]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28686\n",
      "[1797]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28692\n",
      "[1798]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28702\n",
      "[1799]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.2872\n",
      "[1800]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28734\n",
      "[1801]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28746\n",
      "[1802]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.2875\n",
      "[1803]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28756\n",
      "[1804]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28754\n",
      "[1805]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28736\n",
      "[1806]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28758\n",
      "[1807]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28748\n",
      "[1808]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28764\n",
      "[1809]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28756\n",
      "[1810]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28758\n",
      "[1811]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28726\n",
      "[1812]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28748\n",
      "[1813]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28752\n",
      "[1814]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.2875\n",
      "[1815]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28744\n",
      "[1816]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.2875\n",
      "[1817]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28748\n",
      "[1818]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28734\n",
      "[1819]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28734\n",
      "[1820]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28722\n",
      "[1821]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28722\n",
      "[1822]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28716\n",
      "[1823]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28734\n",
      "[1824]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.28726\n",
      "[1825]\ttraining's binary_error: 6e-05\tvalid_1's binary_error: 0.2873\n",
      "[1826]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28722\n",
      "[1827]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1828]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28732\n",
      "[1829]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28728\n",
      "[1830]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2872\n",
      "[1831]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28726\n",
      "[1832]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28726\n",
      "[1833]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28704\n",
      "[1834]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2872\n",
      "[1835]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28714\n",
      "[1836]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28756\n",
      "[1837]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28764\n",
      "[1838]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2875\n",
      "[1839]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28738\n",
      "[1840]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28738\n",
      "[1841]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28736\n",
      "[1842]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28754\n",
      "[1843]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28738\n",
      "[1844]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28732\n",
      "[1845]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2873\n",
      "[1846]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28742\n",
      "[1847]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1848]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28744\n",
      "[1849]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28764\n",
      "[1850]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2873\n",
      "[1851]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28738\n",
      "[1852]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2874\n",
      "[1853]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28754\n",
      "[1854]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28762\n",
      "[1855]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2876\n",
      "[1856]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28768\n",
      "[1857]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2874\n",
      "[1858]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28748\n",
      "[1859]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28736\n",
      "[1860]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28734\n",
      "[1861]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1862]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28714\n",
      "[1863]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2872\n",
      "[1864]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1865]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28712\n",
      "[1866]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28722\n",
      "[1867]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28734\n",
      "[1868]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1869]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1870]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28706\n",
      "[1871]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1872]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28716\n",
      "[1873]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28718\n",
      "[1874]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28716\n",
      "[1875]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28712\n",
      "[1876]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28692\n",
      "[1877]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28718\n",
      "[1878]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28708\n",
      "[1879]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28708\n",
      "[1880]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2869\n",
      "[1881]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.287\n",
      "[1882]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28698\n",
      "[1883]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28696\n",
      "[1884]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28706\n",
      "[1885]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28716\n",
      "[1886]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28734\n",
      "[1887]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28736\n",
      "[1888]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2872\n",
      "[1889]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28718\n",
      "[1890]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1891]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28694\n",
      "[1892]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28692\n",
      "[1893]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28678\n",
      "[1894]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28672\n",
      "[1895]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28688\n",
      "[1896]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28698\n",
      "[1897]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.287\n",
      "[1898]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28686\n",
      "[1899]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28692\n",
      "[1900]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1901]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28704\n",
      "[1902]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28708\n",
      "[1903]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28696\n",
      "[1904]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28702\n",
      "[1905]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2871\n",
      "[1906]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28718\n",
      "[1907]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28718\n",
      "[1908]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28722\n",
      "[1909]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28732\n",
      "[1910]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2873\n",
      "[1911]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28734\n",
      "[1912]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28728\n",
      "[1913]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28724\n",
      "[1914]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28698\n",
      "[1915]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28698\n",
      "[1916]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28692\n",
      "[1917]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28696\n",
      "[1918]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28672\n",
      "[1919]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2867\n",
      "[1920]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28662\n",
      "[1921]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28668\n",
      "[1922]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28654\n",
      "[1923]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28648\n",
      "[1924]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28642\n",
      "[1925]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28664\n",
      "[1926]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28654\n",
      "[1927]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28678\n",
      "[1928]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2867\n",
      "[1929]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.28668\n",
      "[1930]\ttraining's binary_error: 4e-05\tvalid_1's binary_error: 0.2867\n",
      "[1931]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1932]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2872\n",
      "[1933]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28726\n",
      "[1934]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28746\n",
      "[1935]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28738\n",
      "[1936]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28732\n",
      "[1937]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28734\n",
      "[1938]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28724\n",
      "[1939]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28722\n",
      "[1940]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28722\n",
      "[1941]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28714\n",
      "[1942]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28724\n",
      "[1943]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28738\n",
      "[1944]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28716\n",
      "[1945]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2872\n",
      "[1946]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1947]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28728\n",
      "[1948]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28732\n",
      "[1949]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28718\n",
      "[1950]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28722\n",
      "[1951]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28728\n",
      "[1952]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2872\n",
      "[1953]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28704\n",
      "[1954]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28694\n",
      "[1955]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28694\n",
      "[1956]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28682\n",
      "[1957]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2869\n",
      "[1958]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28692\n",
      "[1959]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.287\n",
      "[1960]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28718\n",
      "[1961]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2871\n",
      "[1962]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28688\n",
      "[1963]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2869\n",
      "[1964]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28678\n",
      "[1965]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28688\n",
      "[1966]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28686\n",
      "[1967]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28688\n",
      "[1968]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28694\n",
      "[1969]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28704\n",
      "[1970]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28702\n",
      "[1971]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28712\n",
      "[1972]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28702\n",
      "[1973]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1974]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28694\n",
      "[1975]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28678\n",
      "[1976]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2867\n",
      "[1977]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28678\n",
      "[1978]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28656\n",
      "[1979]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28672\n",
      "[1980]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28676\n",
      "[1981]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28686\n",
      "[1982]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2869\n",
      "[1983]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28686\n",
      "[1984]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28694\n",
      "[1985]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28686\n",
      "[1986]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1987]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28714\n",
      "[1988]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28718\n",
      "[1989]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2871\n",
      "[1990]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28702\n",
      "[1991]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1992]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28708\n",
      "[1993]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28712\n",
      "[1994]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.2871\n",
      "[1995]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28704\n",
      "[1996]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28692\n",
      "[1997]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28696\n",
      "[1998]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28706\n",
      "[1999]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28688\n",
      "[2000]\ttraining's binary_error: 2e-05\tvalid_1's binary_error: 0.28712\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 31, 'objective': 'binary', 'feature_fraction':0.8, 'bagging_fraction':0.8, 'metric':'binary_error'}\n",
    "num_round=2000\n",
    "eval_list = [train_dataset, test_dataset]\n",
    "lgb_model = lgb.train(params, train_dataset, num_round, valid_sets=eval_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Orx1fAGVJGj"
   },
   "source": [
    "How can we improve (save) our deep learning model. \n",
    "1. Discretize\n",
    "2. Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gSeOMEFlVjUN"
   },
   "outputs": [],
   "source": [
    "# coding = 'utf-8'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "# label encoding\n",
    "def encode_label(x):\n",
    "    # np.nan, np.inf, -np.inf 会有多个。\n",
    "    unique=sorted(list(set([str(item) for item in np.unique(x)])))\n",
    "    kv = {unique[i]: i for i in range(len(unique))}\n",
    "    vfunc = np.vectorize(lambda x: kv[str(x)])\n",
    "    return vfunc(x)\n",
    "\n",
    "def encode_label_mat(x):\n",
    "    _, ncol = x.shape\n",
    "    result = np.empty_like(x, dtype=int)\n",
    "    for col in range(ncol):\n",
    "        result[:,col] = encode_label(x[:, col])\n",
    "    return result\n",
    "\n",
    "def impute_nan(x, method='median'):\n",
    "    _, ncol = x.shape\n",
    "    result = np.empty_like(x)\n",
    "\n",
    "    for col in range(ncol):\n",
    "        if method == 'median':\n",
    "            data = x[:, col]\n",
    "            # 计算中位数时，先排除null, np.inf, -np.inf\n",
    "            impute_value = np.median(data[~pd.isnull(data) & (data != np.inf) & (data != -np.inf)])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        func = np.vectorize(lambda x: impute_value if pd.isnull(x) else x)\n",
    "        result[:, col] = func(x[:, col])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_uniform_interval(minimum, maximum, nbins):\n",
    "    result = [minimum]\n",
    "    step_size = (float(maximum - minimum)) / nbins\n",
    "    for index in range(nbins - 1):\n",
    "        result.append(minimum + step_size * (index + 1))\n",
    "    result.append(maximum)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 找 x 在sorted_intervals 中的位置。\n",
    "def get_interval_v2(x, sorted_intervals):\n",
    "    if pd.isnull(x):\n",
    "        return -1\n",
    "    if x == np.inf:\n",
    "        return -2\n",
    "    if x == -np.inf:\n",
    "        return -3\n",
    "    interval = 0\n",
    "    found = False\n",
    "    sorted_intervals.append(np.inf)\n",
    "    while not found and interval < len(sorted_intervals) - 1:\n",
    "        if sorted_intervals[interval] <= x < sorted_intervals[interval + 1]:\n",
    "            return interval\n",
    "        else:\n",
    "            interval += 1\n",
    "\n",
    "\n",
    "# 计算data的任意分位数序列\n",
    "def get_quantile_interval(data, nbins):\n",
    "    quantiles = get_uniform_interval(0, 1, nbins)\n",
    "    return list(np.quantile(data[(~pd.isnull(data)) & (data != np.inf) & (data != -np.inf)], quantiles))\n",
    "\n",
    "# 对 x的每列 按 nbins 个分位数的方式进行离散化。\n",
    "# 同时返回每个列的切分 分位数。\n",
    "# 输入x必须是 numpy.array\n",
    "def discretize(x, nbins=20):\n",
    "    nrow, ncol = x.shape\n",
    "    result = np.empty_like(x)\n",
    "    interval_list = list()\n",
    "    for col in range(ncol):\n",
    "        # 分位数 可能有重复，去重。\n",
    "        intervals = sorted(list(set(get_quantile_interval(x[:, col], nbins))))\n",
    "        interval_centroid = list()\n",
    "\n",
    "        for i in range(len(intervals) - 1):\n",
    "            interval_centroid.append(0.5 * (intervals[i] + intervals[i + 1]))\n",
    "        func = np.vectorize(lambda x: get_interval_v2(x, intervals))\n",
    "        result[:, col] = encode_label(func(x[:, col]))\n",
    "        interval_list.append(interval_centroid)\n",
    "    return result.astype(np.int64), interval_list\n",
    "\n",
    "def get_var_type(df):\n",
    "    columns = df.columns\n",
    "    continuous_vars = [x for x in columns if x.startswith('continuous_')]\n",
    "    discrete_vars = [x for x in columns if x.startswith('discrete_')]\n",
    "    other_vars = list()\n",
    "    for column in columns:\n",
    "        if column not in continuous_vars and column not in discrete_vars:\n",
    "            other_vars.append(column)\n",
    "    return {'continuous': continuous_vars,\n",
    "            'discrete': discrete_vars,\n",
    "            'other': other_vars}\n",
    "\n",
    "\n",
    "def get_cont_var(df):\n",
    "    var_types = get_var_type(df)\n",
    "    return var_types['continuous']\n",
    "\n",
    "\n",
    "def get_dis_var(df):\n",
    "    var_types = get_var_type(df)\n",
    "    return var_types['discrete']\n",
    "\n",
    "def drop_const_var(data):\n",
    "    result = data.copy(deep=True)\n",
    "    for col in data.columns:\n",
    "        if len(data.loc[~pd.isnull(data[col]), col].unique()) <= 1:\n",
    "            result.drop(columns=col, inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2-nHUa7FYU8G"
   },
   "outputs": [],
   "source": [
    "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
    "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
    "x = np.concatenate([x_train_np, x_test_np])\n",
    "x_dis, centroids = discretize(x)\n",
    "x_dis_train = x_dis[:50000, :]\n",
    "x_dis_test = x_dis[50000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WViXd9Ldc_Uy"
   },
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = torch.from_numpy(x).type(torch.int64) \n",
    "        self.y = torch.from_numpy(y).type(torch.float32).squeeze() \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWljva98c_Uy"
   },
   "source": [
    "### Create Embedding Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVX2rMNMc_Uy"
   },
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7cXlyVDEchPe"
   },
   "outputs": [],
   "source": [
    "class EmbeddingFactory(nn.Module):\n",
    "    def __init__(self, x, dim_out):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.module_list = nn.ModuleList(\n",
    "            #embedding 字典大小，注意去重\n",
    "            [nn.Embedding(len(set(np.unique(x[:, col]))), dim_out) for col in range(x.shape[1])])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # torch spare\n",
    "        result = [self.module_list[col](x[:, col]).unsqueeze(2) for col in range(x.shape[1])]\n",
    "        return torch.cat(result, dim=2) #按照最后一个维度，将所有Embdding输出连接起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "E_LgwyAjc_Uz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asiwen/soft/anaconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "x_dis_test.shape\n",
    "train_dataloader = DataLoader(TabDataset(x_dis_train, y_train_np), batch_size = 32, num_workers=6)\n",
    "test_dataloader = DataLoader(TabDataset(x_dis_test, y_test_np), batch_size = 128, num_workers=6)\n",
    "\n",
    "class TrainingModuleV2(pl.LightningModule):\n",
    "    def __init__(self, x, dim_emb, dim_mlp, res_coef=0, dropout_p=0, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingFactory(x, dim_emb)\n",
    "        self.backbone = MyNetwork(x.shape[1]*dim_emb, dim_mlp, res_coef, dropout_p, n_layers)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, \"b h e -> b (h e)\")\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y)\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, \"b h e -> b (h e)\")\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y)\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Training loss\", loss)\n",
    "        self.log(\"Training acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpCnDu0Hc_Uz"
   },
   "outputs": [],
   "source": [
    "# from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "# training_module = TrainingModuleV2(x_dis, 16, 64, 0.5, 0.1, 10)\n",
    "# trainer = pl.Trainer(max_epochs=10, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
    "# trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y66ZQw5Ec_Uz"
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfBt32oNc_U0"
   },
   "source": [
    "### Let us try TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jN1dxePIoqCY"
   },
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2XIjemhTpOvz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "x_1 = torch.randn(100000)\n",
    "x_2 = torch.randn(100000)\n",
    "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
    "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
    "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
    "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
    "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
    "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
    "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) \n",
    "\n",
    "x_train, x_test = x[:50000, :], x[50000:, :]\n",
    "y_train, y_test = y[:50000], y[50000:]\n",
    "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
    "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
    "x = np.concatenate([x_train_np, x_test_np])\n",
    "x_dis, centroids = discretize(x)\n",
    "x_dis_train = x_dis[:50000, :]\n",
    "x_dis_test = x_dis[50000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sY2Lbb-Fc_U1"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Other possible implementations:\n",
    "https://github.com/KrisKorrel/sparsemax-pytorch/blob/master/sparsemax.py\n",
    "https://github.com/msobroza/SparsemaxPytorch/blob/master/mnist/sparsemax.py\n",
    "https://github.com/vene/sparse-structured-attention/blob/master/pytorch/torchsparseattn/sparsemax.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py\n",
    "def _make_ix_like(input, dim=0):\n",
    "    d = input.size(dim)\n",
    "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
    "    view = [1] * input.dim()\n",
    "    view[0] = -1\n",
    "    return rho.view(view).transpose(0, dim)\n",
    "\n",
    "# 自己定义后续传导时要用 Function的派生类\n",
    "class SparsemaxFunction(Function):\n",
    "    \"\"\"\n",
    "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
    "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
    "    By Ben Peters and Vlad Niculae\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : torch.autograd.function._ContextMethodMixin\n",
    "        input : torch.Tensor\n",
    "            any shape\n",
    "        dim : int\n",
    "            dimension along which to apply sparsemax\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : torch.Tensor\n",
    "            same shape as input\n",
    "\n",
    "        \"\"\"\n",
    "        ctx.dim = dim\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input -= max_val  # same numerical stability trick as for softmax\n",
    "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
    "        output = torch.clamp(input - tau, min=0)\n",
    "        ctx.save_for_backward(supp_size, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = (grad_input.sum(dim=dim) / supp_size).squeeze()\n",
    "        v_hat = v_hat.unsqueeze(dim)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
    "        return grad_input, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        \"\"\"Sparsemax building block: compute the threshold\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            any dimension\n",
    "        dim : int\n",
    "            dimension along which to apply the sparsemax\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tau : torch.Tensor\n",
    "            the threshold value\n",
    "        support_size : torch.Tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "        input_cumsum = input_srt.cumsum(dim) - 1\n",
    "        rhos = _make_ix_like(input, dim)\n",
    "        support = rhos * input_srt > input_cumsum\n",
    "\n",
    "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
    "        tau = input_cumsum.gather(dim, support_size - 1)\n",
    "        tau /= support_size.to(input.dtype)\n",
    "        return tau, support_size\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sparsemax(input, self.dim)\n",
    "\n",
    "\n",
    "class Entmax15(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init_()\n",
    "        self.dim=dim\n",
    "            \n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "\n",
    "        rho = _make_ix_like(input, dim)\n",
    "        mean = Xsrt.cumsum(dim) / rho\n",
    "        mean_sq = (Xsrt ** 2).cumsum(dim) / rho\n",
    "        ss = rho * (mean_sq - mean ** 2)\n",
    "        delta = (1 - ss) / rho\n",
    "\n",
    "        delta_nz = torch.clamp(delta, 0)\n",
    "        tau = mean - torch.sqrt(delta_nz)\n",
    "\n",
    "        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n",
    "        tau_star = tau.gather(dim, support_size - 1)\n",
    "        return tau_star, support_size\n",
    "    def forward(self, input):\n",
    "        max_val, _ = input.max(dim=self.dim, keepdim=True)\n",
    "        input = input - max_val  # same numerical stability trick as for softmax\n",
    "        input = input / 2  # divide by 2 to solve actual Entmax\n",
    "\n",
    "        tau_star, _ = Entmax15Function._threshold_and_support(input, self.dim)\n",
    "        output = torch.clamp(input - tau_star, min=0) ** 2\n",
    "        ctx.save_for_backward(output)\n",
    "        return output \n",
    "\n",
    "    def backward(self, output, grad):\n",
    "        Y = output\n",
    "        gppr = Y.sqrt()  # = 1 / g'' (Y)\n",
    "        dX = grad_output * gppr\n",
    "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
    "        q = q.unsqueeze(ctx.dim)\n",
    "        dX -= q * gppr\n",
    "        return dX, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDCekrVoc_U1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def initialize_non_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        return torch.cat(res, dim=0)\n",
    "\n",
    "\n",
    "class TabNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 n_d=64, n_a=64,\n",
    "                 n_steps=5, gamma=1.3,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines main part of the TabNet network without the embedding layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features\n",
    "        output_dim : int or list of int for multi task classification\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.is_multi_task = isinstance(output_dim, list)\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.mask_type = mask_type\n",
    "        self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
    "\n",
    "        if self.n_shared > 0:\n",
    "            shared_feat_transform = torch.nn.ModuleList()\n",
    "            for i in range(self.n_shared):\n",
    "                if i == 0:\n",
    "                    shared_feat_transform.append(Linear(self.input_dim,\n",
    "                                                        2*(n_d + n_a),\n",
    "                                                        bias=False))\n",
    "                else:\n",
    "                    shared_feat_transform.append(Linear(n_d + n_a, 2*(n_d + n_a), bias=False))\n",
    "\n",
    "        else:\n",
    "            shared_feat_transform = None\n",
    "\n",
    "        self.initial_splitter = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                                n_glu_independent=self.n_independent,\n",
    "                                                virtual_batch_size=self.virtual_batch_size,\n",
    "                                                momentum=momentum)\n",
    "\n",
    "        self.feat_transformers = torch.nn.ModuleList()\n",
    "        self.att_transformers = torch.nn.ModuleList()\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            transformer = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                          n_glu_independent=self.n_independent,\n",
    "                                          virtual_batch_size=self.virtual_batch_size,\n",
    "                                          momentum=momentum)\n",
    "            attention = AttentiveTransformer(n_a, self.input_dim,\n",
    "                                             virtual_batch_size=self.virtual_batch_size,\n",
    "                                             momentum=momentum,\n",
    "                                             mask_type=self.mask_type)\n",
    "            self.feat_transformers.append(transformer)\n",
    "            self.att_transformers.append(attention)\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            self.multi_task_mappings = torch.nn.ModuleList()\n",
    "            for task_dim in output_dim:\n",
    "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
    "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
    "                self.multi_task_mappings.append(task_mapping)\n",
    "        else:\n",
    "            self.final_mapping = Linear(n_d, output_dim, bias=False)\n",
    "            initialize_non_glu(self.final_mapping, n_d, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = 0\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape, device=x.device)\n",
    "        M_loss = 0\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),\n",
    "                                           dim=1))\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = ReLU()(out[:, :self.n_d])\n",
    "            res = torch.add(res, d)\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        M_loss /= self.n_steps\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            # Result will be in list format\n",
    "            out = []\n",
    "            for task_mapping in self.multi_task_mappings:\n",
    "                out.append(task_mapping(res))\n",
    "        else:\n",
    "            out = self.final_mapping(res)\n",
    "        return out, M_loss\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape, device=x.device)\n",
    "        M_explain = torch.zeros(x.shape, device=x.device)\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "        masks = {}\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            masks[step] = M\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = ReLU()(out[:, :self.n_d])\n",
    "            # explain\n",
    "            step_importance = torch.sum(d, dim=1)\n",
    "            M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        return M_explain, masks\n",
    "\n",
    "class AttentiveTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 virtual_batch_size=128,\n",
    "                 momentum=0.02,\n",
    "                 mask_type=\"entmax\"):\n",
    "        \"\"\"\n",
    "        Initialize an attention transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.fc = Linear(input_dim, output_dim, bias=False)\n",
    "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
    "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        if mask_type == \"sparsemax\":\n",
    "            # Sparsemax\n",
    "            self.selector = Sparsemax(dim=-1)\n",
    "        elif mask_type == \"entmax\":\n",
    "            # Entmax\n",
    "            self.selector = Entmax15(dim=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please choose either sparsemax\" +\n",
    "                                      \"or entmax as masktype\")\n",
    "\n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.fc(processed_feat)\n",
    "        x = self.bn(x)\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.selector(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_layers, n_glu_independent,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        shared_layers : torch.nn.ModuleList\n",
    "            The shared block that should be common to every step\n",
    "        n_glu_independant : int\n",
    "            Number of independent GLU layers\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization within GLU block(s)\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_glu': n_glu_independent,\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        if shared_layers is None:\n",
    "            # no shared layers\n",
    "            self.shared = torch.nn.Identity()\n",
    "            is_first = True\n",
    "        else:\n",
    "            self.shared = GLU_Block(input_dim, output_dim,\n",
    "                                    first=True,\n",
    "                                    shared_layers=shared_layers,\n",
    "                                    n_glu=len(shared_layers),\n",
    "                                    virtual_batch_size=virtual_batch_size,\n",
    "                                    momentum=momentum)\n",
    "            is_first = False\n",
    "\n",
    "        if n_glu_independent == 0:\n",
    "            # no independent layers\n",
    "            self.specifics = torch.nn.Identity()\n",
    "        else:\n",
    "            spec_input_dim = input_dim if is_first else output_dim\n",
    "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
    "                                       first=is_first,\n",
    "                                       **params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        x = self.specifics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.shared_layers = shared_layers\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "\n",
    "        params = {\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        fc = shared_layers[0] if shared_layers else None\n",
    "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                         fc=fc,\n",
    "                                         **params))\n",
    "        for glu_id in range(1, self.n_glu):\n",
    "            fc = shared_layers[glu_id] if shared_layers else None\n",
    "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                             fc=fc,\n",
    "                                             **params))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = math.sqrt(0.5)\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, fc=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = Linear(input_dim, 2*output_dim, bias=False)\n",
    "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "\n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        # attention-like? transformer\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0imEJmmfc_U2"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = torch.from_numpy(x).type(torch.int64) \n",
    "        self.y = torch.from_numpy(y).type(torch.float32).squeeze() \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "train_dataloader = DataLoader(TabDataset(x_dis_train, y_train_np), batch_size = 32, num_workers=6)\n",
    "test_dataloader = DataLoader(TabDataset(x_dis_test, y_test_np), batch_size = 128, num_workers=6)\n",
    "class TrainingModuleV2(pl.LightningModule):\n",
    "    def __init__(self, x, dim_emb, dim_out, penalty=1e-3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.penalty = penalty\n",
    "        self.embedding = EmbeddingFactory(x, dim_emb)\n",
    "        self.backbone = TabNet(x.shape[1]*dim_emb, dim_out, **kwargs)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.backbone(x)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, 'b n e -> b (n e)')\n",
    "        x, _ = self.backbone(x)\n",
    "        x = self.sigmoid(x.squeeze())\n",
    "        loss = self.loss(x.squeeze(), y.type(torch.float32))\n",
    "        acc = self.accuracy(x.squeeze(), y)\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, 'b n e -> b (n e)')\n",
    "        x, m_loss = self.backbone(x)\n",
    "        x = self.sigmoid(x.squeeze())\n",
    "        loss = self.loss(x, y.type(torch.float32)) - self.penalty*m_loss\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Training loss\", loss)\n",
    "        self.log(\"Training acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "training_module = TrainingModuleV2(x_dis, 32, 1, n_steps=2, n_independent=4, n_shared=4,)\n",
    "trainer = pl.Trainer(max_epochs=10, tpu_cores=8, progress_bar_refresh_rate=100, val_check_interval=0.5)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-uyBdKLc_U3"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p80Wv972uJiU"
   },
   "source": [
    "### Embedding with Distance Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aLqOqPwduJiW"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "class EntityEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_level, emdedding_dim, centroid):\n",
    "        super(EntityEmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_level, emdedding_dim)\n",
    "        self.centroid = torch.tensor(centroid, dtype=torch.float32).detach_()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = x[:, None]\n",
    "        d = 1.0 / ((x - self.centroid).abs() + EPS)\n",
    "        w = self.softmax(d)\n",
    "        v = torch.mm(w, self.embedding.weight)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jvGjhmHeuJiX"
   },
   "outputs": [],
   "source": [
    "embedding = EntityEmbeddingLayer(20, 4, centroids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pytorch_tutorial_final.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

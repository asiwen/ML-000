{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.target_encoder import TargetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造多层全连接网络(带dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def mish(input):\n",
    "\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        '''\n",
    "        return mish(input)\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, res_coef = 0, dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear  = nn.Linear(dim_in, dim_out)\n",
    "        self.res_coef = res_coef\n",
    "        self.activation = Mish()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.ln = nn.LayerNorm(dim_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.dropout(y)\n",
    "        if self.res_coef == 0:\n",
    "            return self.ln(y)\n",
    "        else:\n",
    "            return self.ln(self.res_coef*x +y )\n",
    "\n",
    "        \n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, dim_in, dim, res_coef=0.5, dropout_p = 0.1, n_layers = 10):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.ModuleList()\n",
    "        self.first_linear = MLPLayer(dim_in, dim)\n",
    "        self.n_layers = n_layers\n",
    "        for i in range(n_layers):\n",
    "            self.mlp.append(MLPLayer(dim, dim, res_coef, dropout_p))\n",
    "        self.final = nn.Linear(dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        x= self.sigmoid(self.final(x))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, dim_in, dim, res_coef=0, dropout_p=0, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.backbone = MyNetwork(dim_in, dim, res_coef, dropout_p, n_layers)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Training loss\", loss)\n",
    "        self.log(\"Training acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "import os\n",
    "class CheckpointEveryNSteps(pl.Callback):\n",
    "    def __init__(self, save_step_frequency):\n",
    "        self.save_step_frequency = save_step_frequency\n",
    "\n",
    "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
    "        epoch = trainer.current_epoch\n",
    "        global_step = trainer.global_step\n",
    "        if global_step % self.save_step_frequency == 0:\n",
    "            filename = \"epoch=\" + str(epoch) + \"_step=\" + str(global_step)+\".ckpt\"\n",
    "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
    "            trainer.save_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些有用的数据处理方法\n",
    "1. one-hot 转 ordinal encoding\n",
    "2. Discretize\n",
    "3. Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# coding = 'utf-8'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "# 将onehot 字段还原 成分类字段\n",
    "def inverse_onehot(df, cols_prefix):\n",
    "    cols = [x for x in df.columns if cols_prefix in x]\n",
    "    onehots = df.loc[:, cols].values\n",
    "    trans = np.arange(1, onehots.shape[1]+1).reshape(onehots.shape[1], -1)\n",
    "    return cols, np.dot(onehots, trans)\n",
    "\n",
    "def inverse_onehot_mat(df, col_prefixes):\n",
    "    df1 = df.copy(deep=True)\n",
    "    drop_cols = []\n",
    "    for col in col_prefixes:\n",
    "        cols, value = inverse_onehot(df1, col)\n",
    "        drop_cols.extend(cols)\n",
    "        df1[col] = value\n",
    "        \n",
    "    df1.drop(columns=drop_cols, inplace=True)\n",
    "    return df1\n",
    "\n",
    "def encode_label(x):\n",
    "    unique=sorted(list(set([str(item) for item in np.unique(x)])))\n",
    "    kv = {unique[i]: i for i in range(len(unique))}\n",
    "    vfunc = np.vectorize(lambda x: kv[str(x)])\n",
    "    return vfunc(x)\n",
    "\n",
    "def encode_label_mat(x):\n",
    "    _, ncol = x.shape\n",
    "    result = np.empty_like(x, dtype=int)\n",
    "    for col in range(ncol):\n",
    "        result[:,col] = encode_label(x[:, col])\n",
    "    return result\n",
    "\n",
    "def impute_nan(x, method='median'):\n",
    "    _, ncol = x.shape\n",
    "    result = np.empty_like(x)\n",
    "\n",
    "    for col in range(ncol):\n",
    "        if method == 'median':\n",
    "            data = x[:, col]\n",
    "            impute_value = np.median(data[~pd.isnull(data) & (data != np.inf) & (data != -np.inf)])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        func = np.vectorize(lambda x: impute_value if pd.isnull(x) else x)\n",
    "        result[:, col] = func(x[:, col])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_uniform_interval(minimum, maximum, nbins):\n",
    "    result = [minimum]\n",
    "    step_size = (float(maximum - minimum)) / nbins\n",
    "    for index in range(nbins - 1):\n",
    "        result.append(minimum + step_size * (index + 1))\n",
    "    result.append(maximum)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_interval_v2(x, sorted_intervals):\n",
    "    if pd.isnull(x):\n",
    "        return -1\n",
    "    if x == np.inf:\n",
    "        return -2\n",
    "    if x == -np.inf:\n",
    "        return -3\n",
    "    interval = 0\n",
    "    found = False\n",
    "    sorted_intervals.append(np.inf)\n",
    "    while not found and interval < len(sorted_intervals) - 1:\n",
    "        if sorted_intervals[interval] <= x < sorted_intervals[interval + 1]:\n",
    "            return interval\n",
    "        else:\n",
    "            interval += 1\n",
    "\n",
    "\n",
    "def get_quantile_interval(data, nbins):\n",
    "    quantiles = get_uniform_interval(0, 1, nbins)\n",
    "    return list(np.quantile(data[(~pd.isnull(data)) & (data != np.inf) & (data != -np.inf)], quantiles))\n",
    "\n",
    "\n",
    "def discretize(x, nbins=20):\n",
    "    nrow, ncol = x.shape\n",
    "    result = np.empty_like(x)\n",
    "    interval_list = list()\n",
    "    for col in range(ncol):\n",
    "        intervals = sorted(list(set(get_quantile_interval(x[:, col], nbins))))\n",
    "        interval_centroid = list()\n",
    "\n",
    "        for i in range(len(intervals) - 1):\n",
    "            interval_centroid.append(0.5 * (intervals[i] + intervals[i + 1]))\n",
    "        func = np.vectorize(lambda x: get_interval_v2(x, intervals))\n",
    "        result[:, col] = encode_label(func(x[:, col]))\n",
    "        interval_list.append(interval_centroid)\n",
    "    return result.astype(np.int64), interval_list\n",
    "\n",
    "def get_var_type(df):\n",
    "    columns = df.columns\n",
    "    continuous_vars = [x for x in columns if x.startswith('continuous_')]\n",
    "    discrete_vars = [x for x in columns if x.startswith('discrete_')]\n",
    "    other_vars = list()\n",
    "    for column in columns:\n",
    "        if column not in continuous_vars and column not in discrete_vars:\n",
    "            other_vars.append(column)\n",
    "    return {'continuous': continuous_vars,\n",
    "            'discrete': discrete_vars,\n",
    "            'other': other_vars}\n",
    "\n",
    "\n",
    "def get_cont_var(df):\n",
    "    var_types = get_var_type(df)\n",
    "    return var_types['continuous']\n",
    "\n",
    "\n",
    "def get_dis_var(df):\n",
    "    var_types = get_var_type(df)\n",
    "    return var_types['discrete']\n",
    "\n",
    "def drop_const_var(data):\n",
    "    result = data.copy(deep=True)\n",
    "    for col in data.columns:\n",
    "        if len(data.loc[~pd.isnull(data[col]), col].unique()) <= 1:\n",
    "            result.drop(columns=col, inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_final.csv')\n",
    "df_test = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置要将onehot转换为target-encoder的 特征\n",
    "cols = ['discrete_addr_state', 'discrete_grade'] #, 'discrete_sub_grade', 'discrete_emp_length']\n",
    "df_train_ = inverse_onehot_mat(df_train, cols)\n",
    "df_test_ = inverse_onehot_mat(df_test, cols)\n",
    "\n",
    "x_train, y_train = df_train_.drop(columns='loan_status'), df_train_.loc[:, 'loan_status']\n",
    "x_test, y_test = df_test_.drop(columns='loan_status'), df_test_.loc[:, 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "\n",
    "x_train_ = torch.from_numpy(impute_nan(x_train.values)).type(torch.float32)\n",
    "y_train_ = torch.from_numpy(y_train.values).type(torch.int)\n",
    "x_test_, y_test_ = torch.from_numpy(impute_nan(x_test.values)).type(torch.float32), torch.from_numpy(y_test.values).type(torch.int)\n",
    "\n",
    "train_dataset = MyDataSet(x_train_, y_train_)\n",
    "test_dataset = MyDataSet(x_test_, y_test_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name     | Type      | Params\n",
      "---------------------------------------\n",
      "0 | backbone | MyNetwork | 1.2 K \n",
      "1 | loss     | BCELoss   | 0     \n",
      "2 | accuracy | Accuracy  | 0     \n",
      "---------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd883c2db3c4407b4618ea100969eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d5ceaa39854277b9e82dd4fdd07db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451f259ffd334624b39f27a309f7abd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a5301f6a7143b68655b6a22f3ec8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7faead6d8d424da17a006274944a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bffee61f124ce3b8b5d22caaa2582a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ec0465b6784976a7e95703c598a61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952c513817e745ba84049c299c8294f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343499b06f67451cabcb139b116b1c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bacf532af54b49b72924dd5f48b072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "save_by_steps = CheckpointEveryNSteps(100)\n",
    "training_module = TrainingModule(x_train.shape[1], 10, 0.5, 0.1, 2)\n",
    "trainer = pl.Trainer(max_epochs=2, gpus=None, progress_bar_refresh_rate=100, val_check_interval=0.25, logger=tb_logger)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-508f202a2ab4e0aa\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-508f202a2ab4e0aa\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
